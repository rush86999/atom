import asyncio
import logging
import uuid
import wave # For saving WAV files
import tempfile # For temporary file storage
import os # For path manipulation
from contextlib import asynccontextmanager
from enum import Enum
from typing import Dict, List, Optional, Any
import datetime
import numpy as np # For handling audio data
from openai import OpenAI, APIError, RateLimitError, AuthenticationError, APIConnectionError # Import OpenAI library and specific errors
from notion_client import AsyncClient as NotionAsyncClient, APIResponseError, APIErrorCode # Import Notion Async Client and error types
import aiosqlite # For async SQLite operations
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type # For retry mechanisms

import sounddevice as sd
from fastapi import FastAPI, HTTPException, Path, Body, BackgroundTasks
from pydantic import BaseModel, Field

# --- Logging Configuration ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- Audio Configuration ---
# Standard audio parameters - consider making these configurable if needed
SAMPLE_RATE = 16000  # Hz (16kHz is common for STT)
CHANNELS = 1         # Mono
DTYPE = 'int16'      # Data type for audio samples
BLOCK_DURATION_MS = 500 # Duration of each audio block processed by the callback, in milliseconds
TEMP_AUDIO_DIR = tempfile.gettempdir() # Or a dedicated temp dir within the app

# --- Database Configuration ---
DATABASE_URL = "/app/data/live_meeting_tasks.db" # Path within the container
# Ensure the /app/data directory is created if it doesn't exist, will be handled in init_db

# --- OpenAI Client Initialization ---
# The API key will be read from the OPENAI_API_KEY environment variable by default by the library
# You can also pass it explicitly: client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
# For robustness, ensure OPENAI_API_KEY is set in the environment where the worker runs.
openai_client: Optional[OpenAI] = None
try:
    if os.getenv("OPENAI_API_KEY"):
        openai_client = OpenAI()
        logger.info("OpenAI client initialized successfully.")
    else:
        logger.warning("OPENAI_API_KEY environment variable not found. STT functionality will be disabled.")
except Exception as e:
    logger.error(f"Failed to initialize OpenAI client: {e}", exc_info=True)
    openai_client = None # Ensure it's None if initialization fails

# --- Notion Client Initialization ---
notion_api_key = os.getenv("NOTION_API_KEY")
# Using a parent page ID for simplicity. If a database is preferred, NOTION_DATABASE_ID would be used.
notion_parent_page_id = os.getenv("NOTION_PARENT_PAGE_ID")
notion_client: Optional[NotionAsyncClient] = None

if notion_api_key:
    try:
        notion_client = NotionAsyncClient(auth=notion_api_key)
        logger.info("Notion client initialized successfully.")
        if not notion_parent_page_id:
            logger.warning("NOTION_PARENT_PAGE_ID environment variable not found. Notes will be created in the root of the workspace or a default private page, which might not be ideal.")
    except Exception as e:
        logger.error(f"Failed to initialize Notion client: {e}", exc_info=True)
        notion_client = None
else:
    logger.warning("NOTION_API_KEY environment variable not found. Notion integration will be disabled.")


# --- Models ---
class AudioDevice(BaseModel):
    id: Any # Can be int or string depending on host API
    name: str

class AudioDeviceList(BaseModel):
    devices: List[AudioDevice]

class TaskStatus(str, Enum):
    PENDING = "pending"
    ACTIVE = "active"
    PROCESSING_COMPLETION = "processing_completion"
    COMPLETED = "completed"
    ERROR = "error"

class StartMeetingRequest(BaseModel):
    platform: str = Field(..., example="zoom")
    meeting_id: str = Field(..., example="https://example.zoom.us/j/1234567890")
    audio_device_id: Any = Field(..., example="default") # Can be int or string
    notion_page_title: str = Field(..., example="Meeting Notes - Project Alpha Q3 Review")
    user_id: str = Field(..., example="user_abc_123")

class StartMeetingResponse(BaseModel):
    task_id: str
    status: TaskStatus
    message: Optional[str] = None

class MeetingTask(BaseModel):
    task_id: str
    user_id: str
    platform: str
    meeting_id: str
    audio_device_id: Any
    notion_page_title: str
    status: TaskStatus = TaskStatus.PENDING
    message: Optional[str] = None
    start_time: Optional[datetime.datetime] = None
    end_time: Optional[datetime.datetime] = None
    duration_seconds: Optional[int] = None
    transcript_preview: Optional[str] = None
    notes_preview: Optional[str] = None
    final_transcript_location: Optional[str] = None
    final_notes_location: Optional[str] = None # Could store path to generated notes file eventually

    # Internal attributes for managing the audio stream etc.
    _audio_stream_object: Optional[sd.InputStream] = None # Holds the sounddevice InputStream
    _audio_stream_task: Optional[asyncio.Task] = None # The asyncio task running the capture loop
    _audio_file_path: Optional[str] = None # Path to the temporary WAV file
    _audio_file_writer: Optional[wave.Wave_write] = None # Wave file writer object
    _stop_event: Optional[asyncio.Event] = None # Event to signal the audio callback to stop

    class Config:
        arbitrary_types_allowed = True # To allow complex types like asyncio.Event, sd.InputStream

# --- Database Helper Functions ---

# Fields to select for reconstructing MeetingTask from DB row
MEETING_TASK_DB_FIELDS = [
    "task_id", "user_id", "platform", "meeting_id", "audio_device_id",
    "notion_page_title", "status", "message", "start_time", "end_time",
    "duration_seconds", "transcript_preview", "notes_preview",
    "final_transcript_location", "final_notes_location", "audio_file_path"
]

def _db_row_to_task_model(row: aiosqlite.Row) -> Optional[MeetingTask]:
    if not row:
        return None
    # Convert row to dict
    task_data = dict(row)
    # Convert datetime strings back to datetime objects if they exist
    if task_data.get("start_time"):
        task_data["start_time"] = datetime.datetime.fromisoformat(task_data["start_time"])
    if task_data.get("end_time"):
        task_data["end_time"] = datetime.datetime.fromisoformat(task_data["end_time"])
    # audio_device_id might be int or str, handle conversion if necessary (though TEXT in DB is fine)
    # For now, assume it's stored as TEXT and can be used directly or converted by Pydantic if needed.
    return MeetingTask(**task_data)

async def add_task_to_db(task: MeetingTask):
    async with get_db_connection() as db:
        # Convert MeetingTask Pydantic model to a dict for DB insertion
        # Exclude fields starting with '_' as they are internal runtime attributes
        task_dict = task.model_dump(exclude_none=True, exclude={"_audio_stream_object", "_audio_stream_task", "_audio_file_writer", "_stop_event"})

        # Convert datetime objects to ISO format strings for DB storage
        if task_dict.get("start_time"):
            task_dict["start_time"] = task_dict["start_time"].isoformat()
        if task_dict.get("end_time"):
            task_dict["end_time"] = task_dict["end_time"].isoformat()

        # Ensure all fields from MEETING_TASK_DB_FIELDS are present, defaulting to None if missing
        # This is important if task_dict from model_dump(exclude_none=True) omits some fields.
        # However, the table schema allows NULLs for most, so direct insertion is fine.
        # For insert, we need to ensure the order of values matches columns or use named placeholders.

        columns = ', '.join(task_dict.keys())
        placeholders = ', '.join('?' for _ in task_dict)
        sql = f"INSERT INTO meeting_tasks ({columns}) VALUES ({placeholders})"

        try:
            await db.execute(sql, list(task_dict.values()))
            await db.commit()
            logger.info(f"Task {task.task_id} added to database.")
        except Exception as e:
            logger.error(f"Task {task.task_id}: Failed to add to database: {e}", exc_info=True)
            # Depending on policy, might re-raise or handle
            raise

async def get_task_from_db(task_id: str) -> Optional[MeetingTask]:
    async with get_db_connection() as db:
        fields_str = ", ".join(MEETING_TASK_DB_FIELDS)
        async with db.execute(f"SELECT {fields_str} FROM meeting_tasks WHERE task_id = ?", (task_id,)) as cursor:
            row = await cursor.fetchone()
            if row:
                return _db_row_to_task_model(row)
            return None

async def update_task_in_db(task: MeetingTask):
    async with get_db_connection() as db:
        task_dict = task.model_dump(exclude_none=False, exclude={"_audio_stream_object", "_audio_stream_task", "_audio_file_writer", "_stop_event"})

        if task_dict.get("start_time"):
            task_dict["start_time"] = task_dict["start_time"].isoformat() if isinstance(task_dict["start_time"], datetime.datetime) else task_dict["start_time"]
        if task_dict.get("end_time"):
            task_dict["end_time"] = task_dict["end_time"].isoformat() if isinstance(task_dict["end_time"], datetime.datetime) else task_dict["end_time"]

        # Ensure status is the string value of the enum
        task_dict["status"] = task.status.value if isinstance(task.status, Enum) else task.status

        # Prepare for UPDATE: remove task_id from fields to update, use it in WHERE
        update_fields = {k: v for k, v in task_dict.items() if k != "task_id"}
        set_clause = ', '.join(f"{key} = ?" for key in update_fields)
        values = list(update_fields.values())
        values.append(task.task_id) # For the WHERE clause

        sql = f"UPDATE meeting_tasks SET {set_clause} WHERE task_id = ?"

        try:
            await db.execute(sql, values)
            await db.commit()
            logger.debug(f"Task {task.task_id} updated in database. Status: {task.status}, Message: {task.message}")
        except Exception as e:
            logger.error(f"Task {task.task_id}: Failed to update in database: {e}", exc_info=True)
            # Depending on policy, might re-raise or handle
            raise

async def get_db_connection():
    # Ensure the data directory exists
    db_dir = os.path.dirname(DATABASE_URL)
    if not os.path.exists(db_dir):
        os.makedirs(db_dir, exist_ok=True)
        logger.info(f"Created database directory: {db_dir}")

    db = await aiosqlite.connect(DATABASE_URL)
    # Use Row factory to access columns by name
    db.row_factory = aiosqlite.Row
    return db

async def init_db():
    async with get_db_connection() as db:
        await db.execute("""
            CREATE TABLE IF NOT EXISTS meeting_tasks (
                task_id TEXT PRIMARY KEY,
                user_id TEXT NOT NULL,
                platform TEXT,
                meeting_id TEXT,
                audio_device_id TEXT,
                notion_page_title TEXT,
                status TEXT NOT NULL,
                message TEXT,
                start_time TEXT,
                end_time TEXT,
                duration_seconds INTEGER,
                transcript_preview TEXT,
                notes_preview TEXT,
                final_transcript_location TEXT,
                final_notes_location TEXT,
                audio_file_path TEXT
            )
        """)
        await db.commit()
        logger.info("Database initialized and meeting_tasks table ensured.")

async def handle_interrupted_tasks():
    """Marks tasks that were active during a restart as ERRORED."""
    logger.info("Checking for tasks interrupted by restart...")
    async with get_db_connection() as db:
        # Find tasks that were in a non-terminal state
        async with db.execute(
            "SELECT task_id FROM meeting_tasks WHERE status = ? OR status = ?",
            (TaskStatus.ACTIVE.value, TaskStatus.PROCESSING_COMPLETION.value)
        ) as cursor:
            interrupted_tasks = await cursor.fetchall()

            if interrupted_tasks:
                for row in interrupted_tasks:
                    task_id = row["task_id"]
                    logger.warning(f"Task {task_id} was interrupted by worker restart. Marking as ERROR.")
                    await db.execute(
                        "UPDATE meeting_tasks SET status = ?, message = ? WHERE task_id = ?",
                        (TaskStatus.ERROR.value, "Task interrupted due to worker restart.", task_id)
                    )
                await db.commit()
                logger.info(f"Marked {len(interrupted_tasks)} tasks as ERRORED due to restart.")
            else:
                logger.info("No interrupted tasks found.")

# In-memory store for active asyncio.Task objects (not for persistent task data)
# These are the actual running audio capture loops.
running_async_tasks: Dict[str, asyncio.Task] = {}
# In-memory store for stop events, associated with task_id. These are used to signal
# the corresponding asyncio.Task in running_async_tasks to stop.
task_stop_events: Dict[str, asyncio.Event] = {}
# The active_tasks dictionary is now removed as task state is managed in the database.


# --- Helper Functions ---

async def _get_audio_devices() -> List[AudioDevice]:
    """Synchronously gets audio devices and wraps in a list."""
    try:
        devices = sd.query_devices()
        logger.info(f"Raw devices found: {devices}")
        input_devices = []
        for i, device in enumerate(devices):
            # Heuristic: check for input channels or common input device names
            # This might need refinement based on how sounddevice reports on different OS
            # On some systems, default might be the only reliable one without deeper inspection
            is_input = False
            try:
                # Try to query if it's an input device by checking its max_input_channels
                if device.get('max_input_channels', 0) > 0:
                    is_input = True
                # Fallback for devices that don't report max_input_channels well but are common inputs
                elif any(keyword in device.get('name', '').lower() for keyword in ['mic', 'input', 'default']):
                     # Check if it's not an output device
                    if device.get('max_output_channels', 0) == 0:
                        is_input = True


            except Exception as e:
                logger.warning(f"Could not determine if device {device.get('name')} is input: {e}")


            if is_input:
                # Use the device index as ID if 'id' field is not directly available or suitable
                # Name is usually descriptive enough.
                # sd.query_devices() returns a list of dicts or a single dict if device is specified.
                # The 'index' or simply its position can serve as an ID.
                # For device selection in sd.InputStream, we might need index or name.
                # Let's use index for robustness if names aren't unique.
                device_id = device.get('index', i) # Prefer 'index' if present
                input_devices.append(AudioDevice(id=device_id, name=device['name']))

        if not input_devices and devices: # If no inputs specifically identified, but devices exist
            logger.warning("Could not specifically identify input devices. Falling back to listing all devices.")
            return [AudioDevice(id=i, name=d['name']) for i, d in enumerate(devices)]

        # Ensure 'default' is an option if available and makes sense
        # sd.default.device can give default input/output. sd.default.device[0] is input.
        try:
            default_input_idx = sd.default.device[0]
            if default_input_idx != -1 and not any(d.id == default_input_idx for d in input_devices):
                 default_device_info = sd.query_devices(default_input_idx)
                 input_devices.append(AudioDevice(id=default_input_idx, name=f"{default_device_info['name']} (Default Input)"))
            elif default_input_idx == -1 and not any('default' in d.name.lower() for d in input_devices):
                # If no system default, but we want a "default" option for simplicity for the user
                # This is tricky, as "default" for sounddevice means system default.
                # Perhaps the first enumerated input device can be a pragmatic "default" if no other default is found.
                pass


        except Exception as e:
            logger.warning(f"Could not query default input device: {e}")


        logger.info(f"Filtered input devices: {input_devices}")
        return input_devices
    except Exception as e:
        logger.error(f"Error querying audio devices: {e}", exc_info=True)
        # Fallback: Provide a mock device if listing fails, so frontend doesn't break.
        # This is crucial for environments where sounddevice might not have access (e.g. some CI/serverless without audio).
        return [AudioDevice(id="mock_input_device", name="Mock Input Device (Error listing real devices)")]


async def transcribe_audio_with_openai(audio_file_path: str, task_id: str) -> Optional[str]:
    """
    Transcribes the given audio file using OpenAI Whisper API.
    Returns the transcribed text or None if an error occurs or STT is disabled.
    """
    if not openai_client:
        logger.warning(f"Task {task_id}: OpenAI client not available. Skipping STT.")
        return None
    if not os.path.exists(audio_file_path):
        logger.error(f"Task {task_id}: Audio file not found at {audio_file_path} for STT.")
        return None

    logger.info(f"Task {task_id}: Starting STT processing for {audio_file_path} using OpenAI Whisper.")

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type((APIError, RateLimitError, APIConnectionError))
    )
    async def _transcribe_with_retry():
        try:
            with open(audio_file_path, "rb") as audio_file_object:
                transcript_response = await asyncio.to_thread(
                    openai_client.audio.transcriptions.create,
                    model="whisper-1",
                    file=audio_file_object
                )
            transcript = transcript_response.text
            logger.info(f"Task {task_id}: STT successful. Transcript length: {len(transcript)} chars.")
            return transcript
        except AuthenticationError as e:
            logger.error(f"Task {task_id}: OpenAI AuthenticationError during STT: {e}. Will not retry.", exc_info=True)
            # Non-retryable, re-raise to be caught by the outer handler or let it propagate if not caught.
            # For this function, we want to return None and log.
            raise # Will be caught by the outer try-except
        except APIError as e: # Includes RateLimitError, APIConnectionError, etc. that are retryable by tenacity
            logger.warning(f"Task {task_id}: OpenAI APIError during STT (will retry if applicable): {e}", exc_info=True)
            raise # Re-raise to trigger tenacity retry
        except Exception as e: # Catch other unexpected errors during the attempt
            logger.error(f"Task {task_id}: Unexpected error during STT attempt: {e}", exc_info=True)
            raise # Re-raise to be caught by the outer try-except

    try:
        return await _transcribe_with_retry()
    except AuthenticationError: # Already logged, just ensure None is returned
        return None
    except Exception as e: # This catches errors after retries are exhausted or non-retryable ones not AuthenticationError
        logger.error(f"Task {task_id}: OpenAI Whisper STT failed for {audio_file_path} after retries or due to non-retryable error: {e}", exc_info=True)
        return None

async def create_notion_page_with_transcript(
    task_id: str,
    page_title: str,
    transcript: str,
    # user_id: str # user_id currently not used for Notion client, assuming one API key for now
) -> Optional[str]:
    """
    Creates a new page in Notion with the given title and transcript content.
    Uses `notion_parent_page_id` from environment if set, otherwise creates in workspace root.
    Returns the URL of the created page or None if an error occurs or Notion is disabled.
    """
    if not notion_client:
        logger.warning(f"Task {task_id}: Notion client not available. Skipping Notion page creation.")
        return None

    logger.info(f"Task {task_id}: Preparing to create Notion page titled '{page_title}'.")

    content_blocks = []
    paragraphs = transcript.split('\n\n')
    if len(paragraphs) == 1 and '\n' in transcript:
        paragraphs = transcript.split('\n')

    for para in paragraphs:
        para = para.strip()
        if not para: continue
        for i in range(0, len(para), 1999):
            chunk = para[i:i+1999]
            content_blocks.append({
                "object": "block", "type": "paragraph",
                "paragraph": {"rich_text": [{"type": "text", "text": {"content": chunk}}]}
            })
            if len(content_blocks) >= 99: break
        if len(content_blocks) >= 99: break

    if not content_blocks:
        content_blocks.append({
            "object": "block", "type": "paragraph",
            "paragraph": {"rich_text": [{"type": "text", "text": {"content": "(Transcript was empty)"}}]}
        })

    page_properties = {"title": [{"type": "text", "text": {"content": page_title}}]}
    create_payload = {"parent": {}, "properties": page_properties, "children": content_blocks}

    if notion_parent_page_id:
        create_payload["parent"] = {"page_id": notion_parent_page_id}
    else:
        logger.warning(f"Task {task_id}: NOTION_PARENT_PAGE_ID not set. Attempting to create page in default location.")

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type(Exception) # Generic exception for network, will refine with specific Notion errors if possible
    )
    async def _create_page_with_retry():
        try:
            logger.info(f"Task {task_id}: Attempting to create Notion page (attempt {getattr(_create_page_with_retry, 'retry', {}).get('statistics', {}).get('attempt_number', 1)}).")
            created_page = await notion_client.pages.create(**create_payload)
            page_url = created_page.get("url")
            logger.info(f"Task {task_id}: Notion page created successfully: {page_url}")
            return page_url
        except APIResponseError as e:
            # Check if the error is retryable
            retryable_notion_errors = [
                APIErrorCode.RateLimited,
                APIErrorCode.InternalServerError,
                APIErrorCode.ServiceUnavailable, # Typically a 503
                # ConflictError might be retryable if it's due to eventual consistency, but often indicates a logic issue.
                # For now, not retrying on ConflictError unless specific use case arises.
            ]
            if e.code in retryable_notion_errors:
                logger.warning(f"Task {task_id}: Notion APIResponseError (retryable - {e.code.value}) while creating page: {e}. Will retry.", exc_info=True)
                raise # Re-raise to trigger tenacity retry
            else:
                logger.error(f"Task {task_id}: Notion APIResponseError (non-retryable - {e.code.value}) while creating page: {e}", exc_info=True)
                raise # Re-raise to be caught by outer try-except as non-retryable
        except Exception as e: # Other exceptions like network issues
            logger.warning(f"Task {task_id}: Unexpected error during Notion page creation (will retry): {e}", exc_info=True)
            raise # Re-raise to trigger tenacity retry

    try:
        return await _create_page_with_retry()
    except APIResponseError as e: # Non-retryable APIResponseError after check inside retry func
        # Error already logged with specifics
        return None
    except Exception as e: # Catches errors after retries are exhausted or other non-APIResponseErrors
        logger.error(f"Task {task_id}: Failed to create Notion page after retries or due to non-retryable error: {e}", exc_info=True)
        return None


async def append_to_notion_page(page_id: str, task_id: str, content_blocks: List[Dict]) -> bool:
    """
    Appends a list of content blocks to an existing Notion page.
    Returns True on success, False on failure or if Notion is disabled.
    """
    if not notion_client:
        logger.warning(f"Task {task_id}: Notion client not available. Skipping append to page {page_id}.")
        return False
    if not content_blocks:
        logger.info(f"Task {task_id}: No content blocks to append to page {page_id}.")
        return True

    logger.info(f"Task {task_id}: Appending {len(content_blocks)} blocks to Notion page {page_id}.")

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type(Exception) # Generic for now, refined inside
    )
    async def _append_blocks_with_retry(block_batch: List[Dict]):
        try:
            attempt_num = getattr(_append_blocks_with_retry, 'retry', {}).get('statistics', {}).get('attempt_number', 1)
            logger.info(f"Task {task_id}: Attempting to append {len(block_batch)} blocks to Notion page {page_id} (attempt {attempt_num}).")
            await notion_client.blocks.children.append(block_id=page_id, children=block_batch)
        except APIResponseError as e:
            retryable_notion_errors = [
                APIErrorCode.RateLimited, APIErrorCode.InternalServerError, APIErrorCode.ServiceUnavailable
            ]
            if e.code in retryable_notion_errors:
                logger.warning(f"Task {task_id}: Notion APIResponseError (retryable - {e.code.value}) while appending to page {page_id}: {e}. Will retry batch.", exc_info=True)
                raise
            else:
                logger.error(f"Task {task_id}: Notion APIResponseError (non-retryable - {e.code.value}) while appending to page {page_id}: {e}", exc_info=True)
                raise # Non-retryable, caught by outer try-except
        except Exception as e:
            logger.warning(f"Task {task_id}: Unexpected error during Notion page append (will retry batch): {e}", exc_info=True)
            raise

    try:
        for i in range(0, len(content_blocks), 100):
            batch = content_blocks[i:i+100]
            await _append_blocks_with_retry(batch) # Apply retry to each batch
            if len(content_blocks) > 100:
                logger.info(f"Task {task_id}: Appended batch starting at index {i} to Notion page {page_id}.")
                await asyncio.sleep(0.5) # Small delay if multiple batches

        logger.info(f"Task {task_id}: Successfully appended all blocks to Notion page {page_id}.")
        return True
    except APIResponseError as e: # Non-retryable APIResponseError
        # Error already logged from within retry logic
        return False
    except Exception as e: # Catches errors after retries on a batch are exhausted
        logger.error(f"Task {task_id}: Failed to append blocks to Notion page {page_id} after retries: {e}", exc_info=True)
        return False

async def _call_openai_chat_completion(task_id: str, system_message: str, user_message: str, model: str = "gpt-3.5-turbo") -> Optional[str]:
    """Helper function to call OpenAI Chat Completion API."""
    if not openai_client:
        logger.warning(f"Task {task_id}: OpenAI client not available. Skipping LLM call.")
        return None

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type((APIError, RateLimitError, APIConnectionError))
    )
    async def _chat_completion_with_retry():
        try:
            logger.info(f"Task {task_id}: Calling OpenAI Chat Completion API with model {model} (attempt {getattr(_chat_completion_with_retry, 'retry', {}).get('statistics', {}).get('attempt_number', 1)}).")
            response = await asyncio.to_thread(
                openai_client.chat.completions.create,
                model=model,
                messages=[
                    {"role": "system", "content": system_message},
                    {"role": "user", "content": user_message},
                ],
                temperature=0.3, # Lower temperature for more factual/deterministic output
            )
            content = response.choices[0].message.content
            logger.info(f"Task {task_id}: LLM call successful. Response length: {len(content or '')} chars.")
            return content.strip() if content else None
        except AuthenticationError as e:
            logger.error(f"Task {task_id}: OpenAI AuthenticationError during chat completion: {e}. Will not retry.", exc_info=True)
            raise
        except APIError as e: # Retryable errors
            logger.warning(f"Task {task_id}: OpenAI APIError during chat completion (will retry if applicable): {e}", exc_info=True)
            raise
        except Exception as e: # Other unexpected errors
            logger.error(f"Task {task_id}: Unexpected error during chat completion attempt: {e}", exc_info=True)
            raise

    try:
        return await _chat_completion_with_retry()
    except AuthenticationError: # Already logged
        return None
    except Exception as e: # Catches errors after retries or non-retryable ones
        logger.error(f"Task {task_id}: OpenAI Chat Completion API call failed for model {model} after retries or due to non-retryable error: {e}", exc_info=True)
        return None

async def get_llm_summary(transcript: str, task_id: str) -> Optional[str]:
    """Generates a summary of the transcript using an LLM."""
    logger.info(f"Task {task_id}: Requesting LLM summary for transcript (length: {len(transcript)}).")
    system_message = "You are a helpful AI assistant tasked with summarizing meeting transcripts. Provide a concise summary that captures the main topics discussed, key insights, and overall outcome of the meeting."
    user_message = f"Please provide a concise summary of the following meeting transcript:\n\n<transcript>\n{transcript}\n</transcript>\n\nSummary:"

    summary = await _call_openai_chat_completion(task_id, system_message, user_message)
    if not summary or summary.lower().strip() == "no summary available." or len(summary.strip()) < 10 : # Basic check for empty/useless summary
        logger.warning(f"Task {task_id}: LLM generated an empty or trivial summary.")
        return None
    return summary

async def get_llm_decisions(transcript: str, task_id: str) -> Optional[str]:
    """Extracts key decisions from the transcript using an LLM."""
    logger.info(f"Task {task_id}: Requesting LLM to extract key decisions (transcript length: {len(transcript)}).")
    system_message = "You are an AI assistant specializing in extracting key decisions from meeting transcripts. List the decisions made during the meeting as a bulleted list. Each bullet point should represent a distinct decision. If no clear decisions are found, state 'No specific decisions were identified.'"
    user_message = f"Extract the key decisions made from the following meeting transcript. Present them as a bulleted list. Each bullet point should represent a distinct decision.\n\n<transcript>\n{transcript}\n</transcript>\n\nKey Decisions:\n-"

    decisions_text = await _call_openai_chat_completion(task_id, system_message, user_message)
    if not decisions_text or "no specific decisions were identified" in decisions_text.lower() or len(decisions_text.strip()) < 5:
        logger.info(f"Task {task_id}: LLM reported no specific decisions or an empty list.")
        return None
    # Prepend the leading dash if the model didn't include it and it's a list
    if not decisions_text.strip().startswith("-"):
        return f"- {decisions_text.strip()}"
    return decisions_text.strip()


async def get_llm_action_items(transcript: str, task_id: str) -> Optional[str]:
    """Extracts action items from the transcript using an LLM."""
    logger.info(f"Task {task_id}: Requesting LLM to extract action items (transcript length: {len(transcript)}).")
    system_message = "You are an AI assistant skilled at identifying action items from meeting transcripts. List all action items discussed. For each action item, if an assignee or owner is mentioned, please include that. If no action items are found, state 'No specific action items were identified.'"
    user_message = f"Identify all action items from the following meeting transcript. Present them as a bulleted list. If possible, for each action item, specify the person assigned or responsible.\n\nFormat example:\n- [Action Item Description] (Assigned to: [Person's Name, if mentioned, otherwise 'N/A'])\n\n<transcript>\n{transcript}\n</transcript>\n\nAction Items:\n-"

    action_items_text = await _call_openai_chat_completion(task_id, system_message, user_message)
    if not action_items_text or "no specific action items were identified" in action_items_text.lower() or len(action_items_text.strip()) < 5:
        logger.info(f"Task {task_id}: LLM reported no specific action items or an empty list.")
        return None
    if not action_items_text.strip().startswith("-"):
        return f"- {action_items_text.strip()}"
    return action_items_text.strip()


# Renamed from audio_capture_placeholder
async def audio_capture_loop(task: MeetingTask):
    """Handles actual audio capture using sounddevice and saves to a WAV file."""
    task._stop_event = asyncio.Event()

    # Define the callback function for the audio stream
    def audio_callback(indata: np.ndarray, frames: int, time_info, status_flags: sd.CallbackFlags):
        if status_flags:
            logger.warning(f"Task {task.task_id}: Audio callback status flags: {status_flags}")
            # Potentially update task.message or handle specific errors like input overflow
            if status_flags.input_overflow:
                task.message = "Warning: Audio input overflow detected."
            # Add more specific error handling based on flags if needed

        if task._stop_event and task._stop_event.is_set():
            logger.info(f"Task {task.task_id}: Stop event received in audio callback. Raising StopException.")
            raise sd.CallbackStop # Signal sounddevice to stop the stream

        try:
            if task._audio_file_writer:
                task._audio_file_writer.writeframes(indata.tobytes())
            # Update previews less frequently to avoid too much logging / state update churn
            if task.duration_seconds and int(task.duration_seconds) % 5 == 0 :
                 task.transcript_preview = f"Captured {int(task.duration_seconds)}s of audio..."
        except Exception as e:
            logger.error(f"Task {task.task_id}: Error in audio_callback: {e}", exc_info=True)
            # Potentially set task to error state here if callback errors are critical
            task.status = TaskStatus.ERROR
            task.message = f"Error during audio data handling: {str(e)}"
            if task._stop_event: task._stop_event.set() # Signal stop on critical error
            raise sd.CallbackStop


    try:
        task.start_time = datetime.datetime.now(datetime.timezone.utc)

        # Ensure TEMP_AUDIO_DIR exists
        os.makedirs(TEMP_AUDIO_DIR, exist_ok=True)
        task._audio_file_path = os.path.join(TEMP_AUDIO_DIR, f"{task.task_id}_audio.wav")

        # Setup WAV file writer
        task._audio_file_writer = wave.open(task._audio_file_path, 'wb')
        task._audio_file_writer.setnchannels(CHANNELS)
        task._audio_file_writer.setsampwidth(np.dtype(DTYPE).itemsize) # itemsize gives bytes per sample
        task._audio_file_writer.setframerate(SAMPLE_RATE)

        logger.info(f"Task {task.task_id}: Starting audio capture for device '{task.audio_device_id}'. Saving to {task._audio_file_path}")
        task.status = TaskStatus.ACTIVE
        task.message = "Audio capture initiated."

        # Attempt to parse audio_device_id if it's a string that looks like an int
        device_id_to_use = task.audio_device_id
        if isinstance(task.audio_device_id, str):
            try:
                device_id_to_use = int(task.audio_device_id)
            except ValueError:
                # If it's not 'default' or an int-like string, sounddevice might handle it as a name substring.
                # Or, you might want to explicitly query devices again to find by name if needed.
                logger.info(f"Task {task.task_id}: audio_device_id '{task.audio_device_id}' is a string, using as is.")

        blocksize_frames = int(SAMPLE_RATE * BLOCK_DURATION_MS / 1000)

        with sd.InputStream(
            samplerate=SAMPLE_RATE,
            device=device_id_to_use,
            channels=CHANNELS,
            dtype=DTYPE,
            callback=audio_callback,
            blocksize=blocksize_frames # Number of frames per callback
        ) as stream:
            task._audio_stream_object = stream
            logger.info(f"Task {task.task_id}: Audio stream opened. Sample rate: {stream.samplerate}, Channels: {stream.channels}, Device: {stream.device}")
            task.message = "Audio capture active."

            # Keep the stream alive until stop_event is set
            while not task._stop_event.is_set():
                # Update duration
                if task.start_time: # Should always be set here
                    task.duration_seconds = (datetime.datetime.now(datetime.timezone.utc) - task.start_time).total_seconds()

                # Check if task was externally marked as error or completed to break loop
                if task.status != TaskStatus.ACTIVE:
                    logger.info(f"Task {task.task_id}: Status changed to {task.status}, stopping capture loop.")
                    task._stop_event.set() # Ensure callback knows to stop
                    break

                await asyncio.sleep(0.1) # Sleep briefly to yield control and check stop_event

            logger.info(f"Task {task.task_id}: Audio capture loop finished.")

    except sd.PortAudioError as pae:
        logger.error(f"Task {task.task_id}: PortAudioError during audio capture setup: {pae}", exc_info=True)
        task.status = TaskStatus.ERROR
        task.message = f"Audio device error: {str(pae)}. Ensure device '{task.audio_device_id}' is valid and available."
    except FileNotFoundError as fnfe: # If temp directory is an issue, though makedirs should handle it
        logger.error(f"Task {task.task_id}: File error during WAV setup: {fnfe}", exc_info=True)
        task.status = TaskStatus.ERROR
        task.message = f"File system error: {str(fnfe)}."
    except Exception as e:
        logger.error(f"Task {task.task_id}: Unexpected error during audio capture: {e}", exc_info=True)
        task.status = TaskStatus.ERROR
        task.message = f"Unexpected error during audio capture: {str(e)}"
    finally:
        logger.info(f"Task {task.task_id}: Cleaning up audio capture resources.")
        if task._audio_stream_object and not task._audio_stream_object.closed:
            # This might already be closed if sd.CallbackStop was raised or context exited
            try:
                task._audio_stream_object.stop() # Ensure stream is stopped
                task._audio_stream_object.close()
                logger.info(f"Task {task.task_id}: Audio stream explicitly stopped and closed.")
            except Exception as e_close:
                logger.error(f"Task {task.task_id}: Error closing audio stream: {e_close}", exc_info=True)

        if task._audio_file_writer:
            try:
                task._audio_file_writer.close()
                logger.info(f"Task {task.task_id}: WAV file closed: {task._audio_file_path}")
                if task.status not in [TaskStatus.ERROR] and os.path.exists(task._audio_file_path or ""):
                     task.final_transcript_location = task._audio_file_path # Using this field for audio file path for now
                     task.message = task.message + f" Audio saved to {task._audio_file_path}."
                elif task._audio_file_path and os.path.exists(task._audio_file_path): # If error, but file exists, maybe keep it
                    logger.warning(f"Task {task.task_id}: Task errored but audio file exists at {task._audio_file_path}")
                    task.final_transcript_location = task._audio_file_path # Still note its location
            except Exception as e_close_wav:
                logger.error(f"Task {task.task_id}: Error closing WAV file: {e_close_wav}", exc_info=True)
                if task.status != TaskStatus.ERROR: # Avoid overwriting a more specific error
                    task.status = TaskStatus.ERROR
                    task.message = "Error finalizing audio file."


        task.end_time = datetime.datetime.now(datetime.timezone.utc)
        if task.start_time: # Should always be set
            task.duration_seconds = (task.end_time - task.start_time).total_seconds()

        if task.status == TaskStatus.ACTIVE: # If loop finished due to _stop_event but not an error
            task.status = TaskStatus.PROCESSING_COMPLETION
            task.message = "Audio capture stopped. Preparing for STT."

        # Attempt STT if audio capture was successful (or processing_completion) and file exists
        if task.status in [TaskStatus.PROCESSING_COMPLETION, TaskStatus.ACTIVE] and \
           task._audio_file_path and os.path.exists(task._audio_file_path):

            logger.info(f"Task {task.task_id}: Proceeding to STT for {task._audio_file_path}.")
            task.message = "Audio captured. Starting transcription..."
            transcript = await transcribe_audio_with_openai(task._audio_file_path, task.task_id)
            if transcript:
                task.transcript_preview = transcript # Store full transcript here for now
                task.message = "Transcription successful."
                logger.info(f"Task {task.task_id}: Transcription successful.")

                logger.info(f"Task {task.task_id}: Attempting to create Notion page with title '{task.notion_page_title}' for transcript.")
                page_url = await create_notion_page_with_transcript(
                    task_id=task.task_id,
                    page_title=task.notion_page_title,
                    transcript=transcript
                )

                if page_url:
                    task.final_notes_location = page_url
                    task.message += f" Transcript saved to Notion: {page_url}."
                    logger.info(f"Task {task.task_id}: Successfully saved transcript to Notion: {page_url}")

                    notion_page_id_for_appends = None
                    try: # Extract page_id from URL for appending
                        notion_page_id_for_appends = page_url.split('/')[-1].split('-')[-1]
                        if not notion_page_id_for_appends or len(notion_page_id_for_appends) < 32:
                             notion_page_id_for_appends = page_url.split('-')[-1]
                             if not notion_page_id_for_appends or len(notion_page_id_for_appends) < 32:
                                notion_page_id_for_appends = None
                                logger.error(f"Task {task.task_id}: Could not reliably extract page_id from URL {page_url} for appends.")
                    except Exception as e_parse_id:
                        logger.error(f"Task {task.task_id}: Error parsing page_id from URL {page_url}: {e_parse_id}")
                        notion_page_id_for_appends = None

                    if notion_page_id_for_appends:
                        # LLM processing and appending to Notion
                        summary = await get_llm_summary(transcript, task.task_id)
                        if summary:
                            task.notes_preview = summary[:200] + "..." if len(summary) > 200 else summary
                            summary_blocks = [
                                {"type": "heading_2", "heading_2": {"rich_text": [{"type": "text", "text": {"content": "Summary"}}]}},
                                {"type": "paragraph", "paragraph": {"rich_text": [{"type": "text", "text": {"content": summary}}]}}
                            ]
                            if await append_to_notion_page(notion_page_id_for_appends, task.task_id, summary_blocks):
                                task.message += " Summary added to Notion."
                            else:
                                task.message += " Failed to add summary to Notion. Some content may be missing."

                        decisions = await get_llm_decisions(transcript, task.task_id)
                        if decisions:
                            decision_blocks = [
                                {"type": "heading_2", "heading_2": {"rich_text": [{"type": "text", "text": {"content": "Key Decisions"}}]}},
                                *[{"type": "bulleted_list_item", "bulleted_list_item": {"rich_text": [{"type": "text", "text": {"content": item.strip("- ")}}]}} for item in decisions.split('\n') if item.strip("- ")]
                            ]
                            if await append_to_notion_page(notion_page_id_for_appends, task.task_id, decision_blocks):
                                task.message += " Decisions added to Notion."
                            else:
                                task.message += " Failed to add decisions to Notion. Some content may be missing."

                        action_items = await get_llm_action_items(transcript, task.task_id)
                        if action_items:
                            action_item_blocks = [
                                {"type": "heading_2", "heading_2": {"rich_text": [{"type": "text", "text": {"content": "Action Items"}}]}},
                                *[{"type": "bulleted_list_item", "bulleted_list_item": {"rich_text": [{"type": "text", "text": {"content": item.strip("- ")}}]}} for item in action_items.split('\n') if item.strip("- ")]
                            ]
                            if await append_to_notion_page(notion_page_id_for_appends, task.task_id, action_item_blocks):
                                task.message += " Action items added to Notion."
                            else:
                                task.message += " Failed to add action items to Notion. Some content may be missing."

                        task.status = TaskStatus.COMPLETED
                        task.message += " Task completed."
                    else: # Could not extract page_id for appends
                        task.status = TaskStatus.ERROR # Or COMPLETED_WITH_ISSUES if we add such a status
                        task.message += " Could not extract Notion page ID for appends. LLM notes not added."
                        logger.error(f"Task {task.task_id}: Could not extract Notion page_id from {page_url}. LLM content not appended.")
                else: # Initial Notion page creation failed
                    task.status = TaskStatus.ERROR
                    task.message = "Transcription successful, but failed to create Notion page. LLM analysis not performed or stored."
                    logger.error(f"Task {task.task_id}: Failed to create initial Notion page for transcript. Transcript available in preview.")
            else: # Transcription failed
                task.status = TaskStatus.ERROR
                task.message = "Transcription failed after retries. No Notion page created or LLM analysis performed."
                logger.error(f"Task {task.task_id}: Transcription failed for {task._audio_file_path}. Further processing halted.")

        elif task.status == TaskStatus.ERROR: # Error occurred before STT/Notion/LLM part
            logger.error(f"Task {task.task_id}: Task ended with error before STT/Notion/LLM processing. Status: {task.status}, Message: {task.message}")
        else: # Should not happen if status was ACTIVE or PROCESSING_COMPLETION before this block
            logger.warning(f"Task {task.task_id}: Task in unexpected state {task.status} at STT/Notion/LLM phase. Marking as error.")
            task.status = TaskStatus.ERROR
            task.message = (task.message or "") + " Task finalized with error due to unexpected state before STT."

        logger.info(f"Task {task.task_id}: Final processing complete. Final status for DB: {task.status}")

        # Clean up internal attributes not meant for external model
        task._audio_stream_object = None
        task._audio_file_writer = None
        task._stop_event = None

        # Attempt to delete the temporary audio file after processing is complete
        if task._audio_file_path and os.path.exists(task._audio_file_path):
            try:
                os.remove(task._audio_file_path)
                logger.info(f"Task {task.task_id}: Temporary audio file {task._audio_file_path} deleted successfully.")
                # If we successfully delete it, we might want to clear task.final_transcript_location
                # if it solely pointed to the audio file and the transcript is now in task.transcript_preview.
                # However, for now, keeping it as a record of where the audio *was* is fine.
                # If STT was successful, the transcript is in task.transcript_preview.
            except OSError as e:
                logger.error(f"Task {task.task_id}: Error deleting temporary audio file {task._audio_file_path}: {e}", exc_info=True)

        # task._audio_file_path can be cleared now, as the file is deleted or attempt was made.
        # This also prevents re-deletion attempts if this finally block were ever re-entered (though it shouldn't).
        task._audio_file_path = None

        # Persist all final changes to the task to the database
        try:
            await update_task_in_db(task)
            logger.info(f"Task {task.task_id}: Final state saved to DB in audio_capture_loop. Status: {task.status}")
        except Exception as e_db_update:
            logger.error(f"Task {task.task_id}: Critical error: Failed to save final task state to DB in audio_capture_loop: {e_db_update}", exc_info=True)
            # This is a significant issue, as the DB might not reflect the task's true end state.
            # Depending on policy, could try a retry mechanism or ensure this is heavily monitored.


# --- FastAPI Application ---

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup logic
    logger.info("Live Meeting Worker starting up...")
    await init_db()
    await handle_interrupted_tasks()
    # You could pre-load other models or connect to other databases here if needed
    yield
    # Shutdown logic
    logger.info("Live Meeting Worker shutting down...")
    # Clean up any ongoing asyncio tasks (which are in-memory)
    for task_id, async_task_obj in list(running_async_tasks.items()):
        if async_task_obj and not async_task_obj.done():
            logger.info(f"Cancelling ongoing asyncio task {task_id} during shutdown.")
            async_task_obj.cancel()
            try:
                await async_task_obj # Give it a chance to clean up
            except asyncio.CancelledError:
                logger.info(f"Asyncio task {task_id} cancelled successfully during shutdown.")
            except Exception as e:
                logger.error(f"Error during cancellation of asyncio task {task_id} on shutdown: {e}")
        # The database record for this task should have already been updated by handle_interrupted_tasks
        # or by the task itself if it completed/errored before shutdown.
        # If we want to be absolutely sure, we could re-query and update DB status here too,
        # but handle_interrupted_tasks on next startup is the primary mechanism for DB state correction.

    logger.info("Live Meeting Worker shutdown complete.")


app = FastAPI(
    title="Live Meeting Attendance Worker",
    description="API for managing live meeting audio capture, transcription, and note-taking.",
    version="0.1.0",
    lifespan=lifespan
)

# --- API Endpoints ---

@app.get("/list_audio_devices", response_model=AudioDeviceList)
async def list_audio_devices():
    """
    Retrieves a list of available audio input devices.
    """
    logger.info("Request received for /list_audio_devices")
    try:
        # Running sounddevice in a thread pool executor as it can be blocking
        devices = await asyncio.to_thread(_get_audio_devices)
        return AudioDeviceList(devices=devices)
    except Exception as e:
        logger.error(f"Failed to list audio devices: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to retrieve audio devices: {str(e)}")

@app.post("/start_meeting_attendance", response_model=StartMeetingResponse, status_code=202)
async def start_meeting_attendance(
    request: StartMeetingRequest,
    background_tasks: BackgroundTasks
):
    """
    Initiates the process of attending a meeting.
    """
    logger.info(f"Request received for /start_meeting_attendance: {request.model_dump_json()}")
    task_id = str(uuid.uuid4())

    # Validate audio_device_id (optional, could do a quick check if it's in the list)
    # For now, we assume the frontend sends a valid one from /list_audio_devices

    task = MeetingTask(
        task_id=task_id,
        user_id=request.user_id,
        platform=request.platform,
        meeting_id=request.meeting_id,
        audio_device_id=request.audio_device_id,
        notion_page_title=request.notion_page_title,
        status=TaskStatus.PENDING,
        message="Task accepted and initializing."
    )

    try:
        await add_task_to_db(task)
        logger.info(f"Task {task_id} created and saved to DB for user {request.user_id}. Status: {task.status}")
    except Exception as e:
        logger.error(f"Task {task_id}: Failed to save initial task to database: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to initialize task in database.")

    # Start the audio capture loop in the background using asyncio.create_task.
    # The audio_capture_loop function will update the task object (fetched from DB or passed)
    # and persist these updates to the database.
    # We pass a *copy* of the task object or relevant details if audio_capture_loop fetches its own state.
    # For now, passing the task object; audio_capture_loop will use it and update DB.
    # The asyncio task itself is stored in running_async_tasks to manage its lifecycle (e.g., for stopping).

    # Create the asyncio.Event for this task
    stop_event = asyncio.Event()
    task_stop_events[task_id] = stop_event
    # The MeetingTask model itself doesn't store the event or the asyncio.Task object,
    # as these are runtime constructs not suitable for DB persistence.
    # We will pass the stop_event to the audio_capture_loop.

    # The `task` object here is Pydantic model. We need to ensure audio_capture_loop uses this instance
    # or fetches a fresh one from DB.
    # For simplicity, audio_capture_loop can receive this `task` instance and be responsible for all DB updates.
    # Crucially, `task._stop_event` needs to be set on this instance if `audio_capture_loop` uses it directly.

    # Re-fetch task from DB to ensure we pass the persisted version if add_task_to_db modified it (e.g. defaults)
    # However, our current add_task_to_db doesn't modify the passed task object in place in a way that's critical here.
    # The main thing is that `audio_capture_loop` must use `update_task_in_db`.

    # Assign the stop event to the task object (will not be persisted)
    task._stop_event = stop_event

    capture_task_async = asyncio.create_task(audio_capture_loop(task))
    running_async_tasks[task_id] = capture_task_async
    # task._audio_stream_task = capture_task_async # This was for the model field, now managed by running_async_tasks

    return StartMeetingResponse(
        task_id=task_id,
        status=task.status, # Will be PENDING initially from the model
        message=task.message
    )

@app.get("/meeting_attendance_status/{task_id}", response_model=MeetingTask)
async def get_meeting_attendance_status(
    task_id: str = Path(..., description="The ID of the task to query")
):
    """
    Polls for the current status and progress of an active meeting attendance task.
    Fetches status directly from the database.
    """
    logger.debug(f"Request received for /meeting_attendance_status/{task_id}")
    task = await get_task_from_db(task_id)

    if not task:
        logger.warning(f"Task not found in DB: {task_id}")
        raise HTTPException(status_code=404, detail=f"Task not found: {task_id}")

    # If the task is marked as ACTIVE in the DB, and we have a running asyncio task for it,
    # we can dynamically update its duration based on the actual start time.
    # The DB record's duration_seconds is only updated when the task stops or errors.
    if task.status == TaskStatus.ACTIVE and task.start_time and task_id in running_async_tasks:
        # Ensure the running_async_tasks dict is for the current, live asyncio tasks
        # and not just leftover from a previous run if not cleaned properly (though lifespan should handle it).
        current_duration = (datetime.datetime.now(datetime.timezone.utc) - task.start_time).total_seconds()
        task.duration_seconds = current_duration # Update for the response, not persisted here

    logger.debug(f"Returning status for task {task_id} from DB: {task.status}, message: {task.message}")
    return task

@app.post("/stop_meeting_attendance/{task_id}", response_model=MeetingTask)
async def stop_meeting_attendance(
    task_id: str = Path(..., description="The ID of the task to stop")
):
    """
    Manually stops an ongoing meeting attendance task.
    Updates status in the database.
    """
    logger.info(f"Request received for /stop_meeting_attendance/{task_id}")
    task = await get_task_from_db(task_id)

    if not task:
        logger.warning(f"Task not found in DB for stopping: {task_id}")
        raise HTTPException(status_code=404, detail=f"Task not found: {task_id}")

    if task.status not in [TaskStatus.ACTIVE, TaskStatus.PENDING]:
        logger.warning(f"Task {task_id} is not active or pending (status: {task.status}), cannot stop. Returning current state.")
        # Return the current state from DB, as it's already in a terminal or non-stoppable state.
        return task

    # Retrieve the stop event and the asyncio task
    stop_event = task_stop_events.get(task_id)
    async_task_obj = running_async_tasks.get(task_id)

    original_status = task.status # Keep original status for logging/logic

    if stop_event and not stop_event.is_set():
        logger.info(f"Task {task_id}: Signaling audio capture to stop via event.")
        task.status = TaskStatus.PROCESSING_COMPLETION # Optimistic: will be completed by loop
        task.message = "Stop request received. Finalizing task..."
        stop_event.set() # Signal the audio_capture_loop to stop

        if async_task_obj and not async_task_obj.done():
            try:
                # Give the audio_capture_loop some time to finish gracefully
                await asyncio.wait_for(async_task_obj, timeout=15.0) # Increased timeout
                logger.info(f"Task {task_id}: Audio capture loop completed after stop signal.")
                # The audio_capture_loop should have updated the task's status in DB to COMPLETED or ERROR.
                # Re-fetch the task to get its final state.
                task = await get_task_from_db(task_id) or task # Fallback to current task if somehow not found
            except asyncio.TimeoutError:
                logger.error(f"Task {task_id}: Timeout waiting for audio capture loop to complete. Forcing cancellation.")
                async_task_obj.cancel()
                task.status = TaskStatus.ERROR
                task.message = "Error: Timeout during stop. Audio capture forcibly cancelled."
                if not task.end_time: task.end_time = datetime.datetime.now(datetime.timezone.utc)
            except Exception as e:
                logger.error(f"Task {task_id}: Error waiting for audio capture loop completion: {e}", exc_info=True)
                task.status = TaskStatus.ERROR
                task.message = f"Error during task stop: {str(e)}"
                if not task.end_time: task.end_time = datetime.datetime.now(datetime.timezone.utc)
        else: # No running async task, or it was already done
             logger.info(f"Task {task_id}: No active async task to wait for, or task was already done. Current DB status: {task.status}")
             if original_status == TaskStatus.PENDING: # If it was pending and never really started
                 task.status = TaskStatus.COMPLETED # Or perhaps a new "CANCELLED" status
                 task.message = "Task stopped before full activation."
                 if not task.end_time: task.end_time = datetime.datetime.now(datetime.timezone.utc)
             # If it was ACTIVE but async task is done, audio_capture_loop's finally should handle DB update.
             # Re-fetch to be sure.
             refetched_task = await get_task_from_db(task_id)
             if refetched_task: task = refetched_task

    elif not stop_event and original_status == TaskStatus.PENDING :
        # Task was PENDING, never fully started (no stop_event created, no async task)
        logger.info(f"Task {task_id}: Was PENDING and no stop event/async task found. Marking as completed/cancelled.")
        task.status = TaskStatus.COMPLETED # Or CANCELLED
        task.message = "Task stopped while pending and before async execution."
        if not task.end_time: task.end_time = datetime.datetime.now(datetime.timezone.utc)

    else: # No stop event, or already set, or no async task associated (should not happen if task is ACTIVE)
        logger.info(f"Task {task_id}: No active audio capture process to signal stop, or already stopping/stopped. Current DB status: {task.status}")
        # If it's already processing completion, completed, or error, that's fine.
        # If it was PENDING and somehow missed above, ensure it's finalized.
        if task.status == TaskStatus.PENDING:
            task.status = TaskStatus.COMPLETED
            task.message = "Task stopped before activation (final check)."
            if not task.end_time: task.end_time = datetime.datetime.now(datetime.timezone.utc)


    # Calculate duration if it hasn't been set by the loop's finalization
    if task.start_time and task.end_time and not task.duration_seconds:
        task.duration_seconds = (task.end_time - task.start_time).total_seconds()
    elif task.start_time and not task.end_time and task.status in [TaskStatus.COMPLETED, TaskStatus.ERROR]:
        # If loop somehow didn't set end_time but task is terminal
        task.end_time = datetime.datetime.now(datetime.timezone.utc)
        task.duration_seconds = (task.end_time - task.start_time).total_seconds()


    # Persist the final state determined by this stop endpoint logic
    try:
        await update_task_in_db(task)
        logger.info(f"Task {task_id} stop processed. Final status in DB: {task.status}")
    except Exception as e:
        logger.error(f"Task {task_id}: Failed to update task status in DB during stop: {e}", exc_info=True)
        # The task object returned might not reflect the DB state if this fails.

    # Clean up from runtime tracking dictionaries
    running_async_tasks.pop(task_id, None)
    task_stop_events.pop(task_id, None)

    return task


if __name__ == "__main__":
    import uvicorn
    # This is for direct execution. `uvicorn main:app --reload` is preferred for dev.
    uvicorn.run(app, host="0.0.0.0", port=8001, log_level="info")

```
