import openai
import json
from serpapi import GoogleSearch, SerpApiClientException
from project.functions import note_utils
import os
import lancedb
from dotenv import load_dotenv
import uuid
from datetime import datetime, timezone

from openai import OpenAI # Ensure OpenAI class is imported

# --- LanceDB Setup ---
load_dotenv() # Load environment variables from .env file

LANCEDB_URI = os.getenv("LANCEDB_RESEARCH_DB_URI", "../../lance_db/ltm_agent_data.lance") # Adjusted path for typical local dev
DEFAULT_VECTOR_DIMENSION_PY = 1536
OPENAI_API_KEY_GLOBAL = os.getenv("OPENAI_API_KEY") # Used by other LLM calls
SERPAPI_API_KEY_GLOBAL = os.getenv("SERPAPI_API_KEY") # Load once

import logging # Import standard logging

# Module-level OpenAI client for embeddings
openai_api_key_py = os.getenv("OPENAI_API_KEY")
openai_client_py = None

# Configure logger for this module
logger = logging.getLogger(__name__)
# Basic config could be set at application entry point. If this module is run standalone,
# basicConfig would be needed here for logs to show up without explicit handler setup.
# Example: logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

if openai_api_key_py:
    openai_client_py = OpenAI(api_key=openai_api_key_py)
    logger.info("OpenAI client for Python embeddings initialized.")
else:
    logger.warning("[ResearchAgent] OPENAI_API_KEY not found for Python. Real embeddings will not be generated by research_agent.py.")

_db_connection = None

def get_lance_db_connection():
    """Establishes or returns existing LanceDB connection."""
    global _db_connection
    if _db_connection:
        return _db_connection
    try:
        logger.info(f"Connecting to LanceDB at URI: {LANCEDB_URI}")
        _db_connection = lancedb.connect(LANCEDB_URI)
        logger.info("Successfully connected to LanceDB.")
        return _db_connection
    except Exception as e:
        logger.error(f"Error connecting to LanceDB: {e}", exc_info=True)
        return None

def generate_embedding_py(text: str) -> list[float] | None:
    """Generates embeddings using OpenAI API for Python."""
    if not openai_client_py:
        logger.error("[ResearchAgent] OpenAI client (Python) not initialized. Cannot generate real embedding.")
        return None

    if not text or not text.strip():
        logger.warning("[ResearchAgent] generate_embedding_py called with empty or whitespace-only text. Returning None.")
        return None

    try:
        trimmed_text = text.strip()
        logger.info(f"Generating OpenAI embedding (Python) for text (first 50 chars): {trimmed_text[:50]}...")

        response = openai_client_py.embeddings.create(
            model="text-embedding-ada-002", # Outputs 1536 dimensions
            input=trimmed_text
        )

        if response.data and len(response.data) > 0 and response.data[0].embedding:
            embedding_vector = response.data[0].embedding
            logger.info(f"[ResearchAgent] Successfully generated OpenAI embedding (Python). Dimension: {len(embedding_vector)}")
            if len(embedding_vector) != DEFAULT_VECTOR_DIMENSION_PY:
                logger.warning(f"[ResearchAgent] Warning: OpenAI embedding dimension mismatch. Expected {DEFAULT_VECTOR_DIMENSION_PY}, Got {len(embedding_vector)}")
            return embedding_vector
        else:
            logger.error(f"[ResearchAgent] OpenAI embedding response (Python) is missing expected data for text: {trimmed_text[:100]}")
            return None
    except Exception as e:
        logger.error(f"[ResearchAgent] Error generating OpenAI embedding (Python) for text '{trimmed_text[:100]}...': {e}", exc_info=True)
        return None

_research_findings_table_checked = False

def ensure_research_findings_table_exists(db_conn):
    """Checks if the 'research_findings' table exists (read-only check)."""
    global _research_findings_table_checked
    if _research_findings_table_checked:
        return

    if not db_conn:
        logger.error("No DB connection to check for research_findings table.")
        return
    try:
        table_names = db_conn.table_names()
        if 'research_findings' not in table_names:
            logger.warning("Table 'research_findings' does not exist. It should be created by the Node.js part of the application. Python side will only add data.")
        else:
            logger.info("Table 'research_findings' confirmed to exist.")
        _research_findings_table_checked = True
    except Exception as e:
        logger.error(f"Error checking for research_findings table: {e}", exc_info=True)


# --- End LanceDB Setup ---


def search_past_research(original_query: str, top_k: int = 3) -> list[dict]:
    """Searches past research findings in LanceDB based on the original query."""
    db_conn = get_lance_db_connection()
    if not db_conn:
        logger.error("Cannot search past research, LanceDB connection not available.")
        return []

    if not original_query.strip():
        logger.warning("Original query is empty, skipping past research search.")
        return []

    try:
        ensure_research_findings_table_exists(db_conn) # Ensure check has run

        query_embedding = generate_embedding_py(original_query)
        if query_embedding is None:
            logger.warning(f"Could not generate embedding for query: {original_query[:100]}. Skipping past research search.")
            return []

        logger.info(f"Searching 'research_findings' for query: {original_query[:100]}...")
        research_table = db_conn.open_table('research_findings')

        # Assuming 'query_embedding' is the vector column for the original queries in 'research_findings'
        results = research_table.search(query_embedding, vector_column_name='query_embedding') \
                                .limit(top_k) \
                                .to_list()

        logger.info(f"Found {len(results)} past research items.")
        return results
    except Exception as e:
        logger.error(f"Error searching past research: {e}", exc_info=True)
        return []


def decompose_query_into_tasks_llm(user_query: str) -> list[str]:
    """Decomposes a user query into actionable sub-tasks using an LLM."""
    if not OPENAI_API_KEY_GLOBAL:
        logger.error("Error: OpenAI API key not provided for task decomposition.")
        return []
    try:
        client = openai.OpenAI(api_key=OPENAI_API_KEY_GLOBAL)
        response = client.chat.completions.create( # type: ignore
            model="gpt-3.5-turbo-1106", # type: ignore
            response_format={"type": "json_object"}, # type: ignore
            messages=[ # type: ignore
                {"role": "system", "content": "You are a research assistant. Given the user's research query, break it down into 3 to 5 specific, actionable sub-queries that can be independently searched on a web search engine. Each sub-query should be a concise string. Respond ONLY with a valid JSON object containing a single key 'tasks' which is a list of these string sub-queries. For example: {\\\"tasks\\\": [\\\"search query 1\\\", \\\"search query 2\\\", \\\"search query 3\\\"]}"}, # type: ignore
                {"role": "user", "content": user_query} # type: ignore
            ],
            temperature=0.2 # type: ignore
        )
        response_content = response.choices[0].message.content
        if response_content:
            data = json.loads(response_content)
            if isinstance(data.get("tasks"), list):
                return [str(task) for task in data["tasks"] if isinstance(task, str)]
        logger.warning("LLM response for task decomposition did not yield a valid task list.")
        return []
    except Exception as e:
        logger.error(f"Error decomposing query with LLM: {e}", exc_info=True)
        return []


def initiate_research_project(user_query: str, user_id: str, project_db_id: str, task_db_id: str) -> dict:
    """
    Initiates a research project by decomposing the query into tasks and creating corresponding Notion pages.
    Assumes Notion client (note_utils.notion) is already initialized by the caller.
    """
    if not OPENAI_API_KEY_GLOBAL:
        return {"status": "error", "message": "OpenAI API key not configured.", "code": "CONFIG_ERROR_OPENAI"}

    if not note_utils.notion: # type: ignore
        return {"status": "error", "message": "Notion client not initialized in note_utils.", "code": "NOTION_CLIENT_ERROR"}

    db_conn = get_lance_db_connection()
    if db_conn:
        ensure_research_findings_table_exists(db_conn)
    else:
        logger.warning("LanceDB connection failed, proceeding without DB interaction for this initiation.")

    original_notes_db_id = note_utils.NOTION_NOTES_DATABASE_ID # type: ignore
    project_page_id = None
    try:
        # Search for past research
        past_findings = search_past_research(user_query)
        if past_findings:
            logger.info(f"Found {len(past_findings)} potentially relevant past research items for query: {user_query}")

        task_descriptions = decompose_query_into_tasks_llm(user_query)
        if not task_descriptions:
            return {"status": "error", "message": "Failed to decompose query into tasks (no tasks returned by LLM).", "code": "LLM_DECOMPOSITION_FAILED"}

        note_utils.NOTION_NOTES_DATABASE_ID = project_db_id # type: ignore
        project_page_title = f"Research Project: {user_query[:100]}"

        project_content = f"Original Query: {user_query}\nUser ID: {user_id}\nStatus: Pending\n\n"

        if past_findings:
            project_content += "--- Relevant Past Research ---\n"
            for find in past_findings:
                # Ensure find is a dict, as to_list() should return list of dicts
                summary_text = find.get('summary', 'N/A')
                if not isinstance(summary_text, str): # Handle cases where summary might not be a string
                    summary_text = str(summary_text)
                project_content += f"- ID: {find.get('finding_id', 'N/A')}, Summary: {summary_text[:100]}...\n"
            project_content += "---\n\n"

        # Note: The original code had project_content defined after task_descriptions.
        # Moved it earlier to include past_findings before creating the Notion page.
        # The content "Status: Pending" was part of the initial string, ensure it's handled correctly.
        # The example in prompt shows it as part of initial string then other content.

        project_creation_response = note_utils.create_notion_note(title=project_page_title, content=project_content.strip(), source="research_agent") # type: ignore

        if project_creation_response["status"] != "success": # type: ignore
            return {"status": "error", "message": f"Failed to create project page in Notion DB {project_db_id}. Details: {project_creation_response.get('message')}", "code": "NOTION_PAGE_CREATION_FAILED"} # type: ignore
        project_page_id = project_creation_response["data"] # Expects page_id here # type: ignore

        created_task_page_ids = []
        note_utils.NOTION_NOTES_DATABASE_ID = task_db_id # type: ignore
        for desc in task_descriptions:
            task_page_title = f"Task: {desc[:100]}"
            task_creation_response = note_utils.create_notion_note(title=task_page_title, content=desc, source="research_agent_task", linked_event_id=project_page_id) # type: ignore
            if task_creation_response["status"] == "success": # type: ignore
                created_task_page_ids.append(task_creation_response["data"]) # Expects page_id # type: ignore
            else:
                logger.warning(f"Failed to create task page for description '{desc}' in Notion DB {task_db_id}. Details: {task_creation_response.get('message')}") # type: ignore

        return {"status": "success", "data": {"project_page_id": project_page_id, "task_page_ids": created_task_page_ids}}
    except Exception as e:
        logger.error(f"Unexpected error in initiate_research_project: {e}", exc_info=True)
        return {"status": "error", "message": f"Unexpected error: {str(e)}", "code": "UNEXPECTED_ERROR_INITIATE_RESEARCH"}
    finally:
        if original_notes_db_id is not None :
            note_utils.NOTION_NOTES_DATABASE_ID = original_notes_db_id # type: ignore


from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

def is_retryable_serpapi_exception(e):
    """Determines if a SerpApiClientException is retryable."""
    if isinstance(e, SerpApiClientException):
        message = str(e).lower()
        # Retry on timeouts or generic connection issues, but not on auth/invalid key/forbidden
        if "request timed out" in message: return True
        if "connection error" in message: return True # Example, adjust if SerpApi has specific connection error messages
        # Avoid retrying on API key issues or structural problems with the request
        if "api_key_invalid" in message or "forbidden" in message or "authorization" in message or "invalid_request" in message:
            return False
        # Potentially retry on 5xx errors if SerpApi client surfaces them this way
        # For now, being conservative and only retrying explicit timeout/connection style errors
        return False # Default to not retry for other SerpApiClientExceptions
    return False # Not a SerpApiClientException

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10),
    retry=retry_if_exception_type(SerpApiClientException) | retry_if_exception_type(ConnectionError), # Broadly retry SerpApiClientException and general ConnectionError
    # For more fine-grained control with SerpApiClientException:
    # retry=retry_if_exception(is_retryable_serpapi_exception)
    before_sleep=lambda retry_state: logger.info(f"Retrying SerpApi call for query '{retry_state.args[0] if retry_state.args else 'N/A'}' (attempt {retry_state.attempt_number}) due to: {retry_state.outcome.exception()}")
)
def python_search_web(query: str) -> dict:
    """
    Performs a web search using SerpApi with tenacity retries.
    """
    operation_name = "python_search_web"
    if not SERPAPI_API_KEY_GLOBAL:
        logger.error(f"[{operation_name}] Search API key is missing.")
        return {"status": "error", "message": "Search API key is missing.", "code": "CONFIG_ERROR_SEARCH_KEY"}

    params = { "q": query, "api_key": SERPAPI_API_KEY_GLOBAL, "engine": "google", "timeout": 20 } # Added timeout to SerpApi call
    search_results: list = []

    logger.info(f"[{operation_name}] Performing SerpApi search for query: {query[:100]}")
    search = GoogleSearch(params) # type: ignore
    results_data = search.get_dict() # This is the call that might raise SerpApiClientException or other network issues

    # Error handling for non-exception errors (e.g., error message in response body)
    if results_data.get("error"):
        error_message = results_data["error"]
        code = "SEARCH_API_ERROR_RESPONSE" # More specific code
        if "invalid api key" in error_message.lower() or "authorization" in error_message.lower() or "forbidden" in error_message.lower():
            code = "SEARCH_API_AUTH_ERROR_RESPONSE"
        logger.warning(f"[{operation_name}] SerpApi returned an error in response body for query '{query}': {error_message}. Code: {code}")
        # This error is from SerpApi's JSON response, not an exception, so tenacity won't retry it.
        # It's typically a non-transient error (e.g. bad API key, bad query).
        return {"status": "error", "message": f"SerpApi error: {error_message}", "code": code, "details": results_data.get("search_parameters", {})}

    organic_results = results_data.get("organic_results", [])
    if not organic_results and "knowledge_graph" in results_data:
        kg = results_data["knowledge_graph"]
        if kg.get("title") and kg.get("source", {}).get("link"):
                search_results.append({
                "title": kg.get("title", "Knowledge Graph Result"),
                "link": kg.get("source", {}).get("link", "#"),
                "snippet": kg.get("description", kg.get("snippet", "No snippet available from Knowledge Graph."))
            })

    for item in organic_results:
        search_results.append({
            "title": item.get("title", "No title"), "link": item.get("link", "#"),
            "snippet": item.get("snippet", "No snippet available.")
        })

    if not search_results and not results_data.get("error"): # Check error again in case it was missed
        logger.info(f"[{operation_name}] No organic results found by SerpApi for query: {query}. State: {results_data.get('search_information', {}).get('organic_results_state', 'N/A')}")

    logger.info(f"[{operation_name}] SerpApi search successful for query: {query[:100]}. Found {len(search_results)} results.")
    return {"status": "success", "data": search_results}

# The main try-except block for python_search_web is now outside the tenacity @retry decorator.
# Tenacity will handle retries for exceptions specified in `retry=`.
# If all retries fail, the exception will propagate and be caught by this outer block.
def python_search_web_wrapper(query: str) -> dict:
    try:
        return python_search_web(query) # Call the tenacity-decorated function
    except SerpApiClientException as e: # type: ignore
        message = str(e)
        code = "SEARCH_API_ERROR_FINAL" # After retries
        if "API_KEY_INVALID" in message or "forbidden" in message.lower() or "authorization" in message.lower():
            code = "SEARCH_API_AUTH_ERROR_FINAL"
        elif "Request timed out" in message:
            code = "NETWORK_ERROR_FINAL"
        logger.error(f"[python_search_web_wrapper] SerpApi client error for query '{query}' after retries: {message}", exc_info=True)
        return {"status": "error", "message": f"SerpApi client error after retries: {message}", "code": code, "details": str(e)}
    except ConnectionError as e: # Catch generic connection errors that might not be SerpApiClientException
        logger.error(f"[python_search_web_wrapper] Connection error during web search for query '{query}' after retries: {e}", exc_info=True)
        return {"status": "error", "message": f"Connection error during web search after retries: {str(e)}", "code": "CONNECTION_ERROR_FINAL", "details": str(e)}
    except Exception as e:
        logger.error(f"[python_search_web_wrapper] Unexpected error during web search (SerpApi query '{query}') after retries: {e}", exc_info=True)
        return {"status": "error", "message": f"Unexpected error during web search after retries: {str(e)}", "code": "UNKNOWN_SEARCH_ERROR_FINAL", "details": str(e)}


def execute_research_task(task_page_id: str) -> dict:
    """
    Executes a single research task. Assumes Notion client is initialized.
    """
    if not note_utils.notion: # type: ignore
        return {"status": "error", "message": "Notion client not initialized.", "code": "NOTION_CLIENT_ERROR"}
    if not SERPAPI_API_KEY_GLOBAL:
        return {"status": "error", "message": "Search API key not provided.", "code": "CONFIG_ERROR_SEARCH_KEY"}

    original_notes_db_id = note_utils.NOTION_NOTES_DATABASE_ID # type: ignore
    task_details_response = note_utils.get_notion_note(page_id=task_page_id) # Returns dict # type: ignore

    if task_details_response["status"] != "success": # type: ignore
        return {"status": "error", "message": f"Could not retrieve task details for page ID: {task_page_id}. Details: {task_details_response.get('message')}", "code": "NOTION_PAGE_RETRIEVAL_FAILED"} # type: ignore
    task_details = task_details_response["data"] # page object # type: ignore

    try:
        search_query = task_details.get("content", "").strip()
        if not search_query:
            search_query = task_details.get("title", "").replace("Task: ", "").strip()
            if not search_query:
                logger.warning(f"Search query not found for page ID: {task_page_id} after checking content and title.")
                return {"status": "error", "message": f"Search query not found for page ID: {task_page_id}", "code": "QUERY_NOT_FOUND"}

        logger.info(f"Executing task {task_page_id}: Searching for '{search_query}'")
        search_response = python_search_web(query=search_query)

        results_log_string = f"\n\n--- Search Task: {search_query} ---\n"
        search_links = [] # For LanceDB storage
        if search_response["status"] == "success": # type: ignore
            search_results = search_response["data"] # type: ignore
            if search_results:
                for res in search_results:
                    results_log_string += f"Title: {res['title']}\nLink: {res['link']}\nSnippet: {res['snippet']}\n---\n"
                    if res['link'] != "#": search_links.append(res['link'])
            else: results_log_string += "No results found for this query.\n"
        else:
            error_info = search_response
            results_log_string += (f"Search failed: {error_info.get('message', 'Unknown error')}\n" # type: ignore
                                   f"Error Code: {error_info.get('code', 'N/A')}\n" # type: ignore
                                   f"Details: {str(error_info.get('details', 'N/A'))}\n---\n") # type: ignore

        current_content = task_details.get("content", "")
        updated_content = current_content + results_log_string

        # Store search_links in the task page temporarily if needed, or pass them up
        # For now, we'll rely on extracting from content later, or assume they are part of the snippet.
        # A more robust way would be to add a property to the Notion page for structured data like URLs.

        update_response = note_utils.update_notion_note(page_id=task_page_id, content=updated_content, linked_task_id="COMPLETED") # type: ignore

        if update_response["status"] == "success": # type: ignore
            return {"status": "success", "message": f"Task {task_page_id} processed. Results/errors saved."}
        else:
            return {"status": "error", "message": f"Failed to update task {task_page_id} in Notion. Details: {update_response.get('message')}", "code": "NOTION_PAGE_UPDATE_FAILED"} # type: ignore

    except Exception as e:
        logger.error(f"Unexpected error executing research task {task_page_id}: {e}", exc_info=True)
        if task_page_id:
            try:
                current_content_for_error = task_details.get("content", "Content not available.") if task_details else "Content not available."
                error_update_content = current_content_for_error + f"\n\n--- ERROR DURING TASK EXECUTION ---\n{str(e)}\n---"
                note_utils.update_notion_note(page_id=task_page_id, content=error_update_content, linked_task_id="ERROR_STATE") # type: ignore
            except Exception as notion_update_err:
                logger.error(f"Additionally failed to update Notion page {task_page_id} with execution error: {notion_update_err}", exc_info=True)
        return {"status": "error", "message": f"Unexpected error executing task {task_page_id}: {str(e)}", "code": "UNEXPECTED_TASK_EXECUTION_ERROR"}
    finally:
        if original_notes_db_id is not None:
             note_utils.NOTION_NOTES_DATABASE_ID = original_notes_db_id # type: ignore


def monitor_and_execute_tasks(task_db_id: str, project_db_id: str) -> dict:
    """
    Monitors Notion for pending tasks, executes them, and triggers synthesis.
    Assumes Notion client is initialized.
    """
    if not SERPAPI_API_KEY_GLOBAL: return {"status": "error", "message": "Search API key not configured.", "code": "CONFIG_ERROR_SEARCH_KEY"}
    if not OPENAI_API_KEY_GLOBAL: return {"status": "error", "message": "OpenAI API key not configured.", "code": "CONFIG_ERROR_OPENAI"}
    if not note_utils.notion: return {"status": "error", "message": "Notion client not initialized.", "code": "NOTION_CLIENT_ERROR"} # type: ignore

    logger.info(f"Checking for pending tasks in database: {task_db_id}")
    original_notes_db_id = note_utils.NOTION_NOTES_DATABASE_ID # type: ignore
    processed_tasks_count = 0; failed_tasks_count = 0
    try:
        note_utils.NOTION_NOTES_DATABASE_ID = task_db_id # type: ignore
        all_tasks_response = note_utils.search_notion_notes(query="", source="research_agent_task") # type: ignore

        if all_tasks_response["status"] != "success": # type: ignore
            return {"status": "error", "message": "Failed to retrieve tasks from Notion.", "code": "NOTION_SEARCH_FAILED", "details": all_tasks_response.get("message")} # type: ignore

        all_tasks_in_db = all_tasks_response["data"] # type: ignore
        pending_tasks = [t for t in all_tasks_in_db if t.get("source") == "research_agent_task" and t.get("linked_task_id") not in ["COMPLETED", "ERROR_STATE"]]

        if not pending_tasks: logger.info("No pending research tasks found.")
        else:
            logger.info(f"Found {len(pending_tasks)} pending tasks to process.")
            for task_summary in pending_tasks:
                task_page_id = task_summary["id"]
                result = execute_research_task(task_page_id=task_page_id)
                if result.get("status") == "success": processed_tasks_count += 1
                else: failed_tasks_count += 1; logger.error(f"Failed task {task_page_id}: {result.get('message')}")
    except Exception as e:
        logger.error(f"Error during task monitoring/execution: {e}", exc_info=True)
        return {"status": "error", "message": f"Error in task monitoring loop: {str(e)}", "code": "TASK_MONITORING_ERROR"}
    finally:
        if original_notes_db_id is not None: note_utils.NOTION_NOTES_DATABASE_ID = original_notes_db_id # type: ignore

    logger.info("Proceeding to check for project completion and synthesis.")
    synthesis_result = check_projects_for_completion_and_synthesize(project_db_id=project_db_id, task_db_id=task_db_id)

    return {"status": "success", "data": {
            "message": f"Task cycle finished. Processed: {processed_tasks_count}, Failed: {failed_tasks_count}.",
            "processed_tasks": processed_tasks_count, "failed_tasks": failed_tasks_count,
            "synthesis_outcome": synthesis_result }}


def synthesize_research_findings_llm(findings: list[str], original_query: str) -> str:
    """Synthesizes research findings using an LLM."""
    if not OPENAI_API_KEY_GLOBAL: return "Error: OpenAI API key not provided for synthesis."
    try:
        client = openai.OpenAI(api_key=OPENAI_API_KEY_GLOBAL)
        MAX_FINDINGS_LENGTH = 15000 # type: ignore
        concatenated_findings = "\n\n---\n\n".join(findings) # type: ignore
        if len(concatenated_findings) > MAX_FINDINGS_LENGTH: # type: ignore
            concatenated_findings = concatenated_findings[:MAX_FINDINGS_LENGTH] + "... (truncated)" # type: ignore

        system_prompt = ("You are a research analyst. Based on the collected information and original query, synthesize a comprehensive report. " # type: ignore
                         "Focus on addressing the query directly, using only the provided information. State if information is insufficient.")
        user_message = f"Original Query: {original_query}\n\nCollected Information:\n{concatenated_findings}" # type: ignore

        response = client.chat.completions.create( # type: ignore
            model="gpt-4-turbo-preview", # type: ignore
            messages=[ {"role": "system", "content": system_prompt}, {"role": "user", "content": user_message} ], # type: ignore
            temperature=0.5 ) # type: ignore
        report = response.choices[0].message.content
        return report if report else "Synthesis LLM returned empty content."
    except Exception as e:
        logger.error(f"Error synthesizing research findings with LLM: {e}", exc_info=True)
        return f"Error during synthesis: {str(e)}"

def extract_source_urls_from_findings(findings_text_list: list[str]) -> list[str]:
    """Rudimentary extraction of URLs from a list of text strings."""
    urls = set()
    import re
    url_pattern = re.compile(r'https?://[^\s/$.?#].[^\s]*') # Basic URL regex
    for text_block in findings_text_list:
        found = url_pattern.findall(text_block)
        for url in found:
            urls.add(url)
    return list(urls)

def store_research_findings_in_ltm(db_conn, project_page_id: str, original_query: str, report_content: str, completed_task_findings: list[str]):
    """Stores the synthesized research findings into LanceDB."""
    if not db_conn:
        logger.error("No LanceDB connection available for storing research findings.")
        return

    try:
        logger.info(f"Preparing to store research findings for project: {project_page_id}")
        finding_id = str(uuid.uuid4())

        query_embedding_val = generate_embedding_py(original_query)
        if query_embedding_val is None:
            logger.warning(f"Failed to generate query embedding for project {project_page_id}. Storing finding with null query_embedding.")

        summary_embedding_val = generate_embedding_py(report_content)
        if summary_embedding_val is None:
            logger.warning(f"Failed to generate summary embedding for project {project_page_id}. Storing finding with null summary_embedding.")

        # Combine all task findings into a single details_text string
        details_text = "\n\n---\n\n".join(completed_task_findings)

        # Extract source references (rudimentary)
        source_references = extract_source_urls_from_findings(completed_task_findings)

        current_time_iso = datetime.now(timezone.utc).isoformat()

        data_to_store = {
            "finding_id": finding_id,
            "query": original_query,
            "query_embedding": query_embedding_val, # Can be None
            "summary": report_content,
            "summary_embedding": summary_embedding_val, # Can be None
            "details_text": details_text,
            "source_references": source_references,
            "project_page_id": project_page_id,
            "created_at": current_time_iso,
            "updated_at": current_time_iso
        }

        research_table = db_conn.open_table("research_findings")
        research_table.add([data_to_store])
        logger.info(f"Successfully stored research finding {finding_id} for project {project_page_id} in LanceDB.")

    except Exception as e:
        logger.error(f"Error storing research finding for project {project_page_id} in LanceDB: {e}", exc_info=True)


def check_projects_for_completion_and_synthesize(project_db_id: str, task_db_id: str) -> dict:
    """
    Checks research projects for completion, triggers synthesis, and stores results in LanceDB.
    Assumes Notion client initialized.
    """
    if not note_utils.notion: return {"status": "error", "message": "Notion client not initialized.", "code": "NOTION_CLIENT_ERROR"} # type: ignore
    if not OPENAI_API_KEY_GLOBAL: return {"status": "error", "message": "OpenAI API key not provided for synthesis.", "code": "CONFIG_ERROR_OPENAI"}

    db_conn = get_lance_db_connection() # Get DB connection for storing results

    logger.info(f"Checking completable projects in DB: {project_db_id}")
    original_notes_db_id = note_utils.NOTION_NOTES_DATABASE_ID # type: ignore
    synthesis_attempts = 0; successful_synthesis_updates = 0
    try:
        note_utils.NOTION_NOTES_DATABASE_ID = project_db_id # type: ignore
        projects_response = note_utils.search_notion_notes(query="Status: Pending", source="research_agent") # Expects dict # type: ignore

        if projects_response["status"] != "success": # type: ignore
            return {"status": "error", "message": f"Failed to search pending projects: {projects_response.get('message')}", "code": "NOTION_SEARCH_FAILED"} # type: ignore

        pending_projects = [p for p in projects_response["data"] if "Status: Pending" in p.get("content","")] # type: ignore
        if not pending_projects:
            return {"status": "success", "data": {"message": "No pending projects for synthesis.", "updated_projects": 0}}

        synthesis_attempts = len(pending_projects)
        logger.info(f"Found {synthesis_attempts} pending projects for synthesis check.")

        for project in pending_projects:
            project_page_id = project["id"]
            original_query_match = [line for line in project.get("content","").split("\n") if line.startswith("Original Query: ")]
            original_query = original_query_match[0].replace("Original Query: ","").strip() if original_query_match else "Unknown Original Query"

            note_utils.NOTION_NOTES_DATABASE_ID = task_db_id # type: ignore
            tasks_response = note_utils.search_notion_notes(query="", source="research_agent_task") # Expects dict # type: ignore
            if tasks_response["status"] != "success": # type: ignore
                logger.warning(f"Failed to retrieve tasks for project {project_page_id}: {tasks_response.get('message')}") # type: ignore
                continue

            project_tasks = [t for t in tasks_response["data"] if t.get("linked_event_id") == project_page_id] # type: ignore
            if not project_tasks: logger.info(f"No tasks for project {project_page_id}."); continue

            all_tasks_processed = True; completed_task_findings: list[str] = []
            for task in project_tasks:
                task_status = task.get("linked_task_id")
                if task_status == "COMPLETED": completed_task_findings.append(task.get("content", ""))
                elif task_status == "ERROR_STATE": completed_task_findings.append(f"Note: Task '{task.get('title')}' error. Content: {task.get('content', '')}")
                else: all_tasks_processed = False; break

            if all_tasks_processed:
                logger.info(f"All tasks for project {project_page_id} processed. Synthesizing report...")
                report_content = synthesize_research_findings_llm(completed_task_findings, original_query) if completed_task_findings else "No findings from tasks."

                # Store in LanceDB
                if db_conn:
                    store_research_findings_in_ltm(db_conn, project_page_id, original_query, report_content, completed_task_findings)
                else:
                    logger.warning(f"Skipping LanceDB storage for project {project_page_id} due to no DB connection.")

                # Update Notion
                note_utils.NOTION_NOTES_DATABASE_ID = project_db_id # type: ignore
                project_current_content = project.get("content","")
                updated_project_content = project_current_content.replace("Status: Pending", "Status: Completed")
                updated_project_content += f"\n\n--- Synthesized Report ---\n{report_content}"

                update_response = note_utils.update_notion_note(page_id=project_page_id, content=updated_project_content, linked_task_id="COMPLETED_PROJECT") # Expects dict # type: ignore
                if update_response["status"] == "success": successful_synthesis_updates +=1 # type: ignore
                else: logger.warning(f"Failed to update project {project_page_id} in Notion. Details: {update_response.get('message')}") # type: ignore
            else: logger.info(f"Project {project_page_id} has unprocessed tasks.")
    except Exception as e:
        logger.error(f"Unexpected error during synthesis check: {str(e)}", exc_info=True)
        return {"status": "error", "message": f"Unexpected error during synthesis check: {str(e)}", "code": "SYNTHESIS_CHECK_ERROR"}
    finally:
        if original_notes_db_id is not None: note_utils.NOTION_NOTES_DATABASE_ID = original_notes_db_id # type: ignore

    return {"status": "success", "data": {"message": f"Synthesis check finished. Attempted: {synthesis_attempts}, Succeeded: {successful_synthesis_updates}",
                                          "attempted_synthesis": synthesis_attempts, "successful_synthesis_updates": successful_synthesis_updates}}

[end of atomic-docker/project/functions/atom-agent/research_agent.py]
