---
phase: 68-byok-cognitive-tier-system
plan: 04
type: execute
wave: 2
depends_on: [68-01]
files_modified:
  - backend/core/llm/minimax_integration.py
  - backend/core/dynamic_pricing_fetcher.py
  - backend/core/benchmarks.py
  - backend/tests/test_minimax_integration.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - MiniMax M2.5 API integration working with httpx async client
    - M2.5 positioned in STANDARD tier with ~$1/M token pricing
    - LiteLLM pricing fetch includes MiniMax when available
    - Paygo pricing supported (no minimum commitment)
    - Graceful fallback when MiniMax API unavailable
  artifacts:
    - path: backend/core/llm/minimax_integration.py
      provides: MiniMaxIntegration class with API wrapper
      min_lines: 200
    - path: backend/core/benchmarks.py
      provides: MiniMax M2.5 quality score
      contains: "minimax-m2.5"
    - path: backend/tests/test_minimax_integration.py
      provides: MiniMax integration tests
      min_lines: 200
  key_links:
    - from: backend/core/llm/minimax_integration.py
      to: backend/core/dynamic_pricing_fetcher.py
      via: get_pricing_fetcher().get_model_price()
      pattern: "from core.dynamic_pricing_fetcher import get_pricing_fetcher"
    - from: backend/core/llm/byok_handler.py
      to: backend/core/llm/minimax_integration.py
      via: MiniMaxIntegration import for STANDARD tier
      pattern: "from core.llm.minimax_integration import MiniMaxIntegration"

## Objective

Integrate MiniMax M2.5 as a cost-effective alternative in the Standard cognitive tier. MiniMax M2.5 (launched Feb 12, 2026) offers competitive reasoning at ~$1/M tokens with paygo pricing. This implementation provides a wrapper for the MiniMax API with graceful fallback and LiteLLM pricing integration.

**Purpose:** Add another cost-effective option for Standard tier queries with paygo pricing (no minimum commitment).

**Output:** MiniMaxIntegration class with API wrapper, pricing integration, and test coverage.

## Context

@/Users/rushiparikh/projects/atom/.planning/phases/68-byok-cognitive-tier-system/68-RESEARCH.md
@/Users/rushiparikh/projects/atom/backend/core/dynamic_pricing_fetcher.py
@/Users/rushiparikh/projects/atom/backend/core/benchmarks.py

## Tasks

<task type="auto">
  <name>Task 1: Create MiniMaxIntegration API wrapper</name>
  <files>backend/core/llm/minimax_integration.py</files>
  <action>
Create new file `backend/core/llm/minimax_integration.py` with:

1. **MiniMaxIntegration class**:
   ```python
   class MiniMaxIntegration:
       BASE_URL = "https://api.minimaxi.com/v1"

       # Estimated pricing (will update when official pricing announced)
       ESTIMATED_PRICING = {
           "input_cost_per_token": 0.000001,  # $1/M tokens
           "output_cost_per_token": 0.000001,
           "max_tokens": 128000,
       }

       # Model capabilities based on research
       CAPABILITIES = {
           "quality_score": 88,  # Between gemini-2.0-flash (86) and deepseek-chat (80)
           "supports_vision": False,
           "supports_tools": True,  # Native agent support
           "supports_cache": False,
           "tier": CognitiveTier.STANDARD,
       }
   ```

2. **Methods**:
   - `__init__(api_key: str)`: Initialize httpx.AsyncClient with BASE_URL, headers, timeout=30s
   - `async generate(prompt, temperature=0.7, max_tokens=1000) -> Optional[str]`:
     - POST to /chat/completions with model="m2.5"
     - Handle 429 as RateLimitedError
     - Return choices[0].message.content or None on error

   - `get_pricing() -> Dict`: Return ESTIMATED_PRICING

   - `async test_connection() -> bool`: GET /models, return True if 200

   - `get_capabilities() -> Dict`: Return CAPABILITIES dict

3. **Error handling**:
   ```python
   class RateLimitedError(Exception):
       pass

   # In generate():
   try:
       response = await self.client.post(...)
       response.raise_for_status()
   except httpx.HTTPStatusError as e:
       if e.response.status_code == 429:
           raise RateLimitedError("MiniMax rate limit exceeded")
       logger.error(f"MiniMax generation failed: {e}")
       return None
   ```

4. **Add comprehensive docstrings**:
   - Note: Pricing is estimated ($1/M) until official announcement
   - M2.5 launched Feb 12, 2026
   - API access currently closed (graceful fallback)

DO NOT implement auth flow - assume API key provided via config.
</action>
  <verify>
pytest tests/test_minimax_integration.py::test_minimax_client_initialization -v
pytest tests/test_minimax_integration.py::test_generate_method -v
  </verify>
  <done>
MiniMaxIntegration class created with generate(), test_connection(), get_pricing(), get_capabilities() methods, RateLimitedError defined, docstrings note estimated pricing
</done>
</task>

<task type="auto">
  <name>Task 2: Add MiniMax to benchmarks and pricing fetcher</name>
  <files>backend/core/benchmarks.py backend/core/dynamic_pricing_fetcher.py</files>
  <action>
1. Extend `backend/core/benchmarks.py`:
   Add to MODEL_QUALITY_SCORES:
   ```python
   "minimax-m2.5": 88,  # Standard tier, between gemini-2.0-flash and deepseek-chat
   ```

2. Extend `backend/core/dynamic_pricing_fetcher.py`:
   Add MiniMax to pricing cache transform (if not in LiteLLM yet):
   ```python
   # In fetch_litellm_pricing(), add fallback after main loop:
   if "minimax-m2.5" not in pricing:
       pricing["minimax-m2.5"] = {
           "input_cost_per_token": 0.000001,  # $1/M estimated
           "output_cost_per_token": 0.000001,
           "max_tokens": 128000,
           "litellm_provider": "minimax",
           "supports_cache": False,
           "source": "estimated",  # Mark as estimated
       }
   ```

3. Add method to check if pricing is estimated:
   ```python
   def is_pricing_estimated(self, model_name: str) -> bool:
       pricing = self.get_model_price(model_name)
       if pricing:
           return pricing.get("source") == "estimated"
       return False
   ```

DO NOT hardcode pricing in byok_handler - always fetch via DynamicPricingFetcher.
</action>
  <verify>
python -c "
from core.benchmarks import get_quality_score
from core.dynamic_pricing_fetcher import get_pricing_fetcher
print('MiniMax quality:', get_quality_score('minimax-m2.5'))
f = get_pricing_fetcher()
p = f.get_model_price('minimax-m2.5')
print('MiniMax pricing:', p.get('input_cost_per_token') if p else None)
"
  </verify>
  <done>
MiniMax M2.5 added to MODEL_QUALITY_SCORES (88), DynamicPricingFetcher returns estimated pricing, is_pricing_estimated() identifies estimated sources
</done>
</task>

<task type="auto">
  <name>Task 3: Integrate MiniMax into BYOK tier routing</name>
  <files>backend/core/llm/byok_handler.py</files>
  <action>
Extend `backend/core/llm/byok_handler.py`:

1. Add MiniMax to providers_config:
   ```python
   providers_config = {
       "openai": {"base_url": None},
       "deepseek": {"base_url": "https://api.deepseek.com/v1"},
       "moonshot": {"base_url": "https://api.moonshot.cn/v1"},
       "deepinfra": {"base_url": "https://api.deepinfra.com/v1/openai"},
       "minimax": {"base_url": "https://api.minimaxi.com/v1"},  # NEW
   }
   ```

2. Add MiniMax to COST_EFFICIENT_MODELS:
   ```python
   COST_EFFICIENT_MODELS = {
       # ... existing ...
       "minimax": {
           QueryComplexity.SIMPLE: "minimax-m2.5",
           QueryComplexity.MODERATE: "minimax-m2.5",
           QueryComplexity.COMPLEX: "minimax-m2.5",
           QueryComplexity.ADVANCED: "minimax-m2.5",
       },
   }
   ```

3. Add minimax to static fallback provider_priority lists in get_ranked_providers():
   ```python
   # For SIMPLE and MODERATE complexity
   provider_priority = ["deepseek", "minimax", "moonshot", "gemini", "openai", "anthropic"]
   ```

4. Add note in docstring about MiniMax positioning:
   ```
   MiniMax M2.5: Positioned in STANDARD tier with estimated $1/M pricing.
   API access may be closed - graceful fallback to next provider.
   ```

DO NOT require MiniMax API key - system works without it (graceful fallback).
</action>
  <verify>
pytest tests/test_minimax_integration.py::test_byok_integration -v
# Test that minimax appears in provider rankings
python -c "
from core.llm.byok_handler import BYOKHandler
h = BYOKHandler()
print('minimax in providers_config:', 'minimax' in str(h.clients))
"
  </verify>
  <done>
MiniMax added to providers_config, COST_EFFICIENT_MODELS, and provider_priority lists, BYOKHandler routes to MiniMax when available
</done>
</task>

<task type="auto">
  <name>Task 4: Create MiniMax integration tests</name>
  <files>backend/tests/test_minimax_integration.py</files>
  <action>
Create `backend/tests/test_minimax_integration.py` with:

1. **Client initialization tests** (3 tests):
   - test_minimax_client_initialization: Client created with correct BASE_URL
   - test_api_key_header_set: Authorization header set correctly
   - test_timeout_configuration: 30s timeout configured

2. **Generate method tests** (4 tests):
   - test_generate_success: Returns response content on success
   - test_generate_rate_limit: Raises RateLimitedError on 429
   - test_generate_error_handling: Returns None on other errors
   - test_temperature_parameter: Passed correctly to API

3. **Pricing and capabilities tests** (3 tests):
   - test_get_pricing_returns_estimated: Returns $1/M pricing
   - test_get_capabilities: Returns correct tier (STANDARD) and quality_score (88)
   - test_supports_tools_true: Native agent support

4. **BYOK integration tests** (3 tests):
   - test_minimax_in_provider_config: Listed in providers_config
   - test_minimax_in_cost_efficient_models: Mapped to m2.5 for all complexities
   - test_minimax_in_static_fallback: Appears in provider_priority

5. **Fallback tests** (3 tests):
   - test_test_connection_returns_false_on_401: Handles invalid API key
   - test_generate_returns_none_on_unavailable: Graceful degradation
   - test_byok_falls_back_to_next_provider: Continues without MiniMax

6. **Benchmark integration test** (1 test):
   - test_quality_score_defined: get_quality_score("minimax-m2.5") returns 88

Use pytest.mark.asyncio for async tests. Mock httpx responses to avoid real API calls.
</action>
  <verify>
pytest tests/test_minimax_integration.py -v --cov=backend/core/llm/minimax_integration.py --cov-report=term-missing
  </verify>
  <done>
17 tests created, all passing, >80% coverage for minimax_integration.py, graceful fallback verified
</done>
</task>

## Verification

1. **MiniMax client initializes correctly**:
   ```bash
   python -c "
   from core.llm.minimax_integration import MiniMaxIntegration
   m = MiniMaxIntegration('test-key')
   assert m.BASE_URL == 'https://api.minimaxi.com/v1'
   assert m.get_pricing()['input_cost_per_token'] == 0.000001
   assert m.get_capabilities()['tier'].value == 'standard'
   "
   ```

2. **Pricing fetcher includes MiniMax**:
   ```bash
   python -c "
   from core.dynamic_pricing_fetcher import get_pricing_fetcher
   f = get_pricing_fetcher()
   p = f.get_model_price('minimax-m2.5')
   assert p is not None
   assert f.is_pricing_estimated('minimax-m2.5') == True
   "
   ```

3. **Benchmark score defined**: get_quality_score("minimax-m2.5") == 88

4. **Test coverage**: >80% for minimax_integration module

## Success Criteria

1. MiniMaxIntegration class with async httpx client
2. generate() method with proper error handling (429 -> RateLimitedError)
3. Estimated pricing ($1/M) with "estimated" source marker
4. Quality score 88 positioned between gemini-2.0-flash and deepseek-chat
5. BYOK routing includes MiniMax in STANDARD tier
6. 17+ tests covering API wrapper, pricing, and integration
7. Graceful fallback when MiniMax unavailable

## Output

After completion, create `.planning/phases/68-byok-cognitive-tier-system/68-04-SUMMARY.md` with:
- MiniMax API wrapper functionality verified
- Pricing and benchmark integration status
- Test coverage report
- Note on official pricing availability (when API opens)
