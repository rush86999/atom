---
phase: 68-byok-cognitive-tier-system
plan: 03
type: execute
wave: 2
depends_on: [68-01]
files_modified:
  - backend/core/llm/escalation_manager.py
  - backend/core/models.py
  - backend/alembic/versions/xxx_add_escalation_log.py
  - backend/tests/test_escalation_manager.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - Automatic escalation triggers on quality threshold breaches (<80 score)
    - Rate limit errors trigger immediate escalation to next tier
    - Escalation cooldown (5 minutes) prevents rapid tier cycling
    - All escalations logged to database for analytics
    - Max escalation limit prevents infinite loops
  artifacts:
    - path: backend/core/llm/escalation_manager.py
      provides: EscalationManager class with automatic quality-based escalation
      min_lines: 300
    - path: backend/core/models.py
      provides: EscalationLog database model
      contains: "class EscalationLog"
    - path: backend/alembic/versions/xxx_add_escalation_log.py
      provides: Database migration for EscalationLog table
      min_lines: 50
    - path: backend/tests/test_escalation_manager.py
      provides: Escalation logic tests with cooldown verification
      min_lines: 350
  key_links:
    - from: backend/core/llm/escalation_manager.py
      to: backend/core/llm/cognitive_tier_system.py
      via: from core.llm.cognitive_tier_system import CognitiveTier
      pattern: "from core.llm.cognitive_tier_system import CognitiveTier"
    - from: backend/core/models.py
      to: backend/core/llm/escalation_manager.py
      via: EscalationLog model import
      pattern: "from core.models import EscalationLog"

## Objective

Implement automatic escalation manager that monitors LLM response quality and failures, automatically escalating queries to higher cognitive tiers when thresholds are breached. Includes cooldown period to prevent rapid cycling and database logging for analytics.

**Purpose:** Maintain response quality while minimizing costs through automatic tier escalation only when needed.

**Output:** EscalationManager with cooldown, quality monitoring, and database logging.

## Context

@/Users/rushiparikh/projects/atom/.planning/phases/68-byok-cognitive-tier-system/68-RESEARCH.md
@/Users/rushiparikh/projects/atom/backend/core/llm/cognitive_tier_system.py (from Plan 01)
@/Users/rushiparikh/projects/atom/backend/core/models.py

## Tasks

<task type="auto">
  <name>Task 1: Create EscalationManager with cooldown logic</name>
  <files>backend/core/llm/escalation_manager.py</files>
  <action>
Create new file `backend/core/llm/escalation_manager.py` with:

1. **EscalationReason enum**:
   ```python
   class EscalationReason(Enum):
       LOW_CONFIDENCE = "low_confidence"
       RATE_LIMITED = "rate_limited"
       ERROR_RESPONSE = "error_response"
       QUALITY_THRESHOLD = "quality_threshold"
       USER_REQUEST = "user_request"
   ```

2. **Escalation thresholds**:
   ```python
   ESCALATION_THRESHOLDS = {
       EscalationReason.LOW_CONFIDENCE: {"confidence": 0.7, "max_retries": 2},
       EscalationReason.QUALITY_THRESHOLD: {"min_quality_score": 80, "max_retries": 1},
       EscalationReason.RATE_LIMITED: {"max_retries": 3},
       EscalationReason.ERROR_RESPONSE: {"max_retries": 2},
   }
   ESCALATION_COOLDOWN = 5  # minutes
   MAX_ESCALATION_LIMIT = 2  # max tier jumps per request
   ```

3. **EscalationManager class** with methods:
   - `should_escalate(current_tier, response_quality, error, rate_limited) -> Tuple[bool, Optional[EscalationReason], Optional[CognitiveTier]]`:
     - Check cooldown first (_is_on_cooldown)
     - Rate limit escalation (highest priority)
     - Error response escalation
     - Low quality escalation
     - Return (should_escalate, reason, target_tier)

   - `_escalate_for_reason(current_tier, reason) -> Tuple[bool, EscalationReason, CognitiveTier]`:
     - Calculate target tier (next in tier_order)
     - Record escalation (_record_escalation)
     - Set cooldown timestamp
     - Handle max tier (COMPLEX) case

   - `_is_on_cooldown(tier) -> bool`:
     - Check if tier in escalation_log
     - Compare last_escalation + ESCALATION_COOLDOWN > now

   - `_record_escalation(from_tier, to_tier, reason)`:
     - Add to in-memory escalation_log dict
     - Persist to database (if db_session available)

   - `get_escalation_count(request_id) -> int`:
     - Track escalations per request to prevent infinite loops

4. **In-memory escalation tracking**:
   ```python
   def __init__(self, db_session=None):
       self.db = db_session
       self.escalation_log = {}  # {tier_value: datetime}
       self.request_escalations = {}  # {request_id: count}
   ```

5. **Add comprehensive docstrings** explaining:
   - Why 5-minute cooldown (prevents cycling, allows cooldown)
   - Quality threshold (80 = acceptable response quality)
   - Max escalation limit (2 = prevents runaway costs)
</action>
  <verify>
pytest tests/test_escalation_manager.py::test_should_escalate_low_quality -v
pytest tests/test_escalation_manager.py::test_escalation_cooldown -v
  </verify>
  <done>
EscalationReason enum defined with 5 reasons, EscalationManager created with should_escalate(), cooldown logic implemented, _record_escalation persists to DB
</done>
</task>

<task type="auto">
  <name>Task 2: Add EscalationLog model to database</name>
  <files>backend/core/models.py</files>
  <action>
Extend `backend/core/models.py` with EscalationLog model:

Add at end of models file (before relationships if any):

```python
class EscalationLog(Base):
    """Database log of all tier escalations for analytics"""
    __tablename__ = "escalation_log"

    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    workspace_id = Column(String, ForeignKey("workspaces.id"), nullable=False, index=True)
    request_id = Column(String, nullable=False, index=True)  # Track escalations per request

    # Escalation details
    from_tier = Column(String, nullable=False)  # micro, standard, versatile, heavy, complex
    to_tier = Column(String, nullable=False)
    reason = Column(String, nullable=False)  # EscalationReason enum value
    trigger_value = Column(Float, nullable=True)  # quality_score or confidence that triggered

    # Response context
    provider_id = Column(String, nullable=True)  # openai, deepseek, etc.
    model = Column(String, nullable=True)
    error_message = Column(Text, nullable=True)

    # Metadata
    prompt_length = Column(Integer, nullable=True)
    estimated_tokens = Column(Integer, nullable=True)
    metadata_json = Column(JSON, nullable=True)  # Flexible context storage

    # Timestamps
    created_at = Column(DateTime(timezone=True), server_default=func.now())

    # Relationships
    workspace = relationship("Workspace", backref="escalation_logs")
```

DO NOT create any indexes beyond what's specified (id primary key, foreign key indexes).

Import datetime at top if not already present (should be).
</action>
  <verify>
python -c "
from core.models import EscalationLog
print('EscalationLog model has workspace_id:', hasattr(EscalationLog, 'workspace_id'))
print('EscalationLog has from_tier:', hasattr(EscalationLog, 'from_tier'))
"
  </verify>
  <done>
EscalationLog model added to models.py with all required fields (workspace_id, request_id, from_tier, to_tier, reason, trigger_value, provider_id, model, error_message, timestamps)
</done>
</task>

<task type="auto">
  <name>Task 3: Create database migration for EscalationLog</name>
  <files>backend/alembic/versions/xxx_add_escalation_log.py</files>
  <action>
Create Alembic migration file:

1. Generate new migration:
   ```bash
   cd backend && alembic revision -m "add escalation log table"
   ```

2. Edit the generated migration file to include:
   ```python
   from alembic import op
   import sqlalchemy as sa
   from sqlalchemy.dialects import postgresql

   def upgrade():
       op.create_table(
           'escalation_log',
           sa.Column('id', sa.String(), nullable=False),
           sa.Column('workspace_id', sa.String(), nullable=False),
           sa.Column('request_id', sa.String(), nullable=False),
           sa.Column('from_tier', sa.String(), nullable=False),
           sa.Column('to_tier', sa.String(), nullable=False),
           sa.Column('reason', sa.String(), nullable=False),
           sa.Column('trigger_value', sa.Float(), nullable=True),
           sa.Column('provider_id', sa.String(), nullable=True),
           sa.Column('model', sa.String(), nullable=True),
           sa.Column('error_message', sa.Text(), nullable=True),
           sa.Column('prompt_length', sa.Integer(), nullable=True),
           sa.Column('estimated_tokens', sa.Integer(), nullable=True),
           sa.Column('metadata_json', sa.JSON(), nullable=True),
           sa.Column('created_at', sa.DateTime(timezone=True), nullable=True),
           sa.ForeignKeyConstraint(['workspace_id'], ['workspaces.id']),
           sa.PrimaryKeyConstraint('id')
       )
       op.create_index('ix_escalation_log_workspace_id', 'escalation_log', ['workspace_id'])
       op.create_index('ix_escalation_log_request_id', 'escalation_log', ['request_id'])

   def downgrade():
       op.drop_index('ix_escalation_log_request_id', table_name='escalation_log')
       op.drop_index('ix_escalation_log_workspace_id', table_name='escalation_log')
       op.drop_table('escalation_log')
   ```

DO NOT use `alembic revision --autogenerate` - write the migration explicitly as shown.

The migration filename will vary (use actual generated filename).
</action>
  <verify>
cd backend && alembic upgrade head && python -c "
from sqlalchemy import inspect
from core.database import engine
inspector = inspect(engine.engine)
print('escalation_log table exists:', 'escalation_log' in inspector.get_table_names())
"
  </verify>
  <done>
Migration file created, escalation_log table created after upgrade, all columns and indexes present
</done>
</task>

<task type="auto">
  <name>Task 4: Create escalation manager tests</name>
  <files>backend/tests/test_escalation_manager.py</files>
  <action>
Create `backend/tests/test_escalation_manager.py` with:

1. **Escalation decision tests** (8 tests):
   - test_should_escalate_low_quality: quality < 80 triggers escalation
   - test_should_escalate_rate_limited: rate_limited=True triggers escalation
   - test_should_escalate_error_response: error message triggers escalation
   - test_should_not_escalate_good_quality: quality >= 80 no escalation
   - test_on_cooldown_blocks_escalation: Recent escalation blocks new one
   - test_max_tier_no_escalation: COMPLEX tier cannot escalate further
   - test_max_escalation_limit: 2 escalations max per request
   - test_cooldown_expires_after_5_minutes: Escalation allowed after cooldown

2. **Escalation logic tests** (6 tests):
   - test_escalate_to_next_tier: MICRO -> STANDARD progression
   - test_escalation_reasons_mapped: All 5 reasons handled correctly
   - test_quality_threshold_config: 80 is minimum acceptable
   - test_rate_limit_max_retries: 3 retries for rate limits
   - test_confidence_threshold: 0.7 confidence threshold
   - test_multiple_escalations_stack: MICRO -> STANDARD -> VERSATILE

3. **Database logging tests** (5 tests):
   - test_escalation_logged_to_db: Escalation creates EscalationLog record
   - test_log_contains_all_fields: from_tier, to_tier, reason, trigger_value
   - test_workspace_id_in_log: Workspace关联正确
   - test_request_id_tracking: Same request_id for escalations
   - test_error_message_logged: Error details captured

4. **Performance tests** (2 tests):
   - test_should_escalate_performance: <5ms decision time
   - test_cooldown_check_performance: <1ms lookup

5. **Integration tests** (3 tests):
   - test_with_cognitive_tier_system: Works with CognitiveTier enum
   - test_database_session_required: Works with real DB session
   - test_escalation_count_tracking: Per-request escalation counting

Use pytest fixtures for CognitiveTier instances and mock database sessions.
</action>
  <verify>
pytest tests/test_escalation_manager.py -v --cov=backend/core/llm/escalation_manager.py --cov-report=term-missing
  </verify>
  <done>
24 tests created, all passing, >80% coverage for escalation_manager.py, cooldown logic verified, database logging functional
</done>
</task>

## Verification

1. **Escalation triggers correctly**:
   ```bash
   python -c "
   from core.llm.escalation_manager import EscalationManager, EscalationReason
   from core.llm.cognitive_tier_system import CognitiveTier
   m = EscalationManager()
   should, reason, target = m.should_escalate(CognitiveTier.STANDARD, response_quality=70, error=None, rate_limited=False)
   assert should == True
   assert reason == EscalationReason.QUALITY_THRESHOLD
   assert target == CognitiveTier.VERSATILE
   "
   ```

2. **Cooldown prevents cycling**: Second escalation within 5 minutes blocked

3. **Database logging**: EscalationLog records created with correct fields

4. **Test coverage**: >80% for escalation_manager module

## Success Criteria

1. EscalationManager with 5 escalation reasons (LOW_CONFIDENCE, RATE_LIMITED, ERROR_RESPONSE, QUALITY_THRESHOLD, USER_REQUEST)
2. Quality threshold of 80 triggers escalation
3. 5-minute cooldown prevents rapid tier cycling
4. EscalationLog model with workspace_id, request_id, from_tier, to_tier, reason
5. Database migration applies cleanly (upgrade/downgrade)
6. 24+ tests covering all escalation scenarios
7. Performance <5ms for escalation decision

## Output

After completion, create `.planning/phases/68-byok-cognitive-tier-system/68-03-SUMMARY.md` with:
- Escalation trigger accuracy metrics
- Cooldown effectiveness (if measured)
- Test coverage report
- Performance benchmarks
- Migration SQL statements
