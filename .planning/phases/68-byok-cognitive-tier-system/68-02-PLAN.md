---
phase: 68-byok-cognitive-tier-system
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/core/llm/cache_aware_router.py
  - backend/core/llm/byok_handler.py
  - backend/core/dynamic_pricing_fetcher.py
  - backend/tests/test_cache_aware_routing.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - Router calculates effective cost accounting for prompt caching (10% cached price)
    - Cache hit probability is predicted based on historical data per workspace/model
    - Providers without caching support are scored with full cost multiplier
    - Prompts below minimum token thresholds (1-2k) get cache probability = 0
    - Effective cost calculation completes in <10ms per provider
  artifacts:
    - path: backend/core/llm/cache_aware_router.py
      provides: CacheAwareRouter class with cost calculation
      min_lines: 250
    - path: backend/core/llm/byok_handler.py
      provides: Cache-aware routing integration
      exports: ["get_ranked_providers"]
    - path: backend/tests/test_cache_aware_routing.py
      provides: Cache hit prediction and cost calculation tests
      min_lines: 300
  key_links:
    - from: backend/core/llm/cache_aware_router.py
      to: backend/core/dynamic_pricing_fetcher.py
      via: get_pricing_fetcher().get_model_price()
      pattern: "from core.dynamic_pricing_fetcher import get_pricing_fetcher"
    - from: backend/core/llm/byok_handler.py
      to: backend/core/llm/cache_aware_router.py
      via: CacheAwareRouter.calculate_effective_cost()
      pattern: "cache_router.calculate_effective_cost"

## Objective

Implement cache-aware cost scoring for LLM routing decisions. The system accounts for prompt caching capabilities (OpenAI, Anthropic, Gemini offer cached tokens at ~10% cost) when ranking providers, potentially routing to a more expensive model with good caching over a cheaper model without caching support.

**Purpose:** Achieve up to 90% cost reduction for cached prompts while maintaining routing speed <100ms.

**Output:** CacheAwareRouter integrated into BYOK ranking with historical cache hit tracking.

## Context

@/Users/rushiparikh/projects/atom/.planning/phases/68-byok-cognitive-tier-system/68-RESEARCH.md
@/Users/rushiparikh/projects/atom/backend/core/dynamic_pricing_fetcher.py
@/Users/rushiparikh/projects/atom/backend/core/llm/byok_handler.py

## Tasks

<task type="auto">
  <name>Task 1: Create CacheAwareRouter with cost calculation</name>
  <files>backend/core/llm/cache_aware_router.py</files>
  <action>
Create new file `backend/core/llm/cache_aware_router.py` with:

1. **Cache capability constants** (research-verified):
   ```python
   CACHE_CAPABILITIES = {
       "openai": {"supports_cache": True, "cached_cost_ratio": 0.10, "min_tokens": 1024},
       "anthropic": {"supports_cache": True, "cached_cost_ratio": 0.10, "min_tokens": 2048},
       "gemini": {"supports_cache": True, "cached_cost_ratio": 0.10, "min_tokens": 1024},
       "deepseek": {"supports_cache": False, "cached_cost_ratio": 1.0, "min_tokens": 0},
       "minimax": {"supports_cache": False, "cached_cost_ratio": 1.0, "min_tokens": 0},
   }
   ```

2. **CacheAwareRouter class** with methods:
   - `calculate_effective_cost(model, provider, estimated_input_tokens, cache_hit_probability=0.5) -> float`:
     - Fetch pricing from DynamicPricingFetcher
     - Check if provider supports caching
     - Return 0 (infinite cost) if model pricing unknown
     - If below min_tokens threshold: return full price
     - Calculate: `input_cost * (cache_hit_prob * cached_ratio + (1 - cache_hit_prob) * 1.0)`

   - `predict_cache_hit_probability(prompt_hash, workspace_id) -> float`:
     - Look up historical cache hits from cache_hit_history dict
     - Return actual hit rate if data exists
     - Default to 0.5 if no history (industry average)

   - `record_cache_outcome(prompt_hash, workspace_id, was_cached)`:
     - Update in-memory cache_hit_history dict
     - Track [hits, total] per workspace:prompt_hash key

   - `get_provider_cache_capability(provider) -> Dict`:
     - Return CACHE_CAPABILITIES entry or default (no cache)

3. **In-memory history storage**:
   ```python
   def __init__(self, pricing_fetcher):
       self.pricing_fetcher = pricing_fetcher
       self.cache_hit_history = {}  # {"workspace_id:prompt_hash": [hits, total]}
   ```

4. **Add comprehensive docstrings** explaining:
   - Why cached_cost_ratio is 0.10 (OpenAI/Anthropic pricing)
   - How min_tokens threshold affects cache eligibility
   - Default 0.5 probability (industry average from research)

DO NOT persist to database in this plan - in-memory is sufficient for initial implementation.
</action>
  <verify>
pytest tests/test_cache_aware_routing.py::test_calculate_effective_cost -v
pytest tests/test_cache_aware_routing.py::test_predict_cache_hit_probability -v
  </verify>
  <done>
CacheAwareRouter class created, calculate_effective_cost() returns cache-adjusted pricing, predict_cache_hit_probability() returns 0-1 probability, record_cache_outcome() updates history
</done>
</task>

<task type="auto">
  <name>Task 2: Extend DynamicPricingFetcher with cache metadata</name>
  <files>backend/core/dynamic_pricing_fetcher.py</files>
  <action>
Extend `backend/core/dynamic_pricing_fetcher.py`:

1. Add `supports_cache` field to pricing transform in `fetch_litellm_pricing()`:
   ```python
   pricing[model_name] = {
       "input_cost_per_token": model_data.get("input_cost_per_token", 0),
       "output_cost_per_token": model_data.get("output_cost_per_token", 0),
       "max_tokens": model_data.get("max_tokens", 0),
       "max_input_tokens": model_data.get("max_input_tokens", 0),
       "max_output_tokens": model_data.get("max_output_tokens", 0),
       "litellm_provider": model_data.get("litellm_provider", "unknown"),
       "mode": model_data.get("mode", "chat"),
       "source": "litellm",
       "supports_cache": model_data.get("supports_cache", False),  # NEW
   }
   ```

2. Update `_save_cache()` to handle new field (no breaking change)

3. Add method to check model cache support:
   ```python
   def model_supports_cache(self, model_name: str) -> bool:
       pricing = self.get_model_price(model_name)
       if pricing:
           return pricing.get("supports_cache", False)
       # Fallback to provider-based check
       provider = self._infer_provider(model_name)
       return provider in ["openai", "anthropic", "google"]
   ```

4. Add `get_cache_min_tokens(model_name: str) -> int`:
   - OpenAI: 1024, Anthropic: 2048, Gemini: 1024
   - Returns 0 if no caching support

DO NOT require re-fetch - use defaults if field not in existing cache.
</action>
  <verify>
python -c "
from core.dynamic_pricing_fetcher import get_pricing_fetcher
f = get_pricing_fetcher()
print('GPT-4o cache:', f.model_supports_cache('gpt-4o'))
print('DeepSeek cache:', f.model_supports_cache('deepseek-chat'))
"
  </verify>
  <done>
DynamicPricingFetcher extended with supports_cache field, model_supports_cache() method works for known providers, get_cache_min_tokens() returns correct thresholds
</done>
</task>

<task type="auto">
  <name>Task 3: Integrate cache-aware routing into BYOKHandler</name>
  <files>backend/core/llm/byok_handler.py</files>
  <action>
Extend `backend/core/llm/byok_handler.py`:

1. Add import and initialization:
   ```python
   from core.llm.cache_aware_router import CacheAwareRouter

   def __init__(self, workspace_id: str = "default", provider_id: str = "auto"):
       # ... existing code ...
       from core.dynamic_pricing_fetcher import get_pricing_fetcher
       self.cache_router = CacheAwareRouter(get_pricing_fetcher())
   ```

2. Extend `get_ranked_providers()` signature:
   - Add parameters: `estimated_tokens: int = 1000`, `workspace_id: str = "default"`
   - Keep backward compatible defaults

3. Modify BPC scoring to use effective cost:
   ```python
   # Generate prompt hash for cache prediction
   import hashlib
   prompt_hash = hashlib.sha256(f"{workspace_id}:{model}".encode()).hexdigest()
   cache_hit_prob = self.cache_router.predict_cache_hit_probability(prompt_hash, workspace_id)

   # Calculate cache-aware effective cost
   effective_cost = self.cache_router.calculate_effective_cost(
       model, provider_id, estimated_tokens, cache_hit_prob
   )

   # Use effective_cost in value_score calculation
   value_score = (quality_score ** 2) / (effective_cost * 1e6)
   ```

4. Add cache outcome recording after successful generation (in generate_response):
   ```python
   # After successful API call, record if cached
   was_cached = response_usage.get("cached", False) if hasattr(response, 'usage') else False
   self.cache_router.record_cache_outcome(prompt_hash, workspace_id, was_cached)
   ```

DO NOT break existing callers - new parameters are optional with sensible defaults.
</action>
  <verify>
pytest tests/test_cache_aware_routing.py::test_byok_integration -v
pytest tests/test_cache_aware_routing.py::test_backward_compatibility -v
  </verify>
  <done>
BYOKHandler uses cache_router for effective cost calculation, get_ranked_providers() accepts estimated_tokens/workspace_id, cache outcomes recorded after generation
</done>
</task>

<task type="auto">
  <name>Task 4: Create cache-aware routing tests</name>
  <files>backend/tests/test_cache_aware_routing.py</files>
  <action>
Create `backend/tests/test_cache_aware_routing.py` with:

1. **Effective cost calculation tests** (6 tests):
   - test_cached_provider_cost_reduction: OpenAI with 90% cache hit = 10% effective cost
   - test_uncached_provider_full_cost: DeepSeek = 100% effective cost
   - test_below_min_threshold_no_cache: 500 tokens < 1024 threshold = full cost
   - test_zero_cache_hit_probability: No cache expected = full cost
   - test_hundred_percent_cache_hit: Full cache = 10% cost
   - test_unknown_model_infinite_cost: Unknown model returns inf

2. **Cache hit prediction tests** (4 tests):
   - test_default_probability_no_history: Returns 0.5 for unknown
   - test_actual_hit_rate_from_history: Returns hits/total ratio
   - test_workspace_specific_tracking: Different workspaces have separate histories
   - test_prompt_hash_prefix_keying: Uses first 16 chars of hash

3. **Cache outcome recording tests** (3 tests):
   - test_record_hit_increments_counter: Hit increments both hit and total
   - test_record_miss_increments_total_only: Miss increments total only
   - test_multiple_outcomes_update_correctly: Hit rate calculated correctly

4. **BYOK integration tests** (5 tests):
   - test_cache_aware_ranking: Cached model ranked higher than cheaper uncached
   - test_cache_unavailable_below_threshold: Short prompts don't benefit from cache
   - test_byok_integration_with_estimated_tokens: Parameter passed through correctly
   - test_backward_compatibility_no_params: Works without new parameters
   - test_cache_outcome_recording: Called after generation

5. **Performance tests** (2 tests):
   - test_effective_cost_calculation_performance: <10ms per provider
   - test_cache_hit_prediction_performance: <1ms lookup

Use pytest fixtures for provider configurations. Mock DynamicPricingFetcher with test data.
</action>
  <verify>
pytest tests/test_cache_aware_routing.py -v --cov=backend/core/llm/cache_aware_router.py --cov-report=term-missing
  </verify>
  <done>
20 tests created, all passing, >80% coverage for cache_aware_router.py, performance targets met (<10ms cost calculation, <1ms prediction)
</done>
</task>

## Verification

1. **Cost calculation accuracy**:
   ```bash
   python -c "
   from core.llm.cache_aware_router import CacheAwareRouter
   from core.dynamic_pricing_fetcher import get_pricing_fetcher
   r = CacheAwareRouter(get_pricing_fetcher())
   # GPT-4o with 90% cache hit should cost ~10% of list price
   cost = r.calculate_effective_cost('gpt-4o', 'openai', 2000, 0.9)
   assert cost < 0.000001  # ~10% of ~$0.00001/token
   "
   ```

2. **Cache hit prediction**: Returns 0.5 default, actual rate when history exists

3. **BYOK integration**: Cached models ranked appropriately in get_ranked_providers()

4. **Test coverage**: >80% for cache_aware_router module

## Success Criteria

1. CacheAwareRouter calculates effective cost with cached_cost_ratio (0.10) applied
2. Cache hit prediction uses historical data (workspace:model) with 0.5 default
3. Minimum token thresholds enforced (1024 for OpenAI/Gemini, 2048 for Anthropic)
4. BYOK ranking uses effective cost for value score calculation
5. 20+ tests covering all routing scenarios
6. Performance <10ms per provider cost calculation

## Output

After completion, create `.planning/phases/68-byok-cognitive-tier-system/68-02-SUMMARY.md` with:
- Cost reduction calculations (theoretical 90% with caching)
- Cache hit prediction accuracy (if measured)
- Test coverage report
- Performance benchmarks
