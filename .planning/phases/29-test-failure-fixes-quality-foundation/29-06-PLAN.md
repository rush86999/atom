---
phase: 29-test-failure-fixes-quality-foundation
plan: 06
type: execute
wave: 2
depends_on: [29-01, 29-02, 29-03, 29-04, 29-05]
files_modified:
  - backend/tests/
  - .planning/phases/29-test-failure-fixes-quality-foundation/29-VERIFICATION.md
autonomous: true

must_haves:
  truths:
    - "Test suite achieves 98%+ pass rate over 3 consecutive full runs"
    - "Test suite completes in <60 minutes with parallel execution"
    - "Zero flaky tests confirmed via 3-run verification"
  artifacts:
    - path: ".planning/phases/29-test-failure-fixes-quality-foundation/29-VERIFICATION.md"
      provides: "Verification report with test pass rates, execution time, and flakiness analysis"
      contains: "pass_rate, execution_time, flaky_tests, regression_tests"
    - path: "backend/tests/FLAKY_TEST_AUDIT.md"
      provides: "Updated flaky test audit with verification results"
      contains: "verified_stable_tests, remaining_flaky_tests"
  key_links:
    - from: "29-06-PLAN.md"
      to: "29-01-PLAN.md, 29-02-PLAN.md, 29-03-PLAN.md, 29-04-PLAN.md, 29-05-PLAN.md"
      via: "verifies all previous fixes meet quality gates"
      pattern: "depends_on: [29-01, 29-02, 29-03, 29-04, 29-05]"
---

<objective>
Verify TQ-02, TQ-03, TQ-04 quality gates after all test fixes.

**TQ-02 (98%+ pass rate)**: Run full test suite 3 times, verify pass rate >= 98%

**TQ-03 (<60min execution)**: Run test suite with parallel execution, verify completion < 60 minutes

**TQ-04 (zero flaky tests)**: Identify flaky tests via 3-run verification, fix or document

**Purpose**: Confirm all test fixes from plans 01-05 produce a stable test baseline for Phase 30 coverage push.

**Output**: Verification report with pass rates, execution time, and flakiness analysis.
</objective>

<execution_context>
@/Users/rushiparikh/.claude/get-shit-done/workflows/execute-plan.md
@/Users/rushiparikh/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP_V2.md
@.planning/STATE.md
@.planning/phases/29-test-failure-fixes-quality-foundation/29-01-PLAN.md
@.planning/phases/29-test-failure-fixes-quality-foundation/29-02-PLAN.md
@.planning/phases/29-test-failure-fixes-quality-foundation/29-03-PLAN.md
@.planning/phases/29-test-failure-fixes-quality-foundation/29-04-PLAN.md
@.planning/phases/29-test-failure-fixes-quality-foundation/29-05-PLAN.md
@backend/tests/conftest.py
@backend/tests/docs/FLAKY_TEST_GUIDE.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Verify TQ-02 - 98%+ pass rate over 3 consecutive runs</name>
  <files>
    .planning/phases/29-test-failure-fixes-quality-foundation/29-VERIFICATION.md
  </files>
  <action>
    Verify TQ-02: Run full test suite 3 times and calculate pass rate.

    **Step 1: Run test suite 3 times**
    ```bash
    cd /Users/rushiparikh/projects/atom/backend
    mkdir -p ../.planning/phases/29-test-failure-fixes-quality-foundation/test-results

    for run in 1 2 3; do
        echo "=== Run $run ===" | tee -a ../.planning/phases/29-test-failure-fixes-quality-foundation/test-results/run-$run.log
        PYTHONPATH=/Users/rushiparikh/projects/atom/backend pytest tests/ \
            -v \
            --tb=short \
            --durations=20 \
            | tee -a ../.planning/phases/29-test-failure-fixes-quality-foundation/test-results/run-$run.log
    done
    ```

    **Step 2: Parse results and calculate pass rate**
    ```bash
    # Extract test counts from each run
    for run in 1 2 3; do
        grep -E "passed|failed|error" ../.planning/phases/29-test-failure-fixes-quality-foundation/test-results/run-$run.log
    done

    # Calculate pass rate: (passed / total) * 100
    # Formula: pass_rate = (total_passed - total_failed) / total_passed * 100
    ```

    **Step 3: Verify pass rate >= 98%**
    - If pass_rate >= 98: Success
    - If pass_rate < 98: Identify failing tests and determine if they're:
      - Fixable regressions (root cause fix needed)
      - Expected failures (marked with pytest.mark.xfail)
      - Flaky tests (address in Task 3)

    **Step 4: Document results**
    Create 29-VERIFICATION.md with:
    - Run 1: X passed, Y failed, Z% pass rate
    - Run 2: X passed, Y failed, Z% pass rate
    - Run 3: X passed, Y failed, Z% pass rate
    - Overall: 98%+ pass rate achieved (or not)
    - Failing tests list (if any)

    **Why**: TQ-02 ensures test suite is stable enough for coverage push. 98% threshold allows 2% of tests to fail (skipped, xfailed, or new issues) while catching real regressions.
  </action>
  <verify>
    # Check pass rate in verification report
    grep "pass rate" .planning/phases/29-test-failure-fixes-quality-foundation/29-VERIFICATION.md
  </verify>
  <done>
    TQ-02 verified: Test suite achieves 98%+ pass rate over 3 consecutive full runs.
    - Run 1: X/Y passed, Z% pass rate
    - Run 2: X/Y passed, Z% pass rate
    - Run 3: X/Y passed, Z% pass rate
    - Overall: >= 98% pass rate
  </done>
</task>

<task type="auto">
  <name>Task 2: Verify TQ-03 - Test suite completes in <60 minutes with parallel execution</name>
  <files>
    .planning/phases/29-test-failure-fixes-quality-foundation/29-VERIFICATION.md
  </files>
  <action>
    Verify TQ-03: Run test suite with pytest-xdist and measure execution time.

    **Step 1: Install pytest-xdist (if not installed)**
    ```bash
    pip install pytest-xdist
    ```

    **Step 2: Run test suite in parallel**
    ```bash
    cd /Users/rushiparikh/projects/atom/backend
    time PYTHONPATH=/Users/rushiparikh/projects/atom/backend pytest tests/ \
        -n auto \
        -v \
        --tb=short \
        --durations=20 \
        | tee ../.planning/phases/29-test-failure-fixes-quality-foundation/test-results/parallel-run.log
    ```

    **Step 3: Measure execution time**
    - Parse `real` time from `time` output
    - Or check pytest duration summary: "=== test session starts ===" to "=== test session finishes ==="

    **Step 4: Verify < 60 minutes**
    - If execution_time < 3600 seconds (60 minutes): Success
    - If execution_time >= 3600: Identify slow tests and consider:
      - Splitting slow tests into separate suite
      - Increasing parallelism (-n auto â†’ -n 8 for 8 workers)
      - Marking slow tests with @pytest.mark.slow

    **Step 5: Document results**
    Update 29-VERIFICATION.md with:
    - Parallel execution time: X minutes
    - Worker count: Y (from -n auto)
    - Slowest 20 tests (from --durations=20)
    - Performance recommendations (if any)

    **Why**: TQ-03 ensures test suite is fast enough for CI/CD. < 60 minutes allows for:
    - Quick feedback on PRs
    - Multiple test runs per day
    - Reasonable CI queue times

    **Optimization tips** (if slow):
    - Use `-n auto` for automatic worker count
    - Exclude slow tests from default suite: `pytest tests/ -m "not slow"`
    - Split unit tests (fast) from integration tests (slow)
  </action>
  <verify>
    # Check execution time in verification report
    grep "execution time" .planning/phases/29-test-failure-fixes-quality-foundation/29-VERIFICATION.md
  </verify>
  <done>
    TQ-03 verified: Test suite completes in <60 minutes with parallel execution.
    - Execution time: X minutes (target: <60)
    - Workers: Y
    - Performance: Optimal (or needs optimization)
  </done>
</task>

<task type="auto">
  <name>Task 3: Verify TQ-04 - Zero flaky tests via 3-run verification</name>
  <files>
    .planning/phases/29-test-failure-fixes-quality-foundation/29-VERIFICATION.md
    backend/tests/FLAKY_TEST_AUDIT.md
  </files>
  <action>
    Verify TQ-04: Identify and fix flaky tests via 3-run verification.

    **Step 1: Identify flaky tests from 3-run data**
    ```bash
    # Compare failures across runs
    cd /Users/rushiparikh/projects/atom/backend

    # Extract failed tests from each run
    for run in 1 2 3; do
        grep "FAILED" ../.planning/phases/29-test-failure-fixes-quality-foundation/test-results/run-$run.log | \
            awk '{print $2}' | \
            sort > ../.planning/phases/29-test-failure-fixes-quality-foundation/test-results/failed-run-$run.txt
    done

    # Find tests that failed in some runs but not others (flaky)
    comm -12 failed-run-1.txt failed-run-2.txt | comm -23 - failed-run-3.txt > flaky-tests.txt
    ```

    **Step 2: Analyze flaky test patterns**
    For each flaky test, categorize:
    - **Race condition**: Add explicit synchronization (see 29-04-PLAN.md)
    - **Shared state**: Use db_session, unique_resource_name (see FLAKY_TEST_GUIDE.md)
    - **External dependency**: Mock external services
    - **Time dependency**: Use freezegun or increase timeout
    - **Order dependency**: Make test independent

    **Step 3: Fix flaky tests or document**
    For each flaky test:
    - If fixable: Apply fix pattern (sync, mock, isolate state)
    - If known issue: Mark with `@pytest.mark.flaky(reruns=3)` with TODO comment
    - If external bug: File issue and document

    **Step 4: Re-verify 3-run stability**
    ```bash
    for run in 1 2 3; do
        PYTHONPATH=/Users/rushiparikh/projects/atom/backend pytest tests/ \
            -v \
            --tb=short \
            | tee -a ../.planning/phases/29-test-failure-fixes-quality-foundation/test-results/verify-run-$run.log
    done

    # Re-check for flaky tests
    # Should return empty
    ```

    **Step 5: Document results**
    Update FLAKY_TEST_AUDIT.md and 29-VERIFICATION.md with:
    - Flaky tests identified: X
    - Flaky tests fixed: Y
    - Flaky tests documented with @pytest.mark.flaky: Z
    - Zero unaddressed flaky tests: Yes/No

    **Why**: TQ-04 ensures test suite is deterministic. Flaky tests erode confidence and mask real failures.

    **Acceptance criteria**:
    - All fixable flaky tests are fixed
    - Remaining flaky tests have @pytest.mark.flaky with TODO/issue link
    - Zero "surprise" flaky tests (all documented)
  </action>
  <verify>
    # Check flaky test count in verification report
    grep "flaky tests" .planning/phases/29-test-failure-fixes-quality-foundation/29-VERIFICATION.md
    grep "unaddressed flaky tests" .planning/phases/29-test-failure-fixes-quality-foundation/29-VERIFICATION.md
  </verify>
  <done>
    TQ-04 verified: Zero flaky tests confirmed via 3-run verification.
    - Flaky tests identified: X
    - Flaky tests fixed: Y
    - Documented with @pytest.mark.flaky: Z
    - Unaddressed flaky tests: 0
  </done>
</task>

</tasks>

<verification>
**Overall Phase Verification**:

1. All quality gates verified:
   - TQ-02: 98%+ pass rate over 3 runs
   - TQ-03: <60min execution with parallel
   - TQ-04: Zero flaky tests

2. Verification report created: `29-VERIFICATION.md`

3. Test suite is stable baseline for Phase 30 coverage push

4. Any remaining issues documented with action plans

**Success Criteria**:
- [ ] TQ-02 pass rate >= 98%
- [ ] TQ-03 execution time < 60 minutes
- [ ] TQ-04 zero unaddressed flaky tests
- [ ] All test fixes from plans 01-05 verified
- [ ] Regression tests added for fixed tests
</verification>

<success_criteria>
1. Test suite achieves 98%+ pass rate:
   - 3 consecutive runs completed
   - Pass rate calculated as (total_passed - total_failed) / total_passed * 100
   - Failing tests documented (if any < 98%)

2. Test suite completes in < 60 minutes:
   - Parallel execution with pytest-xdist (-n auto)
   - Execution time measured from start to finish
   - Performance recommendations documented (if slow)

3. Zero flaky tests confirmed:
   - Flaky tests identified via 3-run comparison
   - Fixable tests fixed (sync, mock, isolate state)
   - Remaining flaky tests marked with @pytest.mark.flaky + TODO

4. All test fixes from plans 01-05 verified:
   - Hypothesis TypeErrors fixed (plan 01)
   - Proposal service tests fixed (plan 02)
   - Graduation governance tests fixed (plan 03)
   - Agent cancellation tests fixed (plan 04)
   - Security/governance tests fixed (plan 05)

5. Regression tests added:
   - Each fix includes regression test to prevent recurrence
   - Regression tests documented in test docstrings
</success_criteria>

<output>
After completion, create `.planning/phases/29-test-failure-fixes-quality-foundation/29-VERIFICATION.md` with:
- TQ-02 results: 98%+ pass rate verification (3 runs, pass counts)
- TQ-03 results: Execution time with parallel execution (worker count, slow tests)
- TQ-04 results: Flaky test analysis (identified, fixed, documented)
- Overall assessment: Ready for Phase 30 coverage push (or not)
- Remaining issues: Any tests that need additional work

Also update `backend/tests/FLAKY_TEST_AUDIT.md` with verification results.
</output>
