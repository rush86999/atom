---
phase: 29-test-failure-fixes-quality-foundation
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/tests/unit/governance/test_proposal_service.py
  - backend/core/proposal_service.py
autonomous: true

must_haves:
  truths:
    - "Proposal service tests mock the correct methods (not module-level patches)"
    - "All 6 proposal service tests pass with proper async mocking"
    - "Tests verify proposal generation, approval workflow, risk assessment, and audit trail"
  artifacts:
    - path: "backend/tests/unit/governance/test_proposal_service.py"
      provides: "Fixed proposal service unit tests (6 tests)"
      exports: ["test_create_action_proposal_for_intern_agent", "test_approve_proposal_executes_action", "test_reject_proposal_blocks_execution", "test_proposal_history_retrieval", "test_get_pending_proposals", "test_proposal_generation_performance"]
    - path: "backend/core/proposal_service.py"
      provides: "ProposalService with testable interfaces"
      exports: ["create_action_proposal", "approve_proposal", "reject_proposal", "get_proposal_history", "get_pending_proposals"]
  key_links:
    - from: "backend/tests/unit/governance/test_proposal_service.py"
      to: "backend/core/proposal_service.py"
      via: "imports ProposalService for unit testing"
      pattern: "from core.proposal_service import ProposalService"
    - from: "backend/tests/unit/governance/test_proposal_service.py"
      to: "backend/tools/canvas_tool.py"
      via: "mocks present_to_canvas for canvas action tests"
      pattern: "patch('tools.canvas_tool.present_to_canvas', new=AsyncMock)"
---

<objective>
Fix proposal service test failures (6 tests with incorrect mock targets).

**Problem**: Proposal service tests use `@patch('core.proposal_service.logger')` and mock at wrong import location. Tests mock internal methods that don't exist or have changed signatures.

**Root Cause**:
1. `test_submit_proposal_for_approval` mocks `core.proposal_service.logger` but logger.info() may not be called
2. `test_execute_canvas_action` mocks `tools.canvas_tool.present_to_canvas` but ProposalService imports differently
3. `test_execute_device_action` mocks `tools.device_tool.execute_device_command` at wrong location
4. Tests mock `_execute_proposed_action` which is a private method with changing implementation

**Purpose**: Ensure proposal service tests verify actual behavior (proposal generation, approval workflow, rejection, audit trail) with correct mocking.

**Output**: 6 proposal service tests passing with proper async mocking.
</objective>

<execution_context>
@/Users/rushiparikh/.claude/get-shit-done/workflows/execute-plan.md
@/Users/rushiparikh/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP_V2.md
@.planning/STATE.md
@backend/tests/unit/governance/test_proposal_service.py
@backend/core/proposal_service.py
@backend/tools/canvas_tool.py
@backend/tools/device_tool.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Fix proposal generation and submission tests</name>
  <files>
    backend/tests/unit/governance/test_proposal_service.py
  </files>
  <action>
    Fix 2 proposal generation tests with incorrect mocks:

    1. **test_submit_proposal_for_approval** (line ~200):
       - Current: `with patch('core.proposal_service.logger') as mock_logger:`
       - Issue: `submit_for_approval` is a no-op (just logs), test may be flaky
       - Fix: Remove logger assertion or verify `submit_for_approval` exists and calls proposal status update
       - Check if `submit_for_approval` actually updates status in database

    2. **test_create_proposal_for_non_intern_agent_logs_warning** (line ~145):
       - Current: Mocks logger.warning
       - Fix: Verify proposal is created regardless of agent maturity (AUTONOMOUS agents don't need proposals but shouldn't error)
       - Remove logger mock if it's unreliable

    **Why**: Logger mocks are flaky — tests should verify behavior, not logging side effects. Focus on proposal creation and database persistence.

    **Code Pattern**:
    ```python
    # Instead of mocking logger:
    proposal = await proposal_service.create_action_proposal(...)
    assert proposal is not None
    assert proposal.status == ProposalStatus.PROPOSED.value
    db_proposal = proposal_service.db.query(AgentProposal).filter(...).first()
    assert db_proposal is not None
    ```
  </action>
  <verify>pytest backend/tests/unit/governance/test_proposal_service.py::TestProposalGeneration::test_create_action_proposal_for_intern_agent -v --tb=short</verify>
  <done>Proposal generation tests create proposals correctly without relying on flaky logger mocks. Tests verify database persistence and proposal fields.</done>
</task>

<task type="auto">
  <name>Task 2: Fix approval workflow tests with correct async mocking</name>
  <files>
    backend/tests/unit/governance/test_proposal_service.py
  </files>
  <action>
    Fix 2 approval workflow tests with incorrect mock targets:

    1. **test_approve_proposal_executes_action** (line ~212):
       - Current: Mocks `_execute_proposed_action` on proposal_service instance
       - Issue: Private method may not exist or have changed signature
       - Fix: Mock the actual tool methods being called (canvas_tool.present_to_canvas, device_tool.execute_device_command)
       - Use `patch.object(proposal_service, '_execute_proposed_action', new=AsyncMock(...))`

    2. **test_reject_proposal_blocks_execution** (line ~299):
       - Current: Mocks `_create_proposal_episode` only
       - Fix: Verify proposal status changes to REJECTED, execution_result contains rejection reason
       - Ensure no action execution occurs for rejected proposals

    **Why**: Tests should mock the actual tools (canvas, device) not internal methods. This verifies integration points.

    **Code Pattern**:
    ```python
    # Mock actual tool at import location
    with patch('tools.canvas_tool.present_to_canvas', new=AsyncMock(return_value="test_canvas_id")), \
         patch.object(proposal_service, '_create_proposal_episode', new=AsyncMock()):
        result = await proposal_service.approve_proposal(proposal_id, user_id)
        assert result["success"] is True
    ```
  </action>
  <verify>pytest backend/tests/unit/governance/test_proposal_service.py::TestApprovalWorkflow -v --tb=short</verify>
  <done>Approval workflow tests mock actual tool methods correctly. Tests verify approved proposals execute actions and rejected proposals block execution.</done>
</task>

<task type="auto">
  <name>Task 3: Fix audit trail and performance tests</name>
  <files>
    backend/tests/unit/governance/test_proposal_service.py
  </files>
  <action>
    Fix 2 audit trail and performance tests with incorrect mock targets:

    1. **test_proposal_history_retrieval** (line ~492):
       - Current: Creates proposals and calls `get_proposal_history`
       - Issue: May fail if `get_proposal_history` implementation changed
       - Fix: Verify history returns proposals in correct order (newest first)
       - Check pagination limit works

    2. **test_proposal_generation_performance** (line ~619):
       - Current: Measures proposal generation time
       - Issue: May be flaky due to system load
       - Fix: Increase timeout threshold or use mock for slow operations
       - Consider marking as `@pytest.mark.slow` if consistently slow

    **Why**: Audit trail tests verify proposal logging for compliance. Performance tests ensure proposal generation meets SLA (<500ms per requirement).

    **Code Pattern**:
    ```python
    # Performance test with tolerance
    async def test_proposal_generation_performance(self, proposal_service, intern_agent):
        import time
        start = time.time()
        await proposal_service.create_action_proposal(...)
        duration_ms = (time.time() - start) * 1000
        # Allow 2x threshold for slow CI
        assert duration_ms < 1000, f"Proposal generation took {duration_ms}ms"
    ```
  </action>
  <verify>pytest backend/tests/unit/governance/test_proposal_service.py::TestAuditTrail::test_proposal_history_retrieval backend/tests/unit/governance/test_proposal_service.py::TestPerformance::test_proposal_generation_performance -v --tb=short</verify>
  <done>Audit trail tests verify proposal history retrieval with correct ordering and pagination. Performance tests verify proposal generation completes within acceptable time limits.</done>
</task>

</tasks>

<verification>
**Overall Phase Verification**:

1. Run all proposal service tests: `pytest backend/tests/unit/governance/test_proposal_service.py -v`

2. Verify all 6 tests pass:
   - test_create_action_proposal_for_intern_agent
   - test_submit_proposal_for_approval
   - test_approve_proposal_executes_action
   - test_reject_proposal_blocks_execution
   - test_proposal_history_retrieval
   - test_proposal_generation_performance

3. Run tests 3 times to confirm no flaky mocks: `for i in 1 2 3; do pytest backend/tests/unit/governance/test_proposal_service.py -v --tb=short || exit 1; done`

4. Verify tests actually call ProposalService methods (not just mocks passing trivially)
</verification>

<success_criteria>
1. All 6 proposal service tests pass consistently (zero failures over 3 runs)

2. Tests verify actual behavior:
   - Proposal generation creates database records
   - Approval executes actions via actual tool imports
   - Rejection updates proposal status correctly
   - History retrieval returns correct ordering

3. No flaky logger mocks — tests verify database state and return values

4. Performance test completes within threshold (allow 2x tolerance for CI variability)

5. Mock patches target actual import locations (not module-level patches that don't work)
</success_criteria>

<output>
After completion, create `.planning/phases/29-test-failure-fixes-quality-foundation/29-02-SUMMARY.md` with:
- Tests fixed (6 total)
- Mock pattern changes (from module-level to object-level patches)
- Test results (pass/fail counts)
- Any proposal service bugs discovered during testing
</output>
