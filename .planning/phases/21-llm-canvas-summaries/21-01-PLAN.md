---
phase: 21-llm-canvas-summaries
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/core/llm/canvas_summary_service.py
  - backend/core/llm/__init__.py
autonomous: true

must_haves:
  truths:
    - "LLM-generated summaries captured for all 7 canvas types (generic, docs, email, sheets, orchestration, terminal, coding)"
    - "Summaries are semantically richer than metadata extraction (business context, intent, decision reasoning)"
    - "Fallback to metadata extraction if LLM fails or times out"
    - "Summary generation includes caching to avoid redundant generation"
    - "Cost tracking implemented for per-episode summary generation"
  artifacts:
    - path: "backend/core/llm/canvas_summary_service.py"
      provides: "LLM summary generation service with canvas-specific prompts"
      min_lines: 300
      exports: ["CanvasSummaryService", "generate_summary", "_build_prompt", "_fallback_to_metadata"]
    - path: "backend/core/llm/__init__.py"
      provides: "LLM module exports including CanvasSummaryService"
      exports: ["CanvasSummaryService"]
  key_links:
    - from: "backend/core/llm/canvas_summary_service.py"
      to: "backend/core/llm/byok_handler.py"
      via: "BYOKHandler.generate_response() for LLM summary generation"
      pattern: "self\\.byok_handler\\.generate_response"
    - from: "backend/core/llm/canvas_summary_service.py"
      to: "backend/core/models.py"
      via: "CanvasAudit model for canvas state input"
      pattern: "CanvasAudit"
---

<objective>
Create LLM-powered canvas presentation summary service that generates semantically rich descriptions of canvas content for episodic memory enhancement.

**Purpose:** Replace Phase 20's metadata-based extraction with LLM-generated summaries that capture business context, intent, and decision reasoning for better episode retrieval and agent learning.

**Output:** CanvasSummaryService with canvas-specific prompts for all 7 canvas types, BYOK integration, caching, and fallback to metadata extraction.
</objective>

<execution_context>
@/Users/rushiparikh/.claude/get-shit-done/workflows/execute-plan.md
@/Users/rushiparikh/.claude/get-shit-done/templates/summary.md

@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/21-llm-canvas-summaries/21-RESEARCH.md

@backend/core/llm/byok_handler.py
@backend/core/episode_segmentation_service.py
@backend/core/models.py
</execution_context>

<context>
## Phase 21 Research: LLM-Generated Canvas Presentation Summaries

Phase 21 enhances Phase 20's canvas context for episodic memory by replacing metadata extraction with LLM-generated summaries.

**Current State (Phase 20):**
- `presentation_summary` field populated via metadata extraction
- Example: "Agent presented approval form with revenue chart"
- Deterministic, fast (~5ms), but limited semantic depth

**Proposed Enhancement (Phase 21):**
- LLM-generated summaries with context, intent, and meaning
- Example: "Agent presented $1.2M workflow approval requiring board consent with Q4 revenue trend chart showing 15% growth, highlighting risks and requesting user decision"

## Key Requirements

1. **Canvas-Specific Prompts**: All 7 canvas types have specialized prompts
2. **BYOK Integration**: Use existing BYOK handler for multi-provider LLM
3. **Caching**: Cache summaries by canvas state hash to avoid redundant generation
4. **Fallback**: Metadata extraction if LLM fails or times out
5. **Cost Tracking**: Track LLM costs per summary generation

## Canvas Types

1. **generic**: Generic canvas presentations
2. **docs**: Document canvas (markdown content)
3. **email**: Email composition canvas
4. **sheets**: Spreadsheet/data canvas
5. **orchestration**: Workflow orchestration canvas
6. **terminal**: Terminal/output canvas
7. **coding**: Code editor canvas

## Quality Targets

- Semantic richness >80% (vs 40% for metadata)
- Hallucination rate = 0% (only facts from canvas state)
- Consistency >90% (same state generates same summary)
- Summary length 50-100 words (concise but complete)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create CanvasSummaryService core structure</name>
  <files>backend/core/llm/canvas_summary_service.py</files>
  <action>
Create `backend/core/llm/canvas_summary_service.py` with:

1. **CanvasSummaryService class** with:
   - `__init__(self, byok_handler: BYOKHandler)` - Initialize with BYOK handler
   - `_summary_cache: Dict[str, str]` - Cache for generated summaries (key: canvas_state_hash)
   - `_cost_tracker: Dict[str, float]` - Track LLM costs per session

2. **Core method signature**:
```python
async def generate_summary(
    self,
    canvas_type: str,
    canvas_state: Dict[str, Any],
    agent_task: Optional[str] = None,
    user_interaction: Optional[str] = None,
    timeout_seconds: int = 2
) -> str:
    """
    Generate LLM-powered semantic summary of canvas presentation.

    Args:
        canvas_type: One of 7 canvas types (generic, docs, email, sheets, orchestration, terminal, coding)
        canvas_state: Canvas state from accessibility tree or state API
        agent_task: Optional task description for context
        user_interaction: Optional user action (submit, close, update, execute)
        timeout_seconds: LLM generation timeout (default 2s)

    Returns:
        Semantic summary (50-100 words) or metadata fallback
    """
```

3. **Canvas type validation**:
   - Validate canvas_type is one of 7 supported types
   - Raise ValueError for invalid canvas_type

Do NOT implement the full method yet - this is just the structure.
  </action>
  <verify>
```bash
# Verify file exists and has class structure
grep -q "class CanvasSummaryService" backend/core/llm/canvas_summary_service.py
grep -q "async def generate_summary" backend/core/llm/canvas_summary_service.py
python -c "from core.llm.canvas_summary_service import CanvasSummaryService; print('Import successful')"
```
  </verify>
  <done>
CanvasSummaryService class exists with generate_summary method signature, BYOK handler injection, cache and cost tracker attributes.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement canvas-specific prompts</name>
  <files>backend/core/llm/canvas_summary_service.py</files>
  <action>
Implement `_build_prompt()` method with canvas-specific prompts for all 7 canvas types:

```python
def _build_prompt(
    self,
    canvas_type: str,
    canvas_state: Dict[str, Any],
    agent_task: Optional[str],
    user_interaction: Optional[str]
) -> str:
    """Build LLM prompt with canvas context"""
```

**Prompt template:**
```
You are analyzing a canvas presentation from an AI agent interaction. Generate a concise semantic summary (50-100 words) that captures:

1. **What was presented**: Canvas type, key elements, data shown
2. **Why it mattered**: Business context, decision required, risks highlighted
3. **Critical data**: Key metrics, amounts, deadlines, stakeholders
4. **User decision**: What the user did (if applicable)

**Canvas Type**: {canvas_type}
**Agent Task**: {agent_task or "Not specified"}
**Canvas State**: {json.dumps(canvas_state, indent=2)}
**User Interaction**: {user_interaction or "None"}

**Summary**:
```

**Canvas-specific instructions:**

1. **generic**: "Focus on: Generic canvas content, purpose, key elements. Extract: canvas_title, components, interaction_type, main_purpose."

2. **orchestration**: "Focus on: Workflow details, approval amounts, stakeholders, risks, decision context. Extract: workflow_id, approval_amount, approvers, blockers, deadline."

3. **sheets**: "Focus on: Data values, calculations, trends, notable entries. Extract: revenue, amounts, key_metrics, data_points."

4. **terminal**: "Focus on: Command output, errors, test results, deployment status. Extract: command, exit_code, error_lines, test_counts, blocking_issues."

5. **docs**: "Focus on: Document content, sections, key information. Extract: document_title, sections, word_count, key_topics."

6. **email**: "Focus on: Email composition, recipients, subject, attachments. Extract: to, cc, subject, attachment_count, draft_status."

7. **coding**: "Focus on: Code content, language, syntax elements. Extract: language, line_count, functions, syntax_errors."

Use a dict `_CANVAS_PROMPTS` to store canvas-specific instructions.
  </action>
  <verify>
```bash
# Verify prompts for all 7 canvas types
grep -q "_CANVAS_PROMPTS" backend/core/llm/canvas_summary_service.py
grep -q "orchestration" backend/core/llm/canvas_summary_service.py
grep -q "sheets" backend/core/llm/canvas_summary_service.py
grep -q "terminal" backend/core/llm/canvas_summary_service.py
```
  </verify>
  <done>
All 7 canvas types have specialized prompts in _CANVAS_PROMPTS dict with extraction guidance for critical fields.
  </done>
</task>

<task type="auto">
  <name>Task 3: Implement LLM generation with caching</name>
  <files>backend/core/llm/canvas_summary_service.py</files>
  <action>
Implement the full `generate_summary()` method with:

1. **Cache key generation**:
```python
cache_key = hashlib.sha256(
    json.dumps({
        "canvas_type": canvas_type,
        "canvas_state": canvas_state,
        "agent_task": agent_task,
        "user_interaction": user_interaction
    }, sort_keys=True).encode()
).hexdigest()[:16]
```

2. **Cache check**:
```python
if cache_key in self._summary_cache:
    return self._summary_cache[cache_key]
```

3. **LLM generation with timeout**:
```python
import asyncio

try:
    prompt = self._build_prompt(canvas_type, canvas_state, agent_task, user_interaction)
    summary = await asyncio.wait_for(
        self._byok_handler.generate_response(
            prompt=prompt,
            system_instruction="You are a canvas presentation analyzer. Generate concise semantic summaries (50-100 words).",
            temperature=0.0,  # Deterministic for consistency
            task_type="analysis"
        ),
        timeout=timeout_seconds
    )
    self._summary_cache[cache_key] = summary
    return summary
except asyncio.TimeoutError:
    logger.warning(f"LLM summary generation timed out after {timeout_seconds}s")
    return self._fallback_to_metadata(canvas_type, canvas_state)
except Exception as e:
    logger.error(f"LLM summary generation failed: {e}")
    return self._fallback_to_metadata(canvas_type, canvas_state)
```

4. **Cost tracking** (track tokens/cost from BYOK response):
```python
# Store cost by session key
session_key = f"{canvas_type}_{cache_key}"
# Cost will be tracked via BYOK handler's internal tracking
```
  </action>
  <verify>
```bash
# Verify LLM generation implementation
grep -q "asyncio.wait_for" backend/core/llm/canvas_summary_service.py
grep -q "hashlib.sha256" backend/core/llm/canvas_summary_service.py
grep -q "_summary_cache" backend/core/llm/canvas_summary_service.py
```
  </verify>
  <done>
LLM generation with caching, timeout handling, and cost tracking implemented. Falls back to metadata on failure.
  </done>
</task>

<task type="auto">
  <name>Task 4: Implement metadata fallback</name>
  <files>backend/core/llm/canvas_summary_service.py</files>
  <action>
Implement `_fallback_to_metadata()` method:

```python
def _fallback_to_metadata(
    self,
    canvas_type: str,
    canvas_state: Dict[str, Any]
) -> str:
    """
    Fallback to metadata extraction if LLM fails.

    Replicates Phase 20 behavior for reliability.
    """
    # Extract visual elements
    components = canvas_state.get("components", [])
    visual_elements = ", ".join([c.get("type", "element") for c in components[:3]])

    # Extract critical data
    critical_data = []
    if "workflow_id" in canvas_state:
        critical_data.append(f"workflow {canvas_state['workflow_id']}")
    if "revenue" in canvas_state:
        critical_data.append(f"${canvas_state['revenue']}")
    if "amount" in canvas_state:
        critical_data.append(f"${canvas_state['amount']}")
    if "command" in canvas_state:
        critical_data.append(f"command: {canvas_state['command']}")

    # Build summary
    if visual_elements:
        summary = f"Agent presented {visual_elements} on {canvas_type} canvas"
    else:
        summary = f"Agent presented {canvas_type} canvas"

    if critical_data:
        summary += f" with {', '.join(critical_data)}"

    return summary
```

This method MUST:
- Return summaries similar to Phase 20's _extract_canvas_context
- Extract visual_elements from components array
- Extract critical_data from common fields (workflow_id, revenue, amount, command)
- Return concise string (30-50 words)
  </action>
  <verify>
```bash
# Verify fallback implementation
grep -q "_fallback_to_metadata" backend/core/llm/canvas_summary_service.py
python -c "
from core.llm.canvas_summary_service import CanvasSummaryService
from unittest.mock import Mock
service = CanvasSummaryService(Mock())
result = service._fallback_to_metadata('sheets', {'revenue': '1000000'})
print('Fallback result:', result)
assert 'sheets' in result and '1000000' in result
"
```
  </verify>
  <done>
Metadata fallback extracts visual elements and critical data, returns Phase 20-compatible summaries.
  </done>
</task>

<task type="auto">
  <name>Task 5: Update LLM module exports</name>
  <files>backend/core/llm/__init__.py</files>
  <action>
Update `backend/core/llm/__init__.py` to export CanvasSummaryService:

```python
"""
LLM integration module for Atom AI Platform.

Multi-provider LLM support via BYOK handler with cost optimization,
streaming responses, and canvas summary generation.
"""

from core.llm.byok_handler import (
    BYOKHandler,
    QueryComplexity,
)
from core.llm.canvas_summary_service import CanvasSummaryService

__all__ = [
    "BYOKHandler",
    "QueryComplexity",
    "CanvasSummaryService",
]
```

Ensure:
- CanvasSummaryService is imported
- Added to __all__ exports
- Module docstring references canvas summary generation
  </action>
  <verify>
```bash
# Verify exports
python -c "from core.llm import CanvasSummaryService; print('Export successful')"
python -c "from core.llm import __all__; assert 'CanvasSummaryService' in __all__"
```
  </verify>
  <done>
CanvasSummaryService exported from core.llm module for use in episode segmentation service.
  </done>
</task>

</tasks>

<verification>
## Overall Verification

1. **Service Creation**: CanvasSummaryService exists in backend/core/llm/canvas_summary_service.py
2. **Prompts**: All 7 canvas types have specialized prompts
3. **LLM Integration**: BYOK handler used for generation
4. **Caching**: Summary cache by state hash implemented
5. **Fallback**: Metadata extraction when LLM fails
6. **Cost Tracking**: Token/cost tracking via BYOK
7. **Exports**: Service exported from core.llm module

Run verification:
```bash
cd backend
python -c "
from core.llm.canvas_summary_service import CanvasSummaryService
from core.llm.byok_handler import BYOKHandler
print('CanvasSummaryService imported successfully')
print('Methods:', [m for m in dir(CanvasSummaryService) if not m.startswith('_')])
"
```
</verification>

<success_criteria>
1. CanvasSummaryService class created with ~300 lines
2. generate_summary() method works for all 7 canvas types
3. LLM prompts capture task, state, and interaction context
4. Caching reduces redundant LLM calls (verify with same input)
5. Fallback to metadata if LLM fails/times out
6. Module exports allow importing from core.llm
7. Code follows existing patterns (BYOK handler, logging, error handling)
</success_criteria>

<output>
After completion, create `.planning/phases/21-llm-canvas-summaries/21-01-SUMMARY.md` with:
- Wave execution summary
- Files created/modified
- Verification results
- Any deviations from plan
</output>
