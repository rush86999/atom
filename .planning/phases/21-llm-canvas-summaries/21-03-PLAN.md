---
phase: 21-llm-canvas-summaries
plan: 03
type: execute
wave: 2
depends_on: ["21-01", "21-02"]
files_modified:
  - backend/tests/test_canvas_summary_service.py
  - backend/tests/integration/test_llm_episode_integration.py
autonomous: true

must_haves:
  truths:
    - "Unit tests cover prompt building for all 7 canvas types"
    - "Unit tests verify summary generation and validation"
    - "Integration tests cover episode creation with LLM summaries"
    - "Quality tests verify semantic richness (>80% vs metadata)"
    - "Hallucination detection tests ensure 0% hallucination rate"
    - "Cost tracking and rate limiting tests pass"
    - "Test coverage >60% for canvas_summary_service.py"
  artifacts:
    - path: "backend/tests/test_canvas_summary_service.py"
      provides: "Unit tests for CanvasSummaryService"
      min_lines: 400
      exports: ["TestCanvasSummaryService", "TestPromptBuilding", "TestFallbackBehavior"]
    - path: "backend/tests/integration/test_llm_episode_integration.py"
      provides: "Integration tests for LLM episode creation"
      min_lines: 350
  key_links:
    - from: "backend/tests/test_canvas_summary_service.py"
      to: "backend/core/llm/canvas_summary_service.py"
      via: "Unit tests directly test CanvasSummaryService methods"
      pattern: "from core\\.llm\\.canvas_summary_service import CanvasSummaryService"
    - from: "backend/tests/integration/test_llm_episode_integration.py"
      to: "backend/core/episode_segmentation_service.py"
      via: "Integration tests test episode creation with LLM summaries"
      pattern: "from core\\.episode_segmentation_service import EpisodeSegmentationService"
---

<objective>
Create comprehensive unit and integration tests for LLM canvas summary generation with quality validation metrics.

**Purpose:** Ensure LLM-generated summaries are semantically rich, accurate (no hallucinations), consistent, and cost-effective.

**Output:** Test suites covering prompt building, summary generation, quality metrics, hallucination detection, cost tracking, and integration with episode creation.
</objective>

<execution_context>
@/Users/rushiparikh/.claude/get-shit-done/workflows/execute-plan.md
@/Users/rushiparikh/.claude/get-shit-done/templates/summary.md

@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/21-llm-canvas-summaries/21-RESEARCH.md
@.planning/phases/21-llm-canvas-summaries/21-01-PLAN.md
@.planning/phases/21-llm-canvas-summaries/21-02-PLAN.md

@backend/core/llm/canvas_summary_service.py
@backend/core/episode_segmentation_service.py
@backend/tests/test_canvas_feedback_episode_integration.py
</execution_context>

<context>
## Phase 21 Plan 03: Quality Testing & Validation

This plan creates comprehensive tests to validate the quality of LLM-generated canvas summaries.

## Test Categories

### 1. Unit Tests (test_canvas_summary_service.py)

**Prompt Building Tests**:
- All 7 canvas types have valid prompts
- Prompts include canvas_type, canvas_state, agent_task, user_interaction
- Canvas-specific instructions are included

**Summary Generation Tests**:
- generate_summary() returns valid summaries
- Caching works (same input returns cached result)
- Timeout behavior works correctly
- Error handling and fallback

**Quality Tests**:
- Semantic richness: Summary contains business context
- Conciseness: Summary 50-100 words
- Consistency: Same state produces similar summary

### 2. Integration Tests (test_llm_episode_integration.py)

**Episode Creation Tests**:
- Episodes created with LLM summaries
- All 7 canvas types work end-to-end
- Fallback to metadata on LLM failure

**Quality Validation Tests**:
- Semantic richness >80% (vs 40% metadata)
- Hallucination rate = 0%
- Information recall >90% (key facts present)

### 3. Quality Metrics

**Semantic Richness Score**:
- Business context present?
- Intent explained?
- Decision reasoning included?
- Critical data captured?

**Hallucination Detection**:
- Check summary against canvas state
- Flag any facts not in original state
- Ensure no fabricated information

**Consistency**:
- Same canvas state â†’ same summary (temperature=0)
- Test with 5 runs, compare similarity

## Test Coverage Target

- >60% coverage for canvas_summary_service.py
- All public methods tested
- All error paths tested

## Files to Create

1. `backend/tests/test_canvas_summary_service.py` - Unit tests (~400 lines)
2. `backend/tests/integration/test_llm_episode_integration.py` - Integration tests (~350 lines)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create unit tests for CanvasSummaryService</name>
  <files>backend/tests/test_canvas_summary_service.py</files>
  <action>
Create `backend/tests/test_canvas_summary_service.py` with comprehensive unit tests:

```python
"""
Unit tests for CanvasSummaryService.

Tests prompt building, summary generation, caching, fallback behavior,
and quality metrics for LLM-generated canvas summaries.
"""

import pytest
from unittest.mock import Mock, AsyncMock, patch
import asyncio
import json

class TestCanvasSummaryServiceInitialization:
    """Test CanvasSummaryService initialization"""

    def test_initialization_with_byok_handler(self):
        """Test service initializes with BYOK handler"""
        from core.llm.canvas_summary_service import CanvasSummaryService
        from core.llm.byok_handler import BYOKHandler

        mock_byok = Mock(spec=BYOKHandler)
        service = CanvasSummaryService(byok_handler=mock_byok)

        assert service.byok_handler == mock_byok
        assert service._summary_cache == {}
        assert service._cost_tracker == {}

    def test_initialization_default_byok(self):
        """Test service creates BYOK handler if not provided"""
        from core.llm.canvas_summary_service import CanvasSummaryService

        # Patch BYOKHandler creation
        with patch('core.llm.canvas_summary_service.BYOKHandler') as mock_byok_class:
            mock_instance = Mock()
            mock_byok_class.return_value = mock_instance
            service = CanvasSummaryService()

            assert service.byok_handler == mock_instance


class TestPromptBuilding:
    """Test canvas-specific prompt building"""

    @pytest.fixture
    def service(self):
        from core.llm.canvas_summary_service import CanvasSummaryService
        mock_byok = Mock()
        return CanvasSummaryService(byok_handler=mock_byok)

    @pytest.mark.parametrize("canvas_type", [
        "generic", "docs", "email", "sheets", "orchestration",
        "terminal", "coding"
    ])
    def test_all_canvas_types_have_prompts(self, service, canvas_type):
        """Test that all 7 canvas types have specialized prompts"""
        assert canvas_type in service._CANVAS_PROMPTS

    def test_orchestration_prompt_includes_workflow_details(self, service):
        """Test orchestration prompt extracts workflow-specific fields"""
        prompt = service._build_prompt(
            canvas_type="orchestration",
            canvas_state={
                "workflow_id": "wf-123",
                "approval_amount": 50000,
                "approvers": ["manager"]
            },
            agent_task="Approve workflow",
            user_interaction="submit"
        )

        assert "orchestration" in prompt.lower()
        assert "wf-123" in prompt
        assert "50000" in prompt

    def test_sheets_prompt_includes_data_fields(self, service):
        """Test sheets prompt focuses on data values"""
        prompt = service._build_prompt(
            canvas_type="sheets",
            canvas_state={"revenue": "1000000", "growth": "15%"},
            agent_task="Review Q4 data",
            user_interaction=None
        )

        assert "sheets" in prompt.lower()
        assert "revenue" in prompt.lower()
        assert "1000000" in prompt

    def test_terminal_prompt_includes_command_output(self, service):
        """Test terminal prompt extracts command info"""
        prompt = service._build_prompt(
            canvas_type="terminal",
            canvas_state={"command": "pytest", "exit_code": "0"},
            agent_task="Run tests",
            user_interaction="present"
        )

        assert "terminal" in prompt.lower()
        assert "pytest" in prompt
        assert "exit_code" in prompt.lower()

    def test_prompt_includes_agent_task_when_provided(self, service):
        """Test agent task is included in prompt"""
        prompt = service._build_prompt(
            canvas_type="generic",
            canvas_state={},
            agent_task="Analyze data trends",
            user_interaction=None
        )

        assert "Analyze data trends" in prompt

    def test_prompt_includes_user_interaction_when_provided(self, service):
        """Test user interaction is included in prompt"""
        prompt = service._build_prompt(
            canvas_type="form",
            canvas_state={},
            agent_task=None,
            user_interaction="submit"
        )

        assert "submit" in prompt


class TestSummaryGeneration:
    """Test LLM summary generation"""

    @pytest.fixture
    def service_with_mock_llm(self):
        from core.llm.canvas_summary_service import CanvasSummaryService
        mock_byok = Mock()
        mock_byok.generate_response = AsyncMock(
            return_value="Agent presented Q4 revenue showing $1.2M with 15% growth."
        )
        return CanvasSummaryService(byok_handler=mock_byok)

    @pytest.mark.asyncio
    async def test_generate_summary_returns_llm_result(self, service_with_mock_llm):
        """Test generate_summary returns LLM-generated summary"""
        result = await service_with_mock_llm.generate_summary(
            canvas_type="sheets",
            canvas_state={"revenue": "1200000"},
            agent_task="Review Q4"
        )

        assert "revenue" in result.lower() or "1.2M" in result
        assert service_with_mock_llm.byok_handler.generate_response.called

    @pytest.mark.asyncio
    async def test_generate_summary_uses_cache(self, service_with_mock_llm):
        """Test identical requests use cached result"""
        canvas_state = {"revenue": "1200000"}

        # First call
        result1 = await service_with_mock_llm.generate_summary(
            canvas_type="sheets",
            canvas_state=canvas_state
        )

        # Second call (should use cache)
        result2 = await service_with_mock_llm.generate_summary(
            canvas_type="sheets",
            canvas_state=canvas_state
        )

        # BYOK should only be called once
        assert service_with_mock_llm.byok_handler.generate_response.call_count == 1
        assert result1 == result2

    @pytest.mark.asyncio
    async def test_generate_summary_timeout_falls_back_to_metadata(self):
        """Test timeout triggers metadata fallback"""
        from core.llm.canvas_summary_service import CanvasSummaryService

        mock_byok = Mock()
        mock_byok.generate_response = AsyncMock(
            side_effect=asyncio.TimeoutError("LLM timeout")
        )

        service = CanvasSummaryService(byok_handler=mock_byok)

        result = await service.generate_summary(
            canvas_type="terminal",
            canvas_state={"command": "ls"},
            timeout_seconds=0.1  # Very short timeout
        )

        # Should return metadata fallback
        assert "terminal" in result.lower()
        assert "summary_source" not in result  # Fallback doesn't include source

    @pytest.mark.asyncio
    async def test_generate_summary_error_falls_back_to_metadata(self):
        """Test LLM error triggers metadata fallback"""
        from core.llm.canvas_summary_service import CanvasSummaryService

        mock_byok = Mock()
        mock_byok.generate_response = AsyncMock(
            side_effect=Exception("LLM API error")
        )

        service = CanvasSummaryService(byok_handler=mock_byok)

        result = await service.generate_summary(
            canvas_type="sheets",
            canvas_state={"revenue": "1000000"}
        )

        # Should return metadata fallback
        assert "sheets" in result.lower()


class TestMetadataFallback:
    """Test metadata extraction fallback behavior"""

    @pytest.fixture
    def service(self):
        from core.llm.canvas_summary_service import CanvasSummaryService
        mock_byok = Mock()
        return CanvasSummaryService(byok_handler=mock_byok)

    def test_fallback_extracts_visual_elements(self, service):
        """Test fallback extracts visual elements from components"""
        canvas_state = {
            "components": [
                {"type": "line_chart"},
                {"type": "data_table"}
            ]
        }

        result = service._fallback_to_metadata("sheets", canvas_state)

        assert "line_chart" in result
        assert "data_table" in result

    def test_fallback_extracts_critical_data(self, service):
        """Test fallback extracts critical data points"""
        canvas_state = {
            "workflow_id": "wf-123",
            "revenue": "50000",
            "command": "pytest"
        }

        result = service._fallback_to_metadata("orchestration", canvas_state)

        assert "wf-123" in result or "50000" in result

    def test_fallback_handles_empty_state(self, service):
        """Test fallback handles empty canvas state"""
        result = service._fallback_to_metadata("generic", {})

        assert "generic" in result
        assert len(result) > 0


class TestQualityMetrics:
    """Test summary quality validation"""

    @pytest.fixture
    def service(self):
        from core.llm.canvas_summary_service import CanvasSummaryService
        mock_byok = Mock()
        return CanvasSummaryService(byok_handler=mock_byok)

    def test_semantic_richness_business_context(self, service):
        """Test semantic richness includes business context"""
        # Rich summary (business context, intent, decision)
        rich_summary = "Agent presented $1.2M workflow approval requiring board consent " \
                      "with Q4 revenue chart showing 15% growth, requesting user decision."

        richness = service._calculate_semantic_richness(rich_summary)

        assert richness > 0.7  # High richness

    def test_semantic_richness_poor_summary(self, service):
        """Test semantic richness scores poor summaries low"""
        # Poor summary (just visual description)
        poor_summary = "Agent presented form with chart."

        richness = service._calculate_semantic_richness(poor_summary)

        assert richness < 0.5  # Low richness

    def test_summary_conciseness(self, service):
        """Test summary is within target word count (50-100 words)"""
        from core.llm.canvas_summary_service import CanvasSummaryService

        service = CanvasSummaryService(byok_handler=Mock())

        # Count words in summary
        summary = "Agent presented Q4 revenue chart showing $1.2M with 15% growth " \
                  "compared to Q3, highlighting December spike due to holiday sales."
        word_count = len(summary.split())

        assert 40 <= word_count <= 120  # Target range

    def test_hallucination_detection_none_in_state(self, service):
        """Test hallucination detection catches facts not in canvas state"""
        summary = "Agent presented workflow wf-456 with $100K approval."  # Wrong ID
        canvas_state = {"workflow_id": "wf-123", "approval_amount": "50000"}

        has_hallucination = service._detect_hallucination(summary, canvas_state)

        assert has_hallucination  # wf-456 and $100K not in state

    def test_hallucination_detection_all_in_state(self, service):
        """Test no hallucination when all facts in state"""
        summary = "Agent presented workflow wf-123 with $50K approval."
        canvas_state = {"workflow_id": "wf-123", "approval_amount": "50000"}

        has_hallucination = service._detect_hallucination(summary, canvas_state)

        assert not has_hallucination  # All facts in state


class TestCostTracking:
    """Test cost tracking for LLM generation"""

    @pytest.fixture
    def service(self):
        from core.llm.canvas_summary_service import CanvasSummaryService
        mock_byok = Mock()
        return CanvasSummaryService(byok_handler=mock_byok)

    def test_cost_tracking_initialization(self, service):
        """Test cost tracker initialized"""
        assert service._cost_tracker == {}

    def test_cost_accumulation(self, service):
        """Test costs accumulate for session"""
        service._cost_tracker["session_1"] = 0.001
        service._cost_tracker["session_1"] += 0.002

        assert service._cost_tracker["session_1"] == 0.003
```
  </action>
  <verify>
```bash
# Run unit tests
cd backend
pytest tests/test_canvas_summary_service.py -v --tb=short
# Verify coverage
pytest tests/test_canvas_summary_service.py --cov=core.llm.canvas_summary_service --cov-report=term-missing
```
  </verify>
  <done>
Unit tests created and passing with >60% coverage for canvas_summary_service.py.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create integration tests for LLM episode creation</name>
  <files>backend/tests/integration/test_llm_episode_integration.py</files>
  <action>
Create `backend/tests/integration/test_llm_episode_integration.py` with integration tests:

```python
"""
Integration tests for LLM canvas summary generation in episodes.

Tests end-to-end episode creation with LLM-generated canvas context,
quality validation, and fallback behavior.
"""

import pytest
from unittest.mock import Mock, AsyncMock, patch
from datetime import datetime, timedelta

class TestLLMEpisodeIntegration:
    """Test LLM canvas summary integration with episode creation"""

    @pytest.fixture
    def db_session(self):
        """Mock database session"""
        from sqlalchemy.orm import Session
        db = Mock(spec=Session)
        db.query.return_value.filter.return_value.first.return_value = None
        db.query.return_value.filter.return_value.order_by.return_value.all.return_value = []
        db.add = Mock()
        db.commit = Mock()
        db.refresh = Mock()
        return db

    @pytest.fixture
    def mock_byok_handler(self):
        """Mock BYOK handler for testing"""
        handler = Mock()
        handler.generate_response = AsyncMock(
            return_value=(
                "Agent presented Q4 revenue chart showing $1.2M in sales "
                "with 15% growth from Q3, highlighting December spike "
                "and requesting budget approval for Q1."
            )
        )
        return handler

    @pytest.fixture
    def segmentation_service(self, db_session, mock_byok_handler):
        """Create EpisodeSegmentationService with mocked dependencies"""
        from core.episode_segmentation_service import EpisodeSegmentationService

        # Mock lancedb handler
        with patch('core.episode_segmentation_service.get_lancedb_handler') as mock_lancedb:
            mock_lancedb.return_value = None
            service = EpisodeSegmentationService(db_session, byok_handler=mock_byok_handler)
            return service

    @pytest.mark.asyncio
    async def test_episode_creation_with_llm_summary(
        self, segmentation_service, mock_byok_handler, db_session
    ):
        """Test episode creation uses LLM-generated canvas summary"""
        from core.models import CanvasAudit, ChatSession, ChatMessage, AgentRegistry

        # Create mock session data
        mock_session = Mock(spec=ChatSession)
        mock_session.id = "session-1"
        mock_session.user_id = "user-1"
        mock_session.workspace_id = "default"
        mock_session.created_at = datetime.utcnow()

        # Create mock canvas audit
        with patch.object(
            segmentation_service,
            '_fetch_canvas_context',
            return_value=[
                Mock(
                    spec=CanvasAudit,
                    id="canvas-1",
                    canvas_type="sheets",
                    action="present",
                    audit_metadata={
                        "revenue": "1200000",
                        "growth": "15",
                        "components": [{"type": "line_chart"}]
                    }
                )
            ]
        ):
            # Extract context with LLM
            result = await segmentation_service._extract_canvas_context_llm(
                canvas_audit=segmentation_service._fetch_canvas_context()[0],
                agent_task="Review Q4 revenue"
            )

        # Verify LLM summary
        assert result["canvas_type"] == "sheets"
        assert result["summary_source"] == "llm"
        assert "revenue" in result.get("critical_data_points", {})

    @pytest.mark.asyncio
    async def test_llm_fallback_on_error(
        self, segmentation_service, db_session
    ):
        """Test episode creation falls back to metadata on LLM error"""
        from core.models import CanvasAudit
        import asyncio

        # Mock BYOK handler that fails
        failing_handler = Mock()
        failing_handler.generate_response = AsyncMock(
            side_effect=asyncio.TimeoutError("LLM timeout")
        )

        # Create service with failing BYOK
        with patch('core.episode_segmentation_service.get_lancedb_handler'):
            from core.episode_segmentation_service import EpisodeSegmentationService
            service = EpisodeSegmentationService(db_session, byok_handler=failing_handler)

        # Create mock canvas audit
        mock_canvas = Mock(
            spec=CanvasAudit,
            id="canvas-2",
            canvas_type="terminal",
            action="present",
            audit_metadata={"command": "pytest", "exit_code": "0"}
        )

        # Should fallback to metadata
        result = await service._extract_canvas_context_llm(
            canvas_audit=mock_canvas
        )

        assert result["summary_source"] == "metadata"
        assert "terminal" in result["canvas_type"]

    @pytest.mark.asyncio
    async def test_all_7_canvas_types_generate_summaries(
        self, segmentation_service, mock_byok_handler
    ):
        """Test that all 7 canvas types generate valid summaries"""
        from core.models import CanvasAudit

        canvas_types = [
            ("generic", {"content": "test"}),
            ("docs", {"word_count": 500, "title": "Spec"}),
            ("email", {"to": "user@example.com", "subject": "Test"}),
            ("sheets", {"revenue": "1000000"}),
            ("orchestration", {"workflow_id": "wf-123"}),
            ("terminal", {"command": "ls", "exit_code": "0"}),
            ("coding", {"language": "python", "line_count": 50})
        ]

        for canvas_type, metadata in canvas_types:
            mock_canvas = Mock(
                spec=CanvasAudit,
                id=f"canvas-{canvas_type}",
                canvas_type=canvas_type,
                action="present",
                audit_metadata=metadata
            )

            result = await segmentation_service._extract_canvas_context_llm(
                canvas_audit=mock_canvas
            )

            assert result["canvas_type"] == canvas_type
            assert "presentation_summary" in result
            assert len(result["presentation_summary"]) > 20

    @pytest.mark.asyncio
    async def test_summary_quality_validation(
        self, segmentation_service, mock_byok_handler
    ):
        """Test LLM summaries meet quality thresholds"""
        from core.models import CanvasAudit

        mock_canvas = Mock(
            spec=CanvasAudit,
            id="canvas-quality",
            canvas_type="orchestration",
            action="present",
            audit_metadata={
                "workflow_id": "wf-budget",
                "approval_amount": 100000,
                "approvers": ["manager", "director"]
            }
        )

        result = await segmentation_service._extract_canvas_context_llm(
            canvas_audit=mock_canvas,
            agent_task="Approve budget"
        )

        summary = result["presentation_summary"]

        # Quality checks:
        # 1. Business context (amount, workflow)
        assert "100000" in summary or "100" in summary or "wf-budget" in summary or "budget" in summary.lower()

        # 2. Conciseness (50-100 words)
        word_count = len(summary.split())
        assert 30 <= word_count <= 150  # Allow some flexibility

        # 3. No empty summary
        assert len(summary.strip()) > 0


class TestSemanticRichnessMetrics:
    """Test semantic richness quality metrics"""

    def test_semantic_richness_scoring(self):
        """Test semantic richness scoring algorithm"""
        from core.llm.canvas_summary_service import CanvasSummaryService

        service = CanvasSummaryService(byok_handler=Mock())

        # Rich summary (business context + intent + decision)
        rich = (
            "Agent presented $1.2M workflow approval requiring board consent "
            "due to budget exceeding $1M threshold, with Q4 revenue chart "
            "showing 15% growth, highlighting 3 pending stakeholder responses "
            "and requesting immediate user decision."
        )

        # Poor summary (minimal information)
        poor = "Agent presented form with chart."

        # Score richness (check for business terms, numbers, context)
        rich_score = service._calculate_semantic_richness(rich)
        poor_score = service._calculate_semantic_richness(poor)

        assert rich_score > 0.7  # High threshold for rich
        assert poor_score < 0.5  # Low threshold for poor


class TestHallucinationDetection:
    """Test hallucination detection in LLM summaries"""

    def test_detect_hallucination_fabricated_facts(self):
        """Test hallucination detection catches fabricated facts"""
        from core.llm.canvas_summary_service import CanvasSummaryService

        service = CanvasSummaryService(byok_handler=Mock())

        summary = "Agent presented workflow wf-999 with $1M approval."  # Wrong ID
        canvas_state = {"workflow_id": "wf-123", "approval_amount": "50000"}

        has_hallucination = service._detect_hallucination(summary, canvas_state)

        # Should detect wf-999 not in state
        assert has_hallucination

    def test_no_hallucination_accurate_summary(self):
        """Test no hallucination when summary matches state"""
        from core.llm.canvas_summary_service import CanvasSummaryService

        service = CanvasSummaryService(byok_handler=Mock())

        summary = "Agent presented workflow wf-123 with $50K approval."
        canvas_state = {"workflow_id": "wf-123", "approval_amount": "50000"}

        has_hallucination = service._detect_hallucination(summary, canvas_state)

        # All facts in state
        assert not has_hallucination


class TestConsistencyValidation:
    """Test summary consistency across multiple runs"""

    @pytest.mark.asyncio
    async def test_consistency_same_state_same_summary(self):
        """Test same canvas state generates consistent summary"""
        from core.llm.canvas_summary_service import CanvasSummaryService
        from core.models import CanvasAudit

        mock_byok = Mock()
        mock_byok.generate_response = AsyncMock(
            return_value="Agent presented workflow wf-123 with $50K approval."
        )

        service = CanvasSummaryService(byok_handler=mock_byok)

        mock_canvas = Mock(
            spec=CanvasAudit,
            canvas_type="orchestration",
            audit_metadata={"workflow_id": "wf-123", "approval_amount": "50000"}
        )

        # Generate summary 5 times
        summaries = []
        for _ in range(5):
            result = await service.generate_summary(
                canvas_type="orchestration",
                canvas_state=mock_canvas.audit_metadata
            )
            summaries.append(result)

        # All should be identical (temperature=0, cached)
        assert len(set(summaries)) == 1
```
  </action>
  <verify>
```bash
# Run integration tests
cd backend
pytest tests/integration/test_llm_episode_integration.py -v --tb=short
# Verify tests pass
```
  </verify>
  <done>
Integration tests created and passing, covering episode creation, all 7 canvas types, quality metrics, hallucination detection, and consistency validation.
  </done>
</task>

<task type="auto">
  <name>Task 3: Add quality metrics methods to CanvasSummaryService</name>
  <files>backend/core/llm/canvas_summary_service.py</files>
  <action>
Add quality validation helper methods to CanvasSummaryService:

```python
def _calculate_semantic_richness(self, summary: str) -> float:
    """
    Calculate semantic richness score (0.0 to 1.0).

    Higher score indicates more business context, intent, and
    decision reasoning present in the summary.

    Args:
        summary: LLM-generated summary

    Returns:
        Richness score (0.0 = poor, 1.0 = excellent)
    """
    if not summary:
        return 0.0

    richness_indicators = [
        # Business context terms
        "approval", "budget", "revenue", "workflow", "stakeholder",
        "decision", "consent", "deadline", "priority",
        # Metrics and numbers
        "%", "growth", "increase", "decrease", "trend",
        "$", "k", "m", "b",
        # Intent and reasoning
        "requiring", "requesting", "highlighting", "showing",
        "due to", "because", "for", "with"
    ]

    summary_lower = summary.lower()
    matches = sum(1 for indicator in richness_indicators if indicator.lower() in summary_lower)

    # Normalize to 0-1 range (assuming ~10 indicators is excellent)
    return min(1.0, matches / 10.0)

def _detect_hallucination(
    self,
    summary: str,
    canvas_state: Dict[str, Any]
) -> bool:
    """
    Detect hallucinations in LLM summary.

    Checks if summary contains facts not present in canvas state.

    Args:
        summary: LLM-generated summary
        canvas_state: Original canvas state

    Returns:
        True if hallucination detected (facts not in state)
    """
    # Extract numbers and IDs from summary
    import re

    # Look for workflow IDs, amounts, etc. in summary
    summary_facts = set()

    # Workflow IDs (e.g., wf-123)
    summary_facts.update(re.findall(r'wf-\d+', summary.lower()))

    # Monetary amounts (e.g., $1.2M, $50K)
    money_patterns = [
        r'\$\d+\.?\d*[mbk]',  # $1.2M, $50K
        r'\$\d+',             # $1000
        r'\d+\s*(dollars|usd)' # 100 dollars
    ]
    for pattern in money_patterns:
        summary_facts.update(re.findall(pattern, summary.lower()))

    # Check if summary facts exist in canvas state
    state_str = json.dumps(canvas_state, default=str).lower()

    for fact in summary_facts:
        # Normalize fact for comparison
        fact_normalized = fact.lower().replace('$', '').replace(',', '')
        if fact_normalized not in state_str:
            # Fact not found in state - potential hallucination
            return True

    return False
```

These methods should be private (_ prefix) and used by tests for quality validation.
  </action>
  <verify>
```bash
# Verify methods exist
grep -q "_calculate_semantic_richness" backend/core/llm/canvas_summary_service.py
grep -q "_detect_hallucination" backend/core/llm/canvas_summary_service.py
python -c "
from core.llm.canvas_summary_service import CanvasSummaryService
service = CanvasSummaryService(byok_handler=Mock())
# Test richness
richness = service._calculate_semantic_richness('Agent presented $1M approval requiring board consent')
print(f'Richness: {richness}')
assert richness > 0.5
# Test hallucination
has_hall = service._detect_hallucination('workflow wf-999', {'workflow_id': 'wf-123'})
print(f'Hallucination detected: {has_hall}')
assert has_hall == True
"
```
  </verify>
  <done>
Quality metrics methods added to CanvasSummaryService with semantic richness scoring and hallucination detection.
  </done>
</task>

</tasks>

<verification>
## Overall Verification

1. **Unit Tests**: test_canvas_summary_service.py created with all test classes passing
2. **Integration Tests**: test_llm_episode_integration.py created with all tests passing
3. **Quality Metrics**: Semantic richness, hallucination detection, consistency tests passing
4. **Coverage**: >60% coverage achieved for canvas_summary_service.py
5. **All Canvas Types**: All 7 canvas types tested
6. **Fallback Behavior**: Timeout and error fallback tested

Run verification:
```bash
cd backend
# Run all LLM summary tests
pytest tests/test_canvas_summary_service.py tests/integration/test_llm_episode_integration.py -v
# Check coverage
pytest tests/test_canvas_summary_service.py --cov=core.llm.canvas_summary_service --cov-report=term-missing | grep TOTAL
```
</verification>

<success_criteria>
1. Unit tests cover prompt building, generation, caching, fallback
2. Integration tests cover episode creation with LLM summaries
3. Quality tests verify semantic richness >80% target
4. Hallucination detection tests ensure 0% fabricated facts
5. Consistency tests verify same state produces same summary
6. Test coverage >60% for canvas_summary_service.py
7. All tests passing with zero errors
</success_criteria>

<output>
After completion, create `.planning/phases/21-llm-canvas-summaries/21-03-SUMMARY.md` with:
- Test results summary
- Coverage metrics achieved
- Quality validation results (richness, hallucination, consistency)
- Any deviations from plan
</output>
