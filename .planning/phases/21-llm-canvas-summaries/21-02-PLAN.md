---
phase: 21-llm-canvas-summaries
plan: 02
type: execute
wave: 2
depends_on: ["21-01"]
files_modified:
  - backend/core/episode_segmentation_service.py
  - backend/tests/test_canvas_summary_integration.py
autonomous: true

must_haves:
  truths:
    - "Episode creation uses LLM-generated summaries instead of metadata extraction"
    - "Summary generation is async with 2-second timeout"
    - "Fallback to metadata if LLM times out or fails"
    - "Episode creation latency remains <3s with LLM generation"
    - "All existing episode creation tests still pass"
    - "Canvas context in EpisodeSegment includes LLM-generated presentation_summary"
  artifacts:
    - path: "backend/core/episode_segmentation_service.py"
      provides: "Episode segmentation with LLM canvas summary integration"
      contains: "_extract_canvas_context_llm", "CanvasSummaryService integration"
      min_lines: 1400
    - path: "backend/tests/test_canvas_summary_integration.py"
      provides: "Integration tests for LLM canvas summary generation"
      min_lines: 200
  key_links:
    - from: "backend/core/episode_segmentation_service.py"
      to: "backend/core/llm/canvas_summary_service.py"
      via: "Import and instantiate CanvasSummaryService"
      pattern: "from core\\.llm\\.canvas_summary_service import CanvasSummaryService"
    - from: "backend/core/episode_segmentation_service.py"
      to: "backend/core/llm/byok_handler.py"
      via: "BYOKHandler for LLM generation (injected via CanvasSummaryService)"
      pattern: "BYOKHandler"
    - from: "backend/tests/test_canvas_summary_integration.py"
      to: "backend/core/llm/canvas_summary_service.py"
      via: "Test imports and validates CanvasSummaryService"
      pattern: "from core\\.llm\\.canvas_summary_service import"
---

<objective>
Integrate LLM-generated canvas summaries into episode segmentation process, replacing Phase 20's metadata extraction with richer semantic summaries.

**Purpose:** Enable episodic memory to capture LLM-generated canvas context for better semantic search and agent learning.

**Output:** Updated episode_segmentation_service.py with LLM summary integration, async generation with timeout, and fallback behavior.
</objective>

<execution_context>
@/Users/rushiparikh/.claude/get-shit-done/workflows/execute-plan.md
@/Users/rushiparikh/.claude/get-shit-done/templates/summary.md

@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/21-llm-canvas-summaries/21-RESEARCH.md
@.planning/phases/21-llm-canvas-summaries/21-01-PLAN.md

@backend/core/episode_segmentation_service.py
@backend/core/llm/canvas_summary_service.py
@backend/core/llm/byok_handler.py
@backend/tests/test_canvas_feedback_episode_integration.py
</execution_context>

<context>
## Phase 21 Plan 02: Episode Segmentation Integration

This plan integrates the CanvasSummaryService (created in Plan 01) into the episode segmentation process.

**Current State (Phase 20):**
- `_extract_canvas_context()` uses metadata extraction
- Returns deterministic but shallow summaries

**Target State (Phase 21):**
- New `_extract_canvas_context_llm()` uses CanvasSummaryService
- Async generation with 2-second timeout
- Fallback to metadata if LLM fails
- Episode creation latency <3s

## Integration Points

1. **EpisodeSegmentationService.__init__**: Add CanvasSummaryService injection
2. **_extract_canvas_context_llm()**: New method for LLM generation
3. **_extract_canvas_context()**: Update to call LLM version first
4. **create_episode_from_session()**: Use LLM summaries for episodes

## Key Considerations

- **Async generation**: Episode creation is already async, so LLM generation fits naturally
- **Timeout**: 2-second timeout to prevent blocking episode creation
- **Fallback**: Metadata extraction ensures reliability
- **Backward compatibility**: Existing tests must pass

## Files to Modify

1. `backend/core/episode_segmentation_service.py`:
   - Import CanvasSummaryService
   - Add to __init__
   - Create _extract_canvas_context_llm method
   - Update _extract_canvas_context to use LLM

2. `backend/tests/test_canvas_summary_integration.py`:
   - New test file for LLM integration
   - Test LLM generation for each canvas type
   - Test fallback behavior
   - Test episode creation with LLM summaries
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add CanvasSummaryService to EpisodeSegmentationService</name>
  <files>backend/core/episode_segmentation_service.py</files>
  <action>
Update `EpisodeSegmentationService.__init__` to include CanvasSummaryService:

1. **Add import at top of file**:
```python
from core.llm.canvas_summary_service import CanvasSummaryService
from core.llm.byok_handler import BYOKHandler
```

2. **Update __init__ method**:
```python
def __init__(self, db: Session, byok_handler: Optional[BYOKHandler] = None):
    self.db = db
    self.lancedb = get_lancedb_handler()
    self.detector = EpisodeBoundaryDetector(self.lancedb)

    # Initialize BYOK handler for LLM summaries
    if byok_handler is None:
        byok_handler = BYOKHandler(workspace_id="default")
    self.byok_handler = byok_handler

    # Initialize CanvasSummaryService
    self.canvas_summary_service = CanvasSummaryService(
        byok_handler=self.byok_handler
    )
```

3. **Make sure all existing __init__ calls work**:
   - If there are existing calls to EpisodeSegmentationService(db), they should still work
   - The byok_handler parameter is optional with default None

Do NOT modify other methods yet - this is just initialization setup.
  </action>
  <verify>
```bash
# Verify import and initialization
grep -q "from core.llm.canvas_summary_service import CanvasSummaryService" backend/core/episode_segmentation_service.py
grep -q "self.canvas_summary_service = CanvasSummaryService" backend/core/episode_segmentation_service.py
python -c "
from unittest.mock import Mock
from core.episode_segmentation_service import EpisodeSegmentationService
db = Mock()
service = EpisodeSegmentationService(db)
assert hasattr(service, 'canvas_summary_service')
print('CanvasSummaryService initialized successfully')
"
```
  </verify>
  <done>
CanvasSummaryService imported and initialized in EpisodeSegmentationService with optional BYOK handler injection.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create _extract_canvas_context_llm method</name>
  <files>backend/core/episode_segmentation_service.py</files>
  <action>
Create new method `_extract_canvas_context_llm()` in EpisodeSegmentationService:

```python
async def _extract_canvas_context_llm(
    self,
    canvas_audit: CanvasAudit,
    agent_task: Optional[str] = None
) -> Dict[str, Any]:
    """
    Extract canvas context with LLM-generated semantic summary.

    Uses CanvasSummaryService to generate rich semantic summaries
    that capture business context, intent, and decision reasoning.

    Args:
        canvas_audit: CanvasAudit record with canvas metadata
        agent_task: Optional agent task description for context

    Returns:
        Canvas context dict with LLM-generated presentation_summary:
        {
            "canvas_type": str,
            "presentation_summary": str,  # LLM-generated
            "visual_elements": List[str],
            "user_interaction": str,
            "critical_data_points": dict,
            "summary_source": "llm" or "metadata"  # Track source
        }
    """
    try:
        # Build canvas state from audit metadata
        canvas_state = canvas_audit.audit_metadata or {}

        # Generate LLM summary with 2-second timeout
        presentation_summary = await self.canvas_summary_service.generate_summary(
            canvas_type=canvas_audit.canvas_type or "generic",
            canvas_state=canvas_state,
            agent_task=agent_task,
            user_interaction=canvas_audit.action,
            timeout_seconds=2
        )

        # Extract visual elements from canvas state
        visual_elements = []
        if "components" in canvas_state:
            visual_elements = [
                c.get("type", "element")
                for c in canvas_state["components"][:5]
            ]

        # Extract user interaction
        interaction_map = {
            "present": "presented to user",
            "submit": "user submitted",
            "close": "user closed",
            "update": "user updated",
            "execute": "user executed"
        }
        user_interaction = interaction_map.get(
            canvas_audit.action,
            f"user action: {canvas_audit.action}"
        )

        # Extract critical data points
        critical_data = {}
        critical_fields = [
            "workflow_id", "approval_status", "revenue", "amount",
            "priority", "command", "exit_code", "file_path", "language"
        ]
        for field in critical_fields:
            if field in canvas_state:
                critical_data[field] = canvas_state[field]

        return {
            "canvas_type": canvas_audit.canvas_type or "generic",
            "presentation_summary": presentation_summary,
            "visual_elements": visual_elements,
            "user_interaction": user_interaction,
            "critical_data_points": critical_data,
            "summary_source": "llm"
        }

    except Exception as e:
        logger.error(f"LLM canvas context extraction failed: {e}")
        # Fallback to metadata extraction
        return self._extract_canvas_context_metadata(canvas_audit, agent_task)
```

Add helper method `_extract_canvas_context_metadata()` for fallback:
```python
def _extract_canvas_context_metadata(
    self,
    canvas_audit: CanvasAudit,
    agent_task: Optional[str] = None
) -> Dict[str, Any]:
    """
    Fallback to metadata extraction (Phase 20 behavior).
    """
    # Use existing _extract_canvas_context logic
    # ... (copy existing implementation or call existing method)
    result = self._extract_canvas_context([canvas_audit])
    result["summary_source"] = "metadata"
    return result
```
  </action>
  <verify>
```bash
# Verify method exists
grep -q "_extract_canvas_context_llm" backend/core/episode_segmentation_service.py
grep -q "async def _extract_canvas_context_llm" backend/core/episode_segmentation_service.py
grep -q "summary_source" backend/core/episode_segmentation_service.py
```
  </verify>
  <done>
_extract_canvas_context_llm method created with async LLM generation, timeout handling, and fallback to metadata.
  </done>
</task>

<task type="auto">
  <name>Task 3: Update create_episode_from_session to use LLM summaries</name>
  <files>backend/core/episode_segmentation_service.py</files>
  <action>
Update `create_episode_from_session()` to use LLM canvas context:

1. **Find the existing canvas context extraction** (around line 194):
```python
# Current code (Phase 20):
canvas_context = self._extract_canvas_context(canvas_audits)
```

2. **Replace with LLM version**:
```python
# New code (Phase 21):
# Extract canvas context with LLM-generated summaries
canvas_context = await self._extract_canvas_context_llm(
    canvas_audit=canvas_audits[0] if canvas_audits else None,
    agent_task=next((messages[0].content for messages in [messages] if messages), None)
) if canvas_audits else {}
```

3. **Handle multiple canvases** (if more than one canvas audit):
```python
if canvas_audits:
    # Generate LLM summary for most recent canvas
    canvas_context = await self._extract_canvas_context_llm(
        canvas_audit=canvas_audits[0],
        agent_task=next((messages[0].content for messages in [messages] if messages), None)
    )
else:
    canvas_context = {}
```

4. **Ensure the method is async** (it should already be):
```python
async def create_episode_from_session(
    self,
    session_id: str,
    agent_id: str,
    title: Optional[str] = None,
    force_create: bool = False
) -> Optional[Episode]:
```

The key changes:
- Call `await self._extract_canvas_context_llm()` instead of `self._extract_canvas_context()`
- Pass agent_task for better context
- Handle empty canvas_audits gracefully
  </action>
  <verify>
```bash
# Verify async call to LLM method
grep -q "await self._extract_canvas_context_llm" backend/core/episode_segmentation_service.py
grep -q "canvas_context = await" backend/core/episode_segmentation_service.py
```
  </verify>
  <done>
create_episode_from_session uses LLM-generated canvas context with async/await pattern.
  </done>
</task>

<task type="auto">
  <name>Task 4: Create integration tests for LLM canvas summaries</name>
  <files>backend/tests/test_canvas_summary_integration.py</files>
  <action>
Create `backend/tests/test_canvas_summary_integration.py` with integration tests:

```python
"""
Integration tests for LLM canvas summary generation in episodes.

Tests the integration between CanvasSummaryService and
EpisodeSegmentationService for LLM-generated canvas context.
"""

import pytest
from unittest.mock import Mock, AsyncMock, patch
from datetime import datetime

class TestCanvasSummaryLLMIntegration:
    """Test LLM canvas summary integration with episode creation"""

    @pytest.fixture
    def db_session(self):
        """Mock database session"""
        db = Mock()
        db.query.return_value.filter.return_value.first.return_value = None
        db.add = Mock()
        db.commit = Mock()
        return db

    @pytest.fixture
    def mock_byok_handler(self):
        """Mock BYOK handler for LLM generation"""
        handler = Mock()
        handler.generate_response = AsyncMock(return_value=(
            "Agent presented Q4 revenue chart showing $1.2M in sales, "
            "15% growth from Q3, highlighting December spike and "
            "requesting approval for Q1 budget."
        ))
        return handler

    def test_canvas_summary_service_initialization(self, db_session, mock_byok_handler):
        """Test CanvasSummaryService is initialized in EpisodeSegmentationService"""
        from core.episode_segmentation_service import EpisodeSegmentationService
        from core.llm.byok_handler import BYOKHandler

        # Create service with BYOK handler
        service = EpisodeSegmentationService(db_session, byok_handler=mock_byok_handler)

        assert hasattr(service, 'canvas_summary_service')
        assert hasattr(service, 'byok_handler')

    @pytest.mark.asyncio
    async def test_llm_canvas_context_extraction(self, db_session, mock_byok_handler):
        """Test LLM-generated canvas context extraction"""
        from core.episode_segmentation_service import EpisodeSegmentationService
        from core.models import CanvasAudit

        service = EpisodeSegmentationService(db_session, byok_handler=mock_byok_handler)

        # Create mock canvas audit
        canvas_audit = CanvasAudit(
            id="test-canvas-1",
            canvas_type="sheets",
            action="present",
            audit_metadata={
                "revenue": "1200000",
                "growth": "15%",
                "components": [{"type": "line_chart"}]
            }
        )

        # Extract context with LLM
        result = await service._extract_canvas_context_llm(
            canvas_audit=canvas_audit,
            agent_task="Review Q4 revenue"
        )

        # Verify result structure
        assert result["canvas_type"] == "sheets"
        assert "presentation_summary" in result
        assert result["summary_source"] == "llm"
        assert "visual_elements" in result
        assert "critical_data_points" in result

    @pytest.mark.asyncio
    async def test_llm_fallback_to_metadata_on_timeout(self, db_session):
        """Test fallback to metadata when LLM times out"""
        from core.episode_segmentation_service import EpisodeSegmentationService
        from core.models import CanvasAudit
        import asyncio

        # Mock BYOK handler that times out
        mock_byok_handler = Mock()
        mock_byok_handler.generate_response = AsyncMock(
            side_effect=asyncio.TimeoutError("LLM timeout")
        )

        service = EpisodeSegmentationService(db_session, byok_handler=mock_byok_handler)

        canvas_audit = CanvasAudit(
            id="test-canvas-2",
            canvas_type="terminal",
            action="present",
            audit_metadata={"command": "pytest tests/", "exit_code": "0"}
        )

        # Should fallback to metadata
        result = await service._extract_canvas_context_llm(canvas_audit=canvas_audit)

        assert result["summary_source"] == "metadata"
        assert "terminal" in result["canvas_type"]

    def test_all_7_canvas_types_have_prompts(self, db_session, mock_byok_handler):
        """Test that all 7 canvas types have specialized prompts"""
        from core.episode_segmentation_service import EpisodeSegmentationService

        service = EpisodeSegmentationService(db_session, byok_handler=mock_byok_handler)

        canvas_types = ["generic", "docs", "email", "sheets", "orchestration", "terminal", "coding"]

        for canvas_type in canvas_types:
            # Verify prompt exists in CanvasSummaryService
            prompts = service.canvas_summary_service._CANVAS_PROMPTS
            assert canvas_type in prompts, f"Missing prompt for {canvas_type}"

    @pytest.mark.asyncio
    async def test_canvas_context_includes_critical_data(self, db_session, mock_byok_handler):
        """Test that critical data points are extracted from canvas state"""
        from core.episode_segmentation_service import EpisodeSegmentationService
        from core.models import CanvasAudit

        service = EpisodeSegmentationService(db_session, byok_handler=mock_byok_handler)

        canvas_audit = CanvasAudit(
            id="test-canvas-3",
            canvas_type="orchestration",
            action="present",
            audit_metadata={
                "workflow_id": "wf-123",
                "approval_amount": 50000,
                "approvers": ["manager", "director"]
            }
        )

        result = await service._extract_canvas_context_llm(canvas_audit=canvas_audit)

        # Verify critical data extracted
        critical_data = result["critical_data_points"]
        assert "workflow_id" in critical_data or "approval_amount" in critical_data
```

Run tests:
```bash
cd backend
pytest tests/test_canvas_summary_integration.py -v
```
  </action>
  <verify>
```bash
# Run tests
cd backend
pytest tests/test_canvas_summary_integration.py -v --tb=short
# Verify all tests pass
```
  </verify>
  <done>
Integration tests created and passing, covering LLM generation, fallback behavior, all 7 canvas types, and critical data extraction.
  </done>
</task>

</tasks>

<verification>
## Overall Verification

1. **Service Integration**: CanvasSummaryService initialized in EpisodeSegmentationService
2. **LLM Method**: _extract_canvas_context_llm() created and async
3. **Episode Creation**: create_episode_from_session() uses LLM summaries
4. **Fallback Behavior**: Metadata extraction used when LLM fails/times out
5. **Test Coverage**: Integration tests cover all canvas types and fallback scenarios
6. **Backward Compatibility**: Existing tests still pass

Run verification:
```bash
cd backend
# Verify imports work
python -c "
from core.episode_segmentation_service import EpisodeSegmentationService
print('EpisodeSegmentationService imports successfully')
"
# Run integration tests
pytest tests/test_canvas_summary_integration.py -v
# Run existing episode tests (sample)
pytest tests/test_episode_segmentation.py::TestEpisodeSegmentationService::test_create_episode_from_session -v
```
</verification>

<success_criteria>
1. CanvasSummaryService initialized in EpisodeSegmentationService.__init__
2. _extract_canvas_context_llm() method generates LLM summaries
3. create_episode_from_session() calls LLM method with await
4. Fallback to metadata when LLM fails or times out
5. Integration tests pass (all 7 canvas types, timeout, fallback)
6. Episode creation latency <3s with LLM generation
7. Existing episode tests remain passing
</success_criteria>

<output>
After completion, create `.planning/phases/21-llm-canvas-summaries/21-02-SUMMARY.md` with:
- Wave execution summary
- Integration changes made
- Test results
- Performance metrics (episode creation latency)
- Any deviations from plan
</output>
