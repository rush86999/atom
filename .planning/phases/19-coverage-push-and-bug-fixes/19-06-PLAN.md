---
phase: 19-coverage-push-and-bug-fixes
plan: 06
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/tests/integration/test_workflow_analytics_integration.py
  - backend/core/workflow_analytics_engine.py
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "All 21 workflow_analytics integration tests pass"
    - "Tests correctly use status enum values (not raw strings)"
    - "Analytics database properly initialized in test fixtures"
    - "Metrics tracking and aggregation queries work end-to-end"
  artifacts:
    - path: "backend/tests/integration/test_workflow_analytics_integration.py"
      provides: "Fixed integration tests for workflow analytics"
      min_lines: 700
    - path: "backend/core/workflow_analytics_engine.py"
      contains: "class WorkflowAnalyticsEngine"
  key_links:
    - from: "test_workflow_analytics_integration.py"
      to: "core/workflow_analytics_engine.py"
      via: "real WorkflowAnalyticsEngine with temp database"
      pattern: "WorkflowAnalyticsEngine\\(db_path=|analytics_engine\\."
---

<objective>
Fix the 21 failing/erroring workflow_analytics integration tests caused by incorrect status parameter usage (passing str where WorkflowStatus enum expected).

**Root Causes (from VERIFICATION.md):**
1. AttributeError: 'str' object has no attribute 'value' - tests passing raw strings to methods expecting WorkflowStatus enum
2. Test fixture setup failing due to API mismatch
3. 10 tests FAILED, 11 tests ERROR in collection/setup

**Gap Closed:** 21 failing/erroring tests in test_workflow_analytics_integration.py

**Output:** Working integration tests that execute real analytics code with temp SQLite databases
</objective>

<execution_context>
@/Users/rushiparikh/.claude/get-shit-done/workflows/execute-plan.md
@/Users/rushiparikh/.claude/get-shit-done/templates/summary.md
@.planning/phases/19-coverage-push-and-bug-fixes/19-VERIFICATION.md
</execution_context>

<context>
@.planning/phases/19-coverage-push-and-bug-fixes/19-01-SUMMARY.md
@.planning/phases/19-coverage-push-and-bug-fixes/19-04-FAILURES.md
@backend/core/workflow_analytics_engine.py
@backend/tests/integration/test_workflow_analytics_integration.py
</context>

<tasks>

<task type="auto">
  <name>Fix status parameter usage in test fixtures and test methods</name>
  <files>backend/tests/integration/test_workflow_analytics_integration.py</files>
  <action>
    The tests fail with: `AttributeError: 'str' object has no attribute 'value'`

    This occurs because tests pass raw strings like "completed" to methods that call `status.value`, but the code expects WorkflowStatus enum.

    Looking at the error traceback:
    ```
    backend/core/workflow_analytics_engine.py:270: in track_workflow_completion
        status=status.value,
    E   AttributeError: 'str' object has no attribute 'value'
    ```

    Fix approach: Update all test calls to use WorkflowStatus enum instead of raw strings.

    Find and replace:
    - `status="completed"` → `status=WorkflowStatus.COMPLETED`
    - `status="failed"` → `status=WorkflowStatus.FAILED`
    - `status="running"` → `status=WorkflowStatus.RUNNING`
    - `status="paused"` → `status=WorkflowStatus.PAUSED`
    - `status="cancelled"` → `status=WorkflowStatus.CANCELLED`
    - `status="timeout"` → `status=WorkflowStatus.TIMEOUT`

    Locations to fix (from grep results):
    - Line ~259 in analytics_engine fixture
    - Line ~380 in analytics_engine fixture
    - Line ~511 in analytics_engine fixture
    - Any other track_workflow_completion, track_workflow_start calls

    Import at top of file (if not present):
    ```python
    from core.workflow_analytics_engine import WorkflowStatus
    ```
  </action>
  <verify>
    # Run the tests that were erroring in setup
    PYTHONPATH=/Users/rushiparikh/projects/atom/backend pytest backend/tests/integration/test_workflow_analytics_integration.py::TestAggregationQueries -v --tb=line 2>&1 | head -50
  </verify>
  <done>No more AttributeError on status.value in fixture setup</done>
</task>

<task type="auto">
  <name>Fix FAILED tests (metrics tracking queries)</name>
  <files>backend/tests/integration/test_workflow_analytics_integration.py</files>
  <action>
    After fixing the status enum issues, some tests may still FAIL (not ERROR) due to assertion problems.

    The 5 original FAILED tests:
    - test_track_step_execution_creates_metrics_record
    - test_track_workflow_execution_creates_summary
    - test_track_execution_time_recorded
    - test_track_error_counts_recorded
    - test_track_success_rates_calculated
    - test_create_alert
    - test_trigger_alert
    - test_list_active_alerts
    - test_metrics_persist_across_restarts
    - test_events_persist_across_restarts

    Common issues:
    1. SQLite queries not finding data due to timing (background processing)
    2. Incorrect assertion expectations
    3. Database cleanup between tests

    Fix each:
    1. Increase time.sleep() after track_* calls to allow background processing (0.1 → 0.2 seconds)
    2. Verify table/column names match actual schema in workflow_analytics_engine.py
    3. Ensure temp database is properly cleaned up between tests

    For test_track_step_execution_creates_metrics_record (~line 53):
    - Ensure time.sleep(0.2) after track_step_execution
    - Check SQLite query matches actual table schema
    - Verify workflow_metrics table exists with correct columns

    Similar fixes for other tracking tests.
  </action>
  <verify>
    PYTHONPATH=/Users/rushiparikh/projects/atom/backend pytest backend/tests/integration/test_workflow_analytics_integration.py::TestWorkflowMetricsTracking -v --tb=short
  </verify>
  <done>All TestWorkflowMetricsTracking tests pass</done>
</task>

<task type="auto">
  <name>Fix aggregation query tests (ERROR tests from setup)</name>
  <files>backend/tests/integration/test_workflow_analytics_integration.py</files>
  <action>
    The TestAggregationQueries, TestPerformanceReporting, TestMultiUserAnalytics tests were ERRORing due to fixture setup.

    After fixing status enum in fixture (Task 1), verify these now pass:
    - test_get_workflow_metrics_by_id
    - test_get_workflow_metrics_by_date_range
    - test_get_average_execution_time
    - test_get_most_failed_steps
    - test_get_workflow_success_rate
    - test_generate_performance_report
    - test_generate_comparison_report
    - test_export_metrics_to_csv
    - test_metrics_trend_analysis
    - test_get_user_specific_metrics
    - test_get_cross_user_comparison

    Common remaining issues:
    1. Methods not returning expected data structure
    2. Empty results when data should exist
    3. Type mismatches in assertions

    For each failing test:
    1. Check what the actual return value is (add debug print)
    2. Compare with expected assertion
    3. Fix either the test expectation or add missing data setup

    Example: If test_get_workflow_metrics_by_id expects dict but gets None:
    - Verify track_workflow_completion was called with correct params
    - Increase wait time for background processing
    - Check if get_workflow_metrics_by_id requires different query params
  </action>
  <verify>
    PYTHONPATH=/Users/rushiparikh/projects/atom/backend pytest backend/tests/integration/test_workflow_analytics_integration.py::TestAggregationQueries -v --tb=short
    PYTHONPATH=/Users/rushiparikh/projects/atom/backend pytest backend/tests/integration/test_workflow_analytics_integration.py::TestPerformanceReporting -v --tb=short
    PYTHONPATH=/Users/rushiparikh/projects/atom/backend pytest backend/tests/integration/test_workflow_analytics_integration.py::TestMultiUserAnalytics -v --tb=short
  </verify>
  <done>All aggregation and reporting tests pass</done>
</task>

<task type="auto">
  <name>Fix alerting and persistence tests</name>
  <files>backend/tests/integration/test_workflow_analytics_integration.py</files>
  <action>
    Fix remaining tests in TestAlerting and TestDataPersistence classes.

    TestAlerting tests:
    - test_create_alert (~line 597)
    - test_trigger_alert (~line 617)
    - test_list_active_alerts (~line 637)

    TestDataPersistence tests:
    - test_metrics_persist_across_restarts (~line 660)
    - test_events_persist_across_restarts (~line 697)

    Issues:
    1. Alert creation/checking may not work as expected
    2. Persistence tests may not properly simulate restart (need to close connection before reopening)
    3. Database file cleanup issues

    For persistence tests specifically:
    - Ensure the first engine instance is properly closed before creating new one
    - Use sqlite3.connect directly to verify data persistence
    - Add time.sleep(0.2) before checking persistence

    For alert tests:
    - Verify create_alert returns an alert object with alert_id
    - check_alert may return False (not triggered) or None - handle both
    - list_alerts should return list, check length >= expected count
  </action>
  <verify>
    PYTHONPATH=/Users/rushiparikh/projects/atom/backend pytest backend/tests/integration/test_workflow_analytics_integration.py::TestAlerting -v --tb=short
    PYTHONPATH=/Users/rushiparikh/projects/atom/backend pytest backend/tests/integration/test_workflow_analytics_integration.py::TestDataPersistence -v --tb=short
  </verify>
  <done>All alerting and persistence tests pass</done>
</task>

<task type="auto">
  <name>Run all workflow_analytics tests and verify 100% pass rate</name>
  <files>backend/tests/integration/test_workflow_analytics_integration.py</files>
  <action>
    Run the full test suite for workflow_analytics integration:

    ```bash
    PYTHONPATH=/Users/rushiparikh/projects/atom/backend pytest backend/tests/integration/test_workflow_analytics_integration.py -v --tb=short
    ```

    Verify all 21 tests pass:
    - TestWorkflowMetricsTracking (5 tests)
    - TestAggregationQueries (5 tests)
    - TestPerformanceReporting (5 tests)
    - TestMultiUserAnalytics (2 tests)
    - TestAlerting (3 tests)
    - TestDataPersistence (2 tests)

    Total should be: 21 passed

    If any fail, debug and fix.
  </action>
  <verify>
    PYTHONPATH=/Users/rushiparikh/projects/atom/backend pytest backend/tests/integration/test_workflow_analytics_integration.py -v
    Expected: 21 passed
  </verify>
  <done>21/21 tests passing (100% pass rate)</done>
</task>

</tasks>

<verification>
Run full test suite to verify fixes:
```bash
PYTHONPATH=/Users/rushiparikh/projects/atom/backend pytest backend/tests/integration/test_workflow_analytics_integration.py -v
```

Verify:
- [ ] All 21 tests pass
- [ ] No AttributeError: 'str' object has no attribute 'value'
- [ ] No ERROR in test collection/setup
- [ ] Actual database operations execute (not just mocks)
</verification>

<success_criteria>
- 21/21 tests pass (100% pass rate)
- 0 setup/collection errors
- 0 AttributeError on status.value
- Tests use real WorkflowAnalyticsEngine with SQLite
</success_criteria>

<output>
After completion, create `.planning/phases/19-coverage-push-and-bug-fixes/19-06-SUMMARY.md` with:
- Number of tests fixed (21)
- Key fix: WorkflowStatus enum usage
- Updated pass rate
</output>
