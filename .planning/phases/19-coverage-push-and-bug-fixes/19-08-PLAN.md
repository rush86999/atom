---
phase: 19-coverage-push-and-bug-fixes
plan: 08
type: execute
wave: 2
depends_on: ["19-05", "19-06", "19-07"]
files_modified:
  - backend/tests/integration/test_atom_agent_endpoints_expanded.py
  - backend/tests/property_tests/governance/test_agent_governance_invariants.py
  - backend/tests/unit/test_canvas_tool_expanded.py
  - backend/core/atom_agent_endpoints.py
  - backend/core/agent_governance_service.py
  - backend/tools/canvas_tool.py
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "Tests actually execute production code (not just mocks)"
    - "Coverage measurement shows >0% for target files"
    - "Real database sessions used (not in-memory mocks)"
    - "Passing tests translate to actual code coverage"
  artifacts:
    - path: "backend/tests/integration/test_atom_agent_endpoints_expanded.py"
      provides: "Integration tests with real code execution"
    - path: "backend/tests/property_tests/governance/test_agent_governance_invariants.py"
      provides: "Property tests with reduced mocking"
    - path: "backend/tests/unit/test_canvas_tool_expanded.py"
      provides: "Unit tests with real code paths"
  key_links:
    - from: "test_atom_agent_endpoints_expanded.py"
      to: "core/atom_agent_endpoints.py"
      via: "FastAPI TestClient with real app (not fully mocked)"
      pattern: "client\\.post|client\\.get|app = "
    - from: "test_agent_governance_invariants.py"
      to: "core/agent_governance_service.py"
      via: "real AgentGovernanceService instances"
      pattern: "AgentGovernanceService\\("
    - from: "test_canvas_tool_expanded.py"
      to: "tools/canvas_tool.py"
      via: "real CanvasTool (not mocked to return None)"
      pattern: "CanvasTool\\("
---

<objective>
Reduce over-mocking across all Phase 19 tests to enable actual code coverage measurement. Tests are passing but not executing production code (0% coverage on atom_agent_endpoints.py despite 28 passing tests).

**Root Causes (from VERIFICATION.md):**
1. Over-mocking: Tests mock everything, don't execute real code
2. atom_agent_endpoints.py: 0% coverage despite 28 passing tests
3. agent_governance_service.py: 15.82% coverage with 13 passing tests
4. Tests validate invariants but don't exercise code paths

**Gap Closed:** 0% coverage files with "passing" tests

**Output:** Tests that execute real code paths for accurate coverage measurement
</objective>

<execution_context>
@/Users/rushiparikh/.claude/get-shit-done/workflows/execute-plan.md
@/Users/rushiparikh/.claude/get-shit-done/templates/summary.md
@.planning/phases/19-coverage-push-and-bug-fixes/19-VERIFICATION.md
</execution_context>

<context>
@.planning/phases/19-coverage-push-and-bug-fixes/19-02-SUMMARY.md
@.planning/phases/19-coverage-push-and-bug-fixes/19-04-COVERAGE.md
@backend/core/atom_agent_endpoints.py
@backend/tests/integration/test_atom_agent_endpoints_expanded.py
@backend/core/agent_governance_service.py
@backend/tests/property_tests/governance/test_agent_governance_invariants.py
@backend/tools/canvas_tool.py
@backend/tests/unit/test_canvas_tool_expanded.py
</context>

<tasks>

<task type="auto">
  <name>Analyze over-mocking in atom_agent_endpoints tests</name>
  <files>backend/tests/integration/test_atom_agent_endpoints_expanded.py</files>
  <action>
    First, understand why 28 tests pass but atom_agent_endpoints.py has 0% coverage.

    Run coverage on just this file:
    ```bash
    PYTHONPATH=/Users/rushiparikh/projects/atom/backend pytest backend/tests/integration/test_atom_agent_endpoints_expanded.py --cov=backend.core.atom_agent_endpoints --cov-report=term-missing
    ```

    Analyze:
    1. What is being mocked? (AsyncMock, MagicMock, patch)
    2. Are test fixtures creating a real FastAPI app or a mock?
    3. Do requests reach the actual endpoint handlers?

    Common anti-patterns to identify:
    - `@patch('core.atom_agent_endpoints.some_function')` - entire function mocked
    - Mocked database sessions instead of real SQLite
    - Mocked entire response objects
    - Tests that never import or call real functions

    Document findings: What needs to be un-mocked to achieve coverage?
  </action>
  <verify>
    # Output should show 0% coverage with specific lines not covered
    PYTHONPATH=/Users/rushiparikh/projects/atom/backend pytest backend/tests/integration/test_atom_agent_endpoints_expanded.py --cov=backend.core.atom_agent_endpoints --cov-report=term-missing -v 2>&1 | tail -30
  </verify>
  <done>Analysis complete: list of mocks preventing coverage</done>
</task>

<task type="auto">
  <name>Refactor atom_agent_endpoints tests to use real app</name>
  <files>backend/tests/integration/test_atom_agent_endpoints_expanded.py</files>
  <action>
    Refactor tests to use real FastAPI app with selective mocking (only external dependencies).

    Principles:
    1. Import and use the actual FastAPI app (from core.main import app or from core.atom_agent_endpoints import router)
    2. Use real database sessions (SQLite in-memory) not mocked sessions
    3. Only mock external services (OpenAI API, external HTTP calls)
    4. Let endpoints execute their logic

    Changes:
    1. Create a test fixture that provides real app with test database:
       ```python
       @pytest.fixture
       def test_client():
           from core.main import app  # Real app
           from core.database import get_db
           from sqlalchemy import create_engine
           from sqlalchemy.orm import sessionmaker

           # Use in-memory SQLite
           engine = create_engine("sqlite:///:memory:")
           TestingSessionLocal = sessionmaker(bind=engine)

           def override_get_db():
               db = TestingSessionLocal()
               try:
                   yield db
               finally:
                   db.close()

           app.dependency_overrides[get_db] = override_get_db
           yield TestClient(app)
           app.dependency_overrides.clear()
       ```

    2. Remove patches that mock internal functions
    3. Update tests to make real HTTP requests through TestClient
    4. Verify responses match expected format

    Goal: Each test should execute the actual endpoint handler code.
  </action>
  <verify>
    PYTHONPATH=/Users/rushiparikh/projects/atom/backend pytest backend/tests/integration/test_atom_agent_endpoints_expanded.py --cov=backend.core.atom_agent_endpoints --cov-report=term-missing -v 2>&1 | grep -E "TOTAL|atom_agent_endpoints"
  </verify>
  <done>Coverage increases from 0% to at least 20%</done>
</task>

<task type="auto">
  <name>Reduce over-mocking in agent_governance property tests</name>
  <files>backend/tests/property_tests/governance/test_agent_governance_invariants.py</files>
  <action>
    The agent_governance_invariants.py tests pass but only achieve 15.82% coverage.

    Analyze and fix:
    1. Run coverage to see which lines AREN'T covered
    2. Identify which test methods use excessive mocking
    3. Refactor to use real AgentGovernanceService instances

    Current coverage: 15.82%
    Target: 40%+ (property tests should validate invariants by executing real code)

    Changes:
    1. Use real AgentGovernanceService with test database
    2. Remove patches that mock governance methods
    3. Let Hypothesis generate real test data that flows through the service

    Example refactoring:
    ```python
    # Before (over-mocked):
    @patch('core.agent_governance_service.AgentGovernanceService.check_maturity')
    def test_maturity_check(self, mock_check):
        mock_check.return_value = "AUTONOMOUS"
        # Test passes but 0% coverage

    # After (real code):
    def test_maturity_check(self):
        service = AgentGovernanceService(test_db)
        result = service.check_maturity(agent_id)
        # Executes real code, achieves coverage
    ```

    Keep Hypothesis strategies but use real service methods.
  </action>
  <verify>
    PYTHONPATH=/Users/rushiparikh/projects/atom/backend pytest backend/tests/property_tests/governance/test_agent_governance_invariants.py --cov=backend.core.agent_governance_service --cov-report=term-missing -v
  </verify>
  <done>Coverage increases from 15.82% to at least 40%</done>
</task>

<task type="auto">
  <name>Verify canvas_tool tests and fix if needed</name>
  <files>backend/tests/unit/test_canvas_tool_expanded.py</files>
  <action>
    Canvas_tool tests currently pass (23/23). Verify they achieve actual coverage.

    Run coverage:
    ```bash
    PYTHONPATH=/Users/rushiparikh/projects/atom/backend pytest backend/tests/unit/test_canvas_tool_expanded.py --cov=backend.tools.canvas_tool --cov-report=term-missing
    ```

    If coverage is good (>30%), no changes needed - document as example.

    If coverage is low despite passing tests:
    1. Identify over-mocked test methods
    2. Refactor to use real CanvasTool instances
    3. Ensure canvas service methods execute real logic

    Canvas_tool tests should be easier to fix (no external APIs usually).
  </action>
  <verify>
    PYTHONPATH=/Users/rushiparikh/projects/atom/backend pytest backend/tests/unit/test_canvas_tool_expanded.py --cov=backend.tools.canvas_tool --cov-report=term-missing -v 2>&1 | tail -20
  </verify>
  <done>Canvas_tool coverage verified >30% or fixed</done>
</task>

<task type="auto">
  <name>Run coverage report on all Phase 19 target files</name>
  <files>backend/tests/coverage_reports/metrics/coverage.json</files>
  <action>
    Run comprehensive coverage report to verify improvements:

    ```bash
    PYTHONPATH=/Users/rushiparikh/projects/atom/backend pytest \
      backend/tests/property_tests/workflows/test_workflow_engine_async_execution.py \
      backend/tests/integration/test_workflow_analytics_integration.py \
      backend/tests/integration/test_atom_agent_endpoints_expanded.py \
      backend/tests/unit/test_byok_handler_expanded.py \
      backend/tests/unit/test_canvas_tool_expanded.py \
      backend/tests/property_tests/governance/test_agent_governance_invariants.py \
      --cov=backend.core.workflow_engine \
      --cov=backend.core.workflow_analytics_engine \
      --cov=backend.core.atom_agent_endpoints \
      --cov=backend.core.llm.byok_handler \
      --cov=backend.tools.canvas_tool \
      --cov=backend.core.agent_governance_service \
      --cov-report=term-missing \
      --cov-report=json:tests/coverage_reports/metrics/coverage.json
    ```

    Check each file:
    - workflow_engine.py: Should be >0% (was 0%)
    - workflow_analytics_engine.py: Should increase from 21.51%
    - atom_agent_endpoints.py: Should be >0% (was 0%)
    - byok_handler.py: Should increase from 9.47%
    - canvas_tool.py: Should stay high
    - agent_governance_service.py: Should increase from 15.82%

    Update trending.json with new numbers.
  </action>
  <verify>
    # Check coverage.json for improvements
    cat backend/tests/coverage_reports/metrics/coverage.json | python3 -m json.tool | grep -E "workflow_engine|workflow_analytics|atom_agent_endpoints|byok_handler|canvas_tool|agent_governance"
  </verify>
  <done>All target files show increased coverage vs baseline</done>
</task>

</tasks>

<verification>
Run full coverage report:
```bash
PYTHONPATH=/Users/rushiparikh/projects/atom/backend pytest \
  backend/tests/property_tests/workflows/ \
  backend/tests/integration/test_workflow_analytics_integration.py \
  backend/tests/integration/test_atom_agent_endpoints_expanded.py \
  backend/tests/unit/test_byok_handler_expanded.py \
  backend/tests/unit/test_canvas_tool_expanded.py \
  backend/tests/property_tests/governance/ \
  --cov=backend.core.workflow_engine \
  --cov=backend.core.workflow_analytics_engine \
  --cov=backend.core.atom_agent_endpoints \
  --cov=backend.core.llm.byok_handler \
  --cov=backend.tools.canvas_tool \
  --cov=backend.core.agent_governance_service \
  --cov-report=term
```

Verify:
- [ ] atom_agent_endpoints.py > 0% coverage
- [ ] agent_governance_service.py > 30% coverage
- [ ] Tests still pass after reducing mocks
- [ ] Overall coverage increased toward 25-27% target
</verification>

<success_criteria>
- atom_agent_endpoints.py: >15% coverage (from 0%)
- agent_governance_service.py: >35% coverage (from 15.82%)
- All tests still passing
- Overall coverage increased by at least 2%
</success_criteria>

<output>
After completion, create `.planning/phases/19-coverage-push-and-bug-fixes/19-08-SUMMARY.md` with:
- Coverage improvements per file
- Mocking reduction strategy
- Overall coverage increase
</output>
