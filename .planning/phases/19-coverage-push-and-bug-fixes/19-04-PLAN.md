---
phase: 19-coverage-push-and-bug-fixes
plan: 04
type: execute
wave: 2
depends_on: [19-01, 19-02, 19-03]
files_modified:
  - backend/tests/coverage_reports/metrics/coverage.json
  - .planning/phases/19-coverage-push-and-bug-fixes/19-PHASE-SUMMARY.md
autonomous: true

must_haves:
  truths:
    - "All test failures discovered during Phase 19 execution are fixed"
    - "Full test suite runs with 98%+ pass rate (TQ-02 requirement)"
    - "No flaky tests remain (TQ-04 requirement)"
    - "Overall coverage increased by 3-5 percentage points"
    - "Coverage report documented and trending.json updated"
  artifacts:
    - path: "backend/tests/coverage_reports/metrics/coverage.json"
      provides: "Final coverage report for Phase 19"
      contains: "Overall coverage >= 26%"
    - path: ".planning/phases/19-coverage-push-and-bug-fixes/19-PHASE-SUMMARY.md"
      provides: "Phase 19 completion summary"
      contains: "Coverage increase, test results, bug fixes applied"
  key_links:
    - from: "Phase 19 execution"
      to: "Phase 20 planning"
      via: "Coverage gaps identified for next phase"
      pattern: "next_phase.*coverage_gaps"
---

# Phase 19 Plan 04: Bug Fixes and Coverage Validation

**Objective:** Fix any test failures discovered during Phase 19 execution and validate final coverage results. Run full suite 3 times for stability verification (TQ-02, TQ-03, TQ-04).

**Purpose:** Ensure quality and stability of new tests while validating coverage targets achieved. Document results for next phase planning.

**Output:** Fixed tests, validated coverage report, phase summary with trending data.

## <execution_context>

@/Users/rushiparikh/.claude/get-shit-done/workflows/execute-plan.md
@/Users/rushiparikh/.claude/get-shit-done/templates/summary.md

</execution_context>

## <context>

@.planning/phases/19-coverage-push-and-bug-fixes/19-RESEARCH.md
@.planning/phases/10-fix-tests/10-fix-tests-01-SUMMARY.md
@backend/tests/conftest.py
@backend/tests/coverage_reports/metrics/trending.json

## <tasks>

<task type="auto">
  <name>Task 1: Run full test suite and identify failures</name>
  <files>backend/tests/coverage_reports/metrics/coverage.json</files>
  <action>
1. Run full test suite with verbose output:
```bash
PYTHONPATH=/Users/rushiparikh/projects/atom/backend pytest \
  backend/tests/ \
  -v \
  --tb=short \
  2>&1 | tee test_results.log
```

2. Parse test results for failures:
```bash
grep -E "FAILED|ERROR" test_results.log > failures.txt || echo "No failures"
```

3. Categorize failures:
- **Test bugs:** Incorrect assertions, wrong mocks, missing imports (fix in test files)
- **Production bugs:** Incorrect logic, missing error handling (fix in source files)
- **Flaky tests:** Timing issues, shared state (fix with fixtures, transactions)

4. Document each failure with:
- File and line number
- Error message
- Category (test/production/flaky)
- Proposed fix
  </action>
  <verify>
cat failures.txt 2>/dev/null || echo "No failures found"
  </verify>
  <done>
All failures categorized and documented in failures.txt (or no failures)
  </done>
</task>

<task type="auto">
  <name>Task 2: Fix discovered bugs (test, production, or flaky)</name>
  <files>backend/tests/ backend/core/ backend/api/ backend/tools/</files>
  <action>
For each failure documented in Task 1:

**Test bugs (fix in test files):**
- Correct assertion values
- Fix mock targets (wrong function mocked)
- Add missing imports
- Fix fixture usage

**Production bugs (fix in source files):**
- Correct logic errors
- Add missing error handling
- Fix API signatures
- Add validation

**Flaky tests (fix with fixtures):**
- Use transaction rollback pattern
- Add proper cleanup in fixtures
- Use @pytest.mark.flaky only as last resort
- Add timing allowances for async tests

**Bug fix workflow (from Phase 10):**
1. Isolate failing test: pytest path/to/test.py::test_name -v
2. Debug and identify root cause
3. Apply fix
4. Verify fix: pytest path/to/test.py::test_name -v
5. Check for regressions: pytest backend/tests/ -v

**Commit each bug fix separately:**
```bash
git commit -m "fix(tests): correct mock target in test_workflow_engine_async_execution"
git commit -m "fix(core): add missing error handling in workflow_analytics_engine"
```
  </action>
  <verify>
PYTHONPATH=/Users/rushiparikh/projects/atom/backend pytest backend/tests/ -v --tb=short
  </verify>
  <done>
All tests pass (100% pass rate for Phase 19 tests)
  </done>
</task>

<task type="auto">
  <name>Task 3: Validate 98%+ pass rate (TQ-02 requirement)</name>
  <files>backend/tests/coverage_reports/metrics/coverage.json</files>
  <action>
1. Run full test suite 3 times to verify stability:
```bash
for run in 1 2 3; do
  echo "=== Run $run ==="
  PYTHONPATH=/Users/rushiparikh/projects/atom/backend pytest \
    backend/tests/ \
    -v \
    --tb=no \
    --q \
    2>&1 | tee test_run_$run.log
done
```

2. Calculate pass rate for each run:
```bash
for run in 1 2 3; do
  passed=$(grep -c "PASSED" test_run_$run.log || echo 0)
  failed=$(grep -c "FAILED" test_run_$run.log || echo 0)
  total=$((passed + failed))
  rate=$(python -c "print(f'{passed/total*100:.2f}')")
  echo "Run $run: $passed/$total passed ($rate%)"
done
```

3. Verify TQ-02 requirement:
- All runs must have 98%+ pass rate
- If any run fails, investigate and fix flaky tests

4. Verify TQ-03 requirement:
- Full suite completes in <60 minutes (from Phase 10)
- If slower, identify slow tests and optimize
  </action>
  <verify>
python -c "
import re
for run in [1, 2, 3]:
    with open(f'test_run_{run}.log') as f:
        content = f.read()
        passed = len(re.findall(r'PASSED', content))
        failed = len(re.findall(r'FAILED', content))
        total = passed + failed
        rate = passed / total * 100 if total > 0 else 0
        assert rate >= 98, f'Run {run}: {rate:.2f}% < 98%'
        print(f'Run {run}: {rate:.2f}% passed')
"
  </verify>
  <done>
All 3 runs achieve 98%+ pass rate, TQ-02 and TQ-03 verified
  </done>
</task>

<task type="auto">
  <name>Task 4: Generate final coverage report and validate targets</name>
  <files>backend/tests/coverage_reports/metrics/coverage.json</files>
  <action>
1. Run full coverage report:
```bash
PYTHONPATH=/Users/rushiparikh/projects/atom/backend pytest \
  backend/tests/ \
  --cov=. \
  --cov-report=html \
  --cov-report=json \
  --cov-report=term-missing \
  --cov-context=test
```

2. Extract overall coverage:
```bash
python -c "
import json
with open('backend/tests/coverage_reports/metrics/coverage.json') as f:
    data = json.load(f)
    overall = data['totals']['percent_covered']
    print(f'Overall coverage: {overall:.2f}%')
    print(f'Target: 26-27%')
    print(f'Achieved: {overall >= 26}')
"
```

3. Verify Phase 19 targets:
- Overall coverage: 26-27% (up from 22.64%)
- Increase: +3.5-4.5 percentage points
- All tested files: 50%+ coverage

4. Update trending.json:
```python
import json
from datetime import datetime

with open('backend/tests/coverage_reports/metrics/trending.json') as f:
    trending = json.load(f)

trending['phases'].append({
    'phase': '19-coverage-push-and-bug-fixes',
    'date': datetime.now().isoformat(),
    'coverage': overall,
    'increase': overall - 22.64,
    'files_tested': 8,
    'tests_created': 200  # Update with actual count
})

with open('backend/tests/coverage_reports/metrics/trending.json', 'w') as f:
    json.dump(trending, f, indent=2)
```
  </action>
  <verify>
python -c "import json; d = json.load(open('backend/tests/coverage_reports/metrics/coverage.json')); print(f'Coverage: {d[\"totals\"][\"percent_covered\"]:.2f}%'); assert d['totals']['percent_covered'] >= 26"
  </verify>
  <done>
Overall coverage 26-27% achieved, trending.json updated
  </done>
</task>

<task type="auto">
  <name>Task 5: Create Phase 19 summary</name>
  <files>.planning/phases/19-coverage-push-and-bug-fixes/19-PHASE-SUMMARY.md</files>
  <action>
Create `.planning/phases/19-coverage-push-and-bug-fixes/19-PHASE-SUMMARY.md` with:

1. **Phase Overview:**
   - Goal: Achieve 25-27% coverage (+3-5% from 22.64%)
   - Plans: 4 (Wave 1: Plans 01-02, Wave 2: Plans 03-04)
   - Files tested: 8 high-impact files

2. **Coverage Results:**
   - Starting coverage: 22.64%
   - Ending coverage: X.XX%
   - Increase: +X.XX percentage points
   - Files tested with results table

3. **Tests Created:**
   - Total test count
   - Test file breakdown
   - Pass rate

4. **Bug Fixes Applied:**
   - Test bugs fixed: N
   - Production bugs fixed: N
   - Flaky tests fixed: N

5. **Quality Metrics (TQ-02, TQ-03, TQ-04):**
   - Pass rate: XX%
   - Duration: XX minutes
   - Flaky tests: 0

6. **Next Phase Readiness:**
   - Remaining high-impact files
   - Recommended focus for Phase 20
  </action>
  <verify>
cat .planning/phases/19-coverage-push-and-bug-fixes/19-PHASE-SUMMARY.md | grep -E "Coverage|Tests|Bug Fixes|Quality"
  </verify>
  <done>
Phase summary created with all sections complete
  </done>
</task>

</tasks>

## <verification>

After completion, verify:
1. All test failures fixed (test_results.log shows 0 FAILED)
2. Pass rate >= 98% across 3 runs (TQ-02)
3. Full suite completes in <60 minutes (TQ-03)
4. No flaky tests (consistent results across runs) (TQ-04)
5. Overall coverage >= 26%
6. Phase summary created and documented

</verification>

## <success_criteria>

- [ ] All test failures fixed
- [ ] 98%+ pass rate across 3 full suite runs
- [ ] Full suite completes in <60 minutes
- [ ] No flaky tests (stable results)
- [ ] Overall coverage >= 26% (target achieved)
- [ ] Phase summary created with trending data
- [ ] trending.json updated with Phase 19 results

## <output>

After completion, create `.planning/phases/19-coverage-push-and-bug-fixes/19-PHASE-SUMMARY.md`
</output>
