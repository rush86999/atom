---
phase: 05-coverage-quality-validation
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/tests/unit/episodes/test_episode_segmentation_service.py
  - backend/tests/unit/episodes/test_episode_retrieval_service.py
  - backend/tests/unit/episodes/test_episode_lifecycle_service.py
  - backend/tests/unit/episodes/test_episode_integration.py
  - backend/tests/unit/episodes/test_agent_graduation_service.py
autonomous: true

must_haves:
  truths:
    - "All episode service methods have unit tests covering happy path and error cases"
    - "Segmentation service correctly detects time gaps, topic changes, and task completion"
    - "Retrieval service supports all four modes: temporal, semantic, sequential, contextual"
    - "Lifecycle service handles decay, consolidation, and archival correctly"
    - "Episode integration tests cover canvas and feedback metadata linking"
    - "Graduation service uses episodic memory for promotion validation"
    - "Episode domain achieves 80% coverage"
  artifacts:
    - path: "backend/tests/unit/episodes/test_episode_segmentation_service.py"
      provides: "Unit tests for episode segmentation logic"
      contains: "test_class: TestEpisodeSegmentationService"
    - path: "backend/tests/unit/episodes/test_episode_retrieval_service.py"
      provides: "Unit tests for episode retrieval modes"
      contains: "test_class: TestEpisodeRetrievalService"
    - path: "backend/tests/unit/episodes/test_episode_lifecycle_service.py"
      provides: "Unit tests for episode lifecycle management"
      contains: "test_class: TestEpisodeLifecycleService"
    - path: "backend/tests/unit/episodes/test_episode_integration.py"
      provides: "Unit tests for episode-canvas-feedback integration"
      contains: "test_class: TestEpisodeIntegration"
    - path: "backend/tests/unit/episodes/test_agent_graduation_service.py"
      provides: "Unit tests for graduation using episodic memory"
      contains: "test_class: TestAgentGraduationService"
  key_links:
    - from: "test_episode_segmentation_service.py"
      to: "core/episode_segmentation_service.py"
      via: "direct imports and mocked agent interactions"
      pattern: "from core.episode_segmentation_service import"
    - from: "test_episode_retrieval_service.py"
      to: "core/episode_retrieval_service.py"
      via: "direct imports and mocked vector database"
      pattern: "from core.episode_retrieval_service import"
    - from: "test_episode_lifecycle_service.py"
      to: "core/episode_lifecycle_service.py"
      via: "direct imports and mocked database"
      pattern: "from core.episode_lifecycle_service import"
    - from: "test_episode_integration.py"
      to: "core/episode_integration.py"
      via: "direct imports and mocked canvas/feedback models"
      pattern: "from core.episode_integration import"
    - from: "test_agent_graduation_service.py"
      to: "core/agent_graduation_service.py"
      via: "direct imports and mocked episode data"
      pattern: "from core.agent_graduation_service import"
---

<objective>
Achieve 80% test coverage for the episodic memory domain by creating comprehensive unit tests for episode segmentation, retrieval, lifecycle, integration, and graduation validation.

Purpose: Episodic memory is a core AI capability that enables agents to learn from past experiences. The research shows episodic memory coverage at 15.52% with several episode services having minimal test coverage. This system implements automatic segmentation, four retrieval modes, lifecycle management, and graduation validation.

Output: Five new test files with 80%+ coverage for episodic memory domain services.
</objective>

<execution_context>
@/Users/rushiparikh/.claude/get-shit-done/workflows/execute-plan.md
@/Users/rushiparikh/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-coverage-quality-validation/05-RESEARCH.md

@backend/core/episode_segmentation_service.py
@backend/core/episode_retrieval_service.py
@backend/core/episode_lifecycle_service.py
@backend/core/episode_integration.py
@backend/core/agent_graduation_service.py
@backend/tests/property_tests/episodes/ (Phase 2 property tests)
@backend/tests/security/test_episode_access.py (Phase 3 access control tests)

# Phase 2 property tests already cover episode invariants
# Phase 3 security tests cover episode access control
# This plan focuses on unit test coverage for individual service methods
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create unit tests for EpisodeSegmentationService</name>
  <files>backend/tests/unit/episodes/test_episode_segmentation_service.py</files>
  <action>
Create comprehensive unit tests for EpisodeSegmentationService covering:

1. **Time-based segmentation**:
   - Test detecting gaps >30 minutes (default threshold)
   - Test configurable gap thresholds
   - Test segmentation of long conversations into multiple episodes
   - Test edge cases (exactly 30 minutes, 29 vs 31 minutes)

2. **Topic change detection**:
   - Test detecting topic shifts using embedding similarity
   - Test similarity threshold tuning (default 0.7)
   - Test multi-turn conversations with topic drift
   - Test handling of out-of-order messages

3. **Task completion detection**:
   - Test detecting when a task is marked complete
   - Test detecting workflow termination signals
   - Test detecting canvas presentation completion
   - Test detecting agent status changes

4. **Episode creation**:
   - Test creating Episode with proper metadata
   - Test creating EpisodeSegments linked to Episode
   - Test AgentExecution linking to Episode
   - Test timestamp accuracy and ordering

5. **Segment metadata**:
   - Test segment type classification (query, response, action)
   - Test segment content storage (truncated if too long)
   - Test segment embedding generation
   - Test segment count limits

Use Hypothesis for generating time series and conversation data.
Mock embedding service (LanceDB/OpenAI embeddings).
Use freezegun for time-based testing.

DO NOT test actual embedding generation (integration tests handle that).
Focus on segmentation logic and decision boundaries.
  </action>
  <verify>
pytest tests/unit/episodes/test_episode_segmentation_service.py -v --cov=core/episode_segmentation_service --cov-report=term-missing
  </verify>
  <done>
Coverage for episode_segmentation_service.py >= 80%
All tests pass with pytest -v
  </done>
</task>

<task type="auto">
  <name>Task 2: Create unit tests for EpisodeRetrievalService</name>
  <files>backend/tests/unit/episodes/test_episode_retrieval_service.py</files>
  <action>
Create comprehensive unit tests for EpisodeRetrievalService covering:

1. **Temporal retrieval**:
   - Test retrieving episodes by time range (start, end)
   - Test sorting by timestamp (newest first)
   - Test time range boundary conditions
   - Test performance target: ~10ms (per RESEARCH.md)

2. **Semantic retrieval**:
   - Test vector similarity search using embeddings
   - Test top-k retrieval (default 10 episodes)
   - Test similarity threshold filtering
   - Test handling of zero-match queries

3. **Sequential retrieval**:
   - Test retrieving full episodes with all segments
   - Test segment ordering within episodes
   - Test canvas and feedback context enrichment
   - Test episode linking (continuation from previous episode)

4. **Contextual retrieval**:
   - Test hybrid temporal + semantic search
   - Test context boosting (recent episodes, similar topics)
   - Test multi-query fusion (combine multiple retrieval strategies)
   - Test ranking and scoring algorithms

5. **Filtering and pagination**:
   - Test filtering by agent ID
   - Test filtering by episode type (workflow, canvas, etc.)
   - Test filtering by canvas type (sheets, charts, forms)
   - Test pagination (limit, offset)

6. **Access logging**:
   - Test EpisodeAccessLog creation for all retrievals
   - Test logging retrieval mode and parameters
   - Test access tracking for analytics

Mock LanceDB vector queries.
Mock PostgreSQL queries for temporal retrieval.
Use factories for Episode and EpisodeSegment models.

DO NOT test actual vector search performance.
Focus on retrieval logic and query construction.
  </action>
  <verify>
pytest tests/unit/episodes/test_episode_retrieval_service.py -v --cov=core/episode_retrieval_service --cov-report=term-missing
  </verify>
  <done>
Coverage for episode_retrieval_service.py >= 80%
All tests pass with pytest -v
  </done>
</task>

<task type="auto">
  <name>Task 3: Create unit tests for EpisodeLifecycleService</name>
  <files>backend/tests/unit/episodes/test_episode_lifecycle_service.py</files>
  <action>
Create comprehensive unit tests for EpisodeLifecycleService covering:

1. **Episode decay**:
   - Test time-based decay scoring (older episodes less relevant)
   - Test decay rate calculation (default half-life)
   - Test decay threshold for archival
   - Test updating episode decay_score in database

2. **Episode consolidation**:
   - Test merging related episodes (same topic, time range)
   - Test consolidation criteria (segment count, similarity)
   - Test preserving original metadata after consolidation
   - Test handling of consolidated segment references

3. **Episode archival**:
   - Test moving cold episodes to LanceDB (PostgreSQL -> LanceDB)
   - Test archival criteria (age, access frequency, decay score)
   - Test archival batch processing
   - Test restoring archived episodes to hot storage

4. **Episode deletion**:
   - Test soft delete (mark as deleted, don't remove)
   - Test hard delete (remove from database and vector store)
   - Test cascade deletion (segments, access logs)
   - Test deletion permissions (owner vs admin)

5. **Lifecycle state transitions**:
   - Test state transitions (ACTIVE -> DECAYED -> CONSOLIDATED -> ARCHIVED)
   - Test invalid state transitions (rejected)
   - Test state transition logging
   - Test state-based filtering in retrieval

6. **Background jobs**:
   - Test scheduled decay calculation job
   - Test scheduled consolidation job
   - Test scheduled archival job
   - Test job idempotency (safe to run multiple times)

Mock database queries and updates.
Mock LanceDB archival operations.
Use freezegun for time-based lifecycle testing.

DO NOT test actual background job scheduling.
Focus on lifecycle logic and state transitions.
  </action>
  <verify>
pytest tests/unit/episodes/test_episode_lifecycle_service.py -v --cov=core/episode_lifecycle_service --cov-report=term-missing
  </verify>
  <done>
Coverage for episode_lifecycle_service.py >= 80%
All tests pass with pytest -v
  </done>
</task>

<task type="auto">
  <name>Task 4: Create unit tests for EpisodeIntegration</name>
  <files>backend/tests/unit/episodes/test_episode_integration.py</files>
  <action>
Create comprehensive unit tests for EpisodeIntegration covering:

1. **Canvas metadata linking**:
   - Test linking CanvasAudit records to episodes
   - Test filtering by canvas action (present, submit, close, update, execute)
   - Test filtering by canvas type (sheets, charts, forms, docs, email, etc.)
   - Test canvas context enrichment in retrieval

2. **Feedback metadata linking**:
   - Test linking AgentFeedback records to episodes
   - Test aggregating feedback scores (-1.0 to +1.0)
   - Test feedback-weighted retrieval (positive boost, negative penalty)
   - Test feedback context enrichment in retrieval

3. **Episode creation with metadata**:
   - Test creating episode with canvas context
   - Test creating episode with feedback context
   - Test creating episode with both metadata types
   - Test metadata-only storage (lightweight references, not full data)

4. **Retrieval with metadata**:
   - Test retrieving episodes with canvas_context included
   - Test retrieving episodes with feedback_context included
   - Test filtering episodes by canvas type
   - Test sorting by feedback score (highest rated first)

5. **Metadata updates**:
   - Test adding canvas metadata to existing episode
   - Test adding feedback metadata to existing episode
   - Test updating aggregated feedback scores
   - Test handling missing metadata (graceful degradation)

6. **Action filtering**:
   - Test filtering canvas interactions by action type
   - Test excluding read-only actions from episode linking
   - Test including state-changing actions (submit, execute)

Mock CanvasAudit and AgentFeedback database queries.
Use factories for Episode, CanvasAudit, and AgentFeedback models.

DO NOT test actual canvas or feedback operations.
Focus on metadata linking and enrichment logic.
  </action>
  <verify>
pytest tests/unit/episodes/test_episode_integration.py -v --cov=core/episode_integration --cov-report=term-missing
  </verify>
  <done>
Coverage for episode_integration.py >= 80%
All tests pass with pytest -v
  </done>
</task>

<task type="auto">
  <name>Task 5: Create unit tests for Agent Graduation using Episodic Memory</name>
  <files>backend/tests/unit/episodes/test_agent_graduation_service.py</files>
  <action>
Create comprehensive unit tests for AgentGraduationService episodic memory integration covering:

**SCOPE:** This test file focuses on episodic memory integration (episode counts, intervention rates from SupervisionSession, constitutional compliance from EpisodeAccessLog). Governance logic (maturity transitions, confidence scores, permission matrix) is tested in Plan 01b Task 2 (test_agent_graduation_governance.py).

1. **Episode count validation**:
   - Test counting completed episodes per maturity level
   - Test excluding failed/incomplete episodes
   - Test verifying minimum episode requirements:
     * STUDENT -> INTERN: 10 episodes
     * INTERN -> SUPERVISED: 25 episodes
     * SUPERVISED -> AUTONOMOUS: 50 episodes

2. **Intervention rate calculation**:
   - Test calculating intervention rate from supervision sessions
   - Test intervention rate threshold validation:
     * STUDENT -> INTERN: <=50% intervention rate
     * INTERN -> SUPERVISED: <=20% intervention rate
     * SUPERVISED -> AUTONOMOUS: 0% intervention rate
   - Test intervention tracking via episodic memory

3. **Constitutional compliance validation**:
   - Test retrieving constitutional violations from episodes
   - Test calculating compliance score (0.0 to 1.0)
   - Test compliance score thresholds:
     * STUDENT -> INTERN: >=0.70 constitutional score
     * INTERN -> SUPERVISED: >=0.85 constitutional score
     * SUPERVISED -> AUTONOMOUS: >=0.95 constitutional score
   - Test Knowledge Graph rule validation

4. **Readiness score calculation**:
   - Test weighted score: 40% episode count, 30% intervention rate, 30% constitutional
   - Test score aggregation from multiple criteria
   - Test threshold-based promotion (score >= 0.7 for promotion)

5. **Graduation execution**:
   - Test successful promotion with all criteria met
   - Test failed promotion with unmet criteria
   - Test rollback on validation failure
   - Test graduation audit trail creation

Mock Episode, EpisodeAccessLog, and SupervisionSession queries.
Mock Knowledge Graph for constitutional rule checks.
Use Hypothesis for generating episode counts and intervention rates.

DO NOT duplicate property tests from Phase 2 (student_training directory).
Focus on episodic memory integration for graduation decisions.
  </action>
  <verify>
pytest tests/unit/episodes/test_agent_graduation_service.py -v --cov=core/agent_graduation_service --cov-report=term-missing
  </verify>
  <done>
Coverage for agent_graduation_service.py >= 80% (episodic memory integration)
All tests pass with pytest -v
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. Run all episode unit tests:
   ```bash
   pytest tests/unit/episodes/ -v --cov=core/episode_segmentation_service --cov=core/episode_retrieval_service --cov=core/episode_lifecycle_service --cov=core/episode_integration --cov=core/agent_graduation_service --cov-report=term-missing
   ```

2. Verify each file achieves >= 80% coverage

3. Verify all tests pass in parallel:
   ```bash
   pytest tests/unit/episodes/ -n auto --dist loadscope
   ```

4. Verify zero shared state (run 10 times, results identical):
   ```bash
   for i in {1..10}; do pytest tests/unit/episodes/ -q; done
   ```

5. Check episode domain coverage in coverage report
</verification>

<success_criteria>
1. All five episode service files have >= 80% code coverage
2. All tests pass with pytest -v
3. All tests pass in parallel with pytest-xdist
4. Zero test failures when run 10 times sequentially (no flaky tests)
5. Total episode domain coverage >= 80% (including existing tests from Phases 2-3)
6. All four retrieval modes (temporal, semantic, sequential, contextual) are tested
</success_criteria>

<output>
After completion, create `.planning/phases/05-coverage-quality-validation/05-03-SUMMARY.md` with:
- Coverage achieved for each episode file
- Total number of tests added
- Any discovered bugs or issues
- Next steps for episode domain
</output>
