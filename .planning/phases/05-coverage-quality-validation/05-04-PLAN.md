---
phase: 05-coverage-quality-validation
plan: 04
type: execute
wave: 2
depends_on: ["05-01", "05-02", "05-03"]
files_modified:
  - backend/pytest.ini
  - backend/tests/test_flaky_detection.py
  - backend/tests/test_isolation_validation.py
  - backend/tests/test_performance_baseline.py
  - backend/.coveragerc
autonomous: true

must_haves:
  truths:
    - "Flaky test detection is configured with pytest-rerunfailures"
    - "Full test suite executes in <5 minutes with parallel execution"
    - "Zero shared state verified by running tests 10 times with identical results"
    - "Test isolation is validated with unique resources and transaction rollback"
    - "Performance baselines are established for critical test suites"
  artifacts:
    - path: "backend/pytest.ini"
      provides: "Pytest configuration with flaky test detection"
      contains: "reruns"
    - path: "backend/tests/test_flaky_detection.py"
      provides: "Flaky test detection and validation"
      contains: "test_class: TestFlakyDetection"
    - path: "backend/tests/test_isolation_validation.py"
      provides: "Test isolation validation suite"
      contains: "test_class: TestIsolationValidation"
    - path: "backend/tests/test_performance_baseline.py"
      provides: "Performance baseline tests"
      contains: "test_class: TestPerformanceBaseline"
    - path: "backend/.coveragerc"
      provides: "Coverage configuration with branch coverage"
      contains: "[run]"
  key_links:
    - from: "pytest.ini"
      to: "pytest-rerunfailures plugin"
      via: "addopts configuration"
      pattern: "--reruns"
    - from: "test_flaky_detection.py"
      to: "pytest-xdist worker isolation"
      via: "worker ID environment variable"
      pattern: "PYTEST_XDIST_WORKER_ID"
    - from: "test_isolation_validation.py"
      to: "conftest.py fixtures"
      via: "unique_resource_name fixture"
      pattern: "unique_resource_name"
    - from: "test_performance_baseline.py"
      to: "pytest --durations reporting"
      via: "slowest test durations"
      pattern: "--durations"
---

<objective>
Configure flaky test detection, validate test isolation, and establish performance baselines to ensure test suite quality. Verify full test suite executes in <5 minutes with zero shared state.

Purpose: Test quality is as important as test coverage. Flaky tests undermine confidence in the test suite, shared state causes false positives/negatives, and slow tests reduce developer productivity. This plan configures infrastructure to detect and prevent these quality issues.

Output: Enhanced pytest configuration, flaky test detection suite, isolation validation tests, and performance baseline documentation.
</objective>

<execution_context>
@/Users/rushiparikh/.claude/get-shit-done/workflows/execute-plan.md
@/Users/rushiparikh/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-coverage-quality-validation/05-RESEARCH.md

@backend/pytest.ini
@backend/tests/conftest.py
@backend/tests/property_tests/conftest.py
@backend/tests/integration/conftest.py
@backend/tests/security/conftest.py
@backend/.planning/phases/01-test-infrastructure/01-test-infrastructure-03-SUMMARY.md

# Phase 1 already configured pytest-xdist for parallel execution
# Phase 1 already configured coverage reporting
# This plan adds flaky test detection and quality validation
</context>

<tasks>

<task type="auto">
  <name>Task 1: Configure pytest-rerunfailures for flaky test detection</name>
  <files>backend/pytest.ini</files>
  <action>
Update pytest.ini to add flaky test detection:

1. **Add pytest-rerunfailures to addopts**:
   - Add `--reruns 3` for automatic retry of failed tests
   - Add `--reruns-delay 1` for 1-second delay between retries
   - This detects flaky tests by retrying before reporting failure

2. **Add flaky test marker**:
   - Add marker: `flaky: Tests that may be flaky and need retry`
   - Document when to use @pytest.mark.flaky

3. **Configure strict mode for flaky detection**:
   - Add `--rerun-exclude` to exclude certain tests from retry
   - Exclude tests that should never pass (expected failures)

4. **Update documentation in pytest.ini**:
   - Document flaky test detection configuration
   - Document how to mark tests as flaky
   - Document how to investigate flaky test failures

DO NOT add @pytest.mark.flaky to existing tests (only mark new flaky tests as discovered).
DO NOT configure --reruns-all (only rerun actual failures).
Focus on detection, not masking of flaky tests.
  </action>
  <verify>
grep -E "(--reruns|flaky)" backend/pytest.ini
pytest --collect-only -q 2>/dev/null | grep "flaky"
  </verify>
  <done>
pytest.ini includes --reruns configuration
Flaky marker defined in markers section
Documentation comment added to pytest.ini
  </done>
</task>

<task type="auto">
  <name>Task 2: Create flaky test detection validation suite</name>
  <files>backend/tests/test_flaky_detection.py</files>
  <action>
Create test suite to validate flaky test detection:

1. **Test that rerunfailures is configured**:
   - Test that pytest has reruns configured
   - Test that failed tests are retried
   - Test that retry count matches configuration (3)

2. **Test that flaky marker exists**:
   - Test that @pytest.mark.flaky is recognized
   - Test that flaky marker can be applied to tests

3. **Create demonstration flaky test**:
   - Create test that fails intermittently (random() < 0.5)
   - Mark with @pytest.mark.flaky
   - Test that it passes with retries

4. **Create documentation test**:
   - Document common causes of flaky tests
   - Document how to fix flaky tests (proper async coordination, mocks vs real services)
   - Document when to use @pytest.mark.flaky (temporary workaround only)

5. **Test that non-flaky tests fail correctly**:
   - Test that tests without @pytest.mark.flaky fail immediately
   - Test that assertion errors are not masked by retries
   - Test that test suite fails if all retries exhausted

Use random for demonstration flaky test.
Mark demonstration test with skipif to avoid running in normal suite.

DO NOT create actual flaky tests in production code.
Focus on detection and documentation.
  </action>
  <verify>
pytest tests/test_flaky_detection.py -v --reruns 3
  </verify>
  <done>
Flaky test detection validated
Documentation for common flaky test causes included
  </done>
</task>

<task type="auto">
  <name>Task 3: Create test isolation validation suite</name>
  <files>backend/tests/test_isolation_validation.py</files>
  <action>
Create test suite to validate zero shared state between tests:

1. **Test unique_resource_name uniqueness**:
   - Test that unique_resource_name returns different values for each call
   - Test that worker ID is included in unique names
   - Test that UUID fragment is unique
   - Test parallel execution (multiple workers get different names)

2. **Test database transaction rollback**:
   - Test that db_session fixture rolls back transactions
   - Test that test data is not visible to other tests
   - Test that foreign key constraints are enforced
   - Test that database is clean after test runs

3. **Test parallel execution isolation**:
   - Create test that writes to shared resource with unique name
   - Run test in parallel (pytest -n auto)
   - Verify no collisions or race conditions

4. **Test fixture cleanup**:
   - Test that auto-use fixtures clean up after tests
   - Test that numpy/pandas modules are restored
   - Test that environment variables are reset

5. **Test global state isolation**:
   - Test that global variables are not mutated across tests
   - Test that singletons are reset between tests
   - Test that caches are cleared between tests

6. **Create integration test that runs full suite 10 times**:
   - Run full test suite 10 times sequentially
   - Verify identical results (all pass or same failures)
   - Report any non-deterministic test failures

Use pytest-xdist for parallel testing.
Use unique_resource_name fixture for collision-free testing.
Use db_session fixture for database isolation testing.

DO NOT modify existing test fixtures (conftest.py).
Focus on validation of existing isolation mechanisms.
  </action>
  <verify>
pytest tests/test_isolation_validation.py -v -n auto
for i in {1..10}; do pytest tests/test_isolation_validation.py -q || exit 1; done
  </verify>
  <done>
Isolation validation tests pass
Parallel execution shows no state sharing
10-run sequential test shows identical results
  </done>
</task>

<task type="auto">
  <name>Task 4: Create performance baseline tests</name>
  <files>backend/tests/test_performance_baseline.py</files>
<action>
Create test suite to establish and validate performance baselines:

1. **Test suite execution time**:
   - Measure full test suite execution time with pytest --durations
   - Verify suite completes in <5 minutes (target per RESEARCH.md)
   - Report slowest tests (>1 second)
   - Categorize tests by speed (fast <0.1s, medium <1s, slow >1s)

2. **Property test performance**:
   - Measure Hypothesis test execution time
   - Verify property tests complete in <1s per test (target per RESEARCH.md)
   - Report slowest property tests
   - Test that max_examples settings are appropriate

3. **Integration test performance**:
   - Measure integration test execution time
   - Verify database rollback overhead is acceptable
   - Verify WebSocket mock overhead is acceptable
   - Report slowest integration tests

4. **Parallel execution efficiency**:
   - Compare serial vs parallel execution time
   - Verify speedup from pytest-xdist (target: 2-3x on 4-core machine)
   - Measure worker startup overhead
   - Report optimal worker count (-n auto vs -n 4)

5. **Coverage calculation performance**:
   - Measure coverage calculation time
   - Verify coverage reporting overhead is acceptable
   - Test coverage JSON generation time

6. **Create performance documentation**:
   - Document baseline execution times
   - Document slowest tests and optimization targets
   - Document performance regression detection

Use pytest --durations for timing.
Use time command for total suite timing.
Create performance log file for trending.

DO NOT optimize test performance in this plan (focus on measurement).
DO NOT change test logic to improve performance.
Focus on establishing baselines for regression detection.
  </action>
  <verify>
pytest tests/test_performance_baseline.py -v --durations=20
time pytest tests/ -q -n auto  # Should complete in <5 minutes
  </verify>
  <done>
Performance baseline established
Full test suite completes in <5 minutes
Slowest tests documented
  </done>
</task>

<task type="auto">
  <name>Task 5: Update .coveragerc for branch coverage and quality gates</name>
  <files>backend/.coveragerc</files>
  <action>
Update or create .coveragerc for enhanced coverage configuration:

1. **Configure branch coverage**:
   - Enable branch coverage with [run] branch = True
   - This provides more accurate coverage measurement

2. **Configure partial coverage**:
   - Set partial_branches = True for partial if/else coverage
   - This improves coverage accuracy for complex conditionals

3. **Configure source paths**:
   - Add source = core, api, tools
   - This ensures coverage is measured for all backend code

4. **Configure omit patterns** (minimal, don't over-exclude):
   - Omit test files: */tests/*
   - Omit venv: */venv/*
   - Omit migrations: */alembic/versions/*
   - DO NOT omit production code (measure everything)

5. **Configure coverage fail under**:
   - Set fail_under = 80 for 80% minimum coverage
   - This enforces quality gate in CI

6. **Configure coverage report options**:
   - Enable HTML report with [html] directory
   - Enable JSON report for trending
   - Enable terminal report with [report] settings

If .coveragerc doesn't exist, create it with standard configuration.
If it exists, update it with the above settings.

DO NOT add aggressive exclusions that inflate coverage numbers.
DO NOT exclude files that should have tests.
Focus on accurate measurement.
  </action>
  <verify>
cat backend/.coveragerc
pytest --cov-report=term 2>/dev/null | grep "branch"
  </verify>
  <done>
.coveragerc exists or updated
Branch coverage enabled
fail_under = 80 configured
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. Run flaky test detection validation:
   ```bash
   pytest tests/test_flaky_detection.py -v --reruns 3
   ```

2. Run isolation validation:
   ```bash
   pytest tests/test_isolation_validation.py -v -n auto
   for i in {1..10}; do pytest tests/test_isolation_validation.py -q; done
   ```

3. Run performance baseline:
   ```bash
   time pytest tests/ -q -n auto --durations=20
   ```

4. Verify coverage configuration:
   ```bash
   pytest --cov=core --cov=api --cov=tools --cov-report=term-missing
   ```

5. Check that all quality gates pass:
   - Flaky test detection is active
   - Isolation validation passes (no shared state)
   - Performance baseline is <5 minutes
   - Coverage fail_under is 80%
</verification>

<success_criteria>
1. pytest-rerunfailures is configured and active
2. Flaky test detection validation suite passes
3. Isolation validation shows zero shared state across 10 runs
4. Full test suite executes in <5 minutes
5. Branch coverage is enabled
6. Coverage fail_under is set to 80%
7. Performance baseline is documented
</success_criteria>

<output>
After completion, create `.planning/phases/05-coverage-quality-validation/05-04-SUMMARY.md` with:
- Flaky test detection configuration
- Isolation validation results
- Performance baseline times
- Coverage configuration summary
- Any discovered quality issues
- Next steps for test quality
</output>
