---
phase: 69-autonomous-coding-agents
plan: 05
type: execute
wave: 3
depends_on: [69-04]
files_modified:
  - backend/core/test_generator_service.py
  - backend/tests/test_test_generator_service.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Tests are generated from implementation task requirements"
    - "Parametrized tests cover multiple scenarios"
    - "Property-based tests use Hypothesis for edge cases"
    - "Fixtures provide proper setup/teardown for database/API"
    - "Coverage targets (85% unit, 70% integration) are enforced"
    - "Integration with pytest, pytest-asyncio, pytest-cov"
  artifacts:
    - path: "backend/core/test_generator_service.py"
      provides: "AI-powered test generation"
      min_lines: 350
      exports: ["TestGeneratorService", "generate_tests", "generate_fixtures", "analyze_coverage"]
    - path: "backend/tests/test_test_generator_service.py"
      provides: "Test coverage for test generator"
      min_lines: 200
  key_links:
    - from: "backend/core/test_generator_service.py"
      to: "core/autonomous_coder_agent.py"
      via: "Generated code as input for test generation"
      pattern: "from core.autonomous_coder_agent import ImplementationTask"
    - from: "backend/core/test_generator_service.py"
      to: "backend/conftest.py"
      via: "Shared fixture patterns"
      pattern: "pytest.fixture"
    - from: "backend/core/test_generator_service.py"
      to: "core/autonomous_planning_agent.py"
      via: "Test requirements from planning"
      pattern: "TestRequirementsGenerator"
---

<objective>
Implement Test Generator Service that automatically creates comprehensive pytest test suites from generated code, including parametrized tests, Hypothesis property-based tests, database fixtures, and coverage target enforcement.

Purpose: Generate production-ready tests that achieve 85% unit and 70% integration coverage automatically, reducing test writing time by 90%.
Output: TestGeneratorService with parametrized tests, Hypothesis integration, fixture generation, comprehensive tests
</objective>

<execution_context>
@/Users/rushiparikh/.claude/get-shit-done/workflows/execute-plan.md
@/Users/rushiparikh/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/69-autonomous-coding-agents/69-RESEARCH.md
@.planning/STATE.md
@backend/conftest.py
@backend/tests/test_agent_governance_service.py
@backend/core/autonomous_coder_agent.py
@backend/core/autonomous_planning_agent.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement test file structure generator</name>
  <files>backend/core/test_generator_service.py</files>
  <action>
    Create TestGeneratorService with file structure generation:

    ```python
    from typing import List, Dict, Any, Optional, Set
    from pathlib import Path
    import ast
    import re

    class TestFileStructureGenerator:
        \"\"\"Generate test file structure from source code.\"\"\"

        def __init__(self, project_root: str = "backend"):
            self.project_root = Path(project_root)
            self.tests_root = self.project_root / "tests"

        def generate_test_file_structure(
            self,
            source_file: str
        ) -> Dict[str, Any]:
            \"\"\"Analyze source file and generate test structure.

            Args:
                source_file: Path to source file (e.g., "core/oauth_service.py")

            Returns:
                {
                    \"test_file_path\": \"tests/test_oauth_service.py\",
                    \"test_class_name\": \"TestOAuthService\",
                    \"functions_to_test\": [
                        {\"name\": \"login\", \"args\": [...], \"return_type\": \"...\"}
                    ],
                    \"classes_to_test\": [
                        {\"name\": \"OAuthService\", \"methods\": [...]}
                    ],
                    \"fixtures_needed\": [\"db_session\", \"mock_oauth_provider\"]
                }
            \"\"\"

        def extract_testable_items(
            self,
            source_code: str
        ) -> Dict[str, List[Dict[str, Any]]]:
            \"\"\"Extract functions and classes that need testing.
            Uses AST parsing to find public methods and functions.
            \"\"\"

        def infer_test_cases(
            self,
            function: Dict[str, Any]
        ) -> List[Dict[str, str]]:
            \"\"\"Infer test cases from function signature and docstring.

            Returns:
                [
                    {\"name\": \"test_login_success\", \"scenario\": \"...\"},
                    {\"name\": \"test_login_failure\", \"scenario\": \"...\"}
                ]
            \"\"\"

        def suggest_test_name(
            self,
            function_name: str,
            scenario: str
        ) -> str:
            \"\"\"Generate test name following Atom conventions.
            Pattern: test_{function}_{scenario}
            \"\"\"
    ```

    Implementation requirements:
    - Use AST to parse source files
    - Extract all public functions and methods
    - Infer test cases from function signatures
    - Generate pytest-compatible test names
    - Minimum 100 lines for structure generation

    Test file naming:
    - Source: core/oauth_service.py
    - Test: tests/test_oauth_service.py
    - Source: api/auth_routes.py
    - Test: tests/api/test_auth_routes.py
  </action>
  <verify>grep -n "class TestFileStructureGenerator\|generate_test_file_structure" backend/core/test_generator_service.py</verify>
  <done>TestFileStructureGenerator with AST-based extraction</done>
</task>

<task type="auto">
  <name>Task 2: Implement parametrized test generator</name>
  <files>backend/core/test_generator_service.py</files>
  <action>
    Add parametrized test generation:

    ```python
    class ParametrizedTestGenerator:
        \"\"\"Generate pytest parametrized tests.\"\"\"

        def generate_parametrized_test(
            self,
            function: Dict[str, Any],
            scenarios: List[Dict[str, Any]]
        ) -> str:
            \"\"\"Generate parametrized test code.

            Example output:
            ```python
            @pytest.mark.parametrize(\"provider,expected_url\", [
                (\"google\", \"https://accounts.google.com/...\"),
                (\"github\", \"https://github.com/login/...\"),
                (\"microsoft\", \"https://login.microsoftonline.com/...\")
            ])
            def test_oauth_redirect_urls(provider, expected_url):
                service = OAuthService()
                url = service.get_authorization_url(provider)
                assert url.startswith(expected_url)
            ```
            \"\"\"

        def generate_test_cases_matrix(
            self,
            parameters: Dict[str, List[Any]],
            outcome_template: str
        ) -> List[Dict[str, Any]]:
            \"\"\"Generate test case matrix from parameter combinations.
            Creates cartesian product of all parameter values.
            \"\"\"

        def generate_idfn(
            self,
            parameters: Dict[str, Any]
        ) -> str:
            \"\"\"Generate idfn for parametrized test names.
            Creates readable test case IDs.
            \"\"\"

        def generate_parametrize_decorator(
            self,
            param_names: str,
            param_values: List[List[Any]]
        ) -> str:
            \"\"\"Generate @pytest.mark.parametrize decorator code.\"\"\"
    ```

    Implementation requirements:
    - Generate @pytest.mark.parametrize decorators
    - Create parameter matrices for combinatorial testing
    - Generate readable test IDs via idfn
    - Support edge cases, boundary values, error cases
    - Minimum 80 lines for parametrized tests

    Parameter strategies:
    - Valid inputs: typical values, boundary values
    - Invalid inputs: None, empty string, wrong type
    - Edge cases: min, max, just outside range
  </action>
  <verify>grep -n "class ParametrizedTestGenerator\|generate_parametrized_test" backend/core/test_generator_service.py</verify>
  <done>ParametrizedTestGenerator with combinatorial testing</done>
</task>

<task type="auto">
  <name>Task 3: Implement Hypothesis property-based test generator</name>
  <files>backend/core/test_generator_service.py</files>
  <action>
    Add Hypothesis property-based test generation:

    ```python
    class PropertyBasedTestGenerator:
        \"\"\"Generate Hypothesis property-based tests.\"\"\"

        def generate_property_test(
            self,
            function: Dict[str, Any],
            invariants: List[str]
        ) -> str:
            \"\"\"Generate property-based test using Hypothesis.

            Example output:
            ```python
            from hypothesis import given, strategies as st

            @given(st.text(min_size=1), st.text(min_size=1))
            def test_user_creation(name, email):
                user = User(name=name, email=email)
                assert user.name == name
                assert user.email == email
                assert user.id is not None  # Auto-generated
            ```
            \"\"\"

        def infer_strategy(
            self,
            param_type: str,
            constraints: Dict[str, Any]
        ) -> str:
            \"\"\"Infer Hypothesis strategy from parameter type.

            Mapping:
            - str -> st.text() or st.email() or st.url()
            - int -> st.integers()
            - float -> st.floats()
            - bool -> st.booleans()
            - List -> st.lists()
            - Dict -> st.dictionaries()
            \"\"\"

        def generate_property_test_invariants(
            self,
            function: Dict[str, Any]
        ) -> List[str]:
            \"\"\"Generate property invariants for function.

            Common invariants:
            - Idempotency: f(f(x)) == f(x)
            - Commutativity: f(a, b) == f(b, a)
            - Identity: f(x, identity) == x
            - Round-trip: decode(encode(x)) == x
            \"\"\"

        def generate_property_test_code(
            self,
            function_name: str,
            strategies: List[str],
            invariants: List[str]
        ) -> str:
            \"\"\"Generate complete property test code.
            Combines strategies and invariants into test.
            \"\"\"
    ```

    Implementation requirements:
    - Use hypothesis.strategies for test data generation
    - Infer strategies from type hints
    - Generate invariants based on function behavior
    - Follow Atom's max_examples standards (200/100/50)
    - Minimum 100 lines for property-based tests

    Strategy examples:
    - Email validation: st.text() @st.email regex
    - User IDs: st.uuid() or st.integers(min_value=1)
    - Timestamps: st.datetimes()
    - Enum values: st.sampled_from([list])
  </action>
  <verify>grep -n "class PropertyBasedTestGenerator\|generate_property_test" backend/core/test_generator_service.py</verify>
  <done>PropertyBasedTestGenerator with Hypothesis integration</done>
</task>

<task type="auto">
  <name>Task 4: Implement fixture generator for database and API</name>
  <files>backend/core/test_generator_service.py</files>
  <action>
    Add fixture generation methods:

    ```python
    class FixtureGenerator:
        \"\"\"Generate pytest fixtures for database and API mocking.\"\"\"

        def generate_db_fixture(
            self,
            models: List[str]
        ) -> str:
            \"\"\"Generate database session fixture.

            Example output:
            ```python
            @pytest.fixture
            def db_session():
                \"\"\"Create test database session with rollback.\"\"\"
                from core.database import SessionLocal, Base
                from core.models import OAuthService, User

                engine = create_engine(\"sqlite:///:memory:\")
                Base.metadata.create_all(engine)
                Session = sessionmaker(bind=engine)

                session = Session()
                try:
                    yield session
                finally:
                    session.rollback()
                    session.close()
            ```
            \"\"\"

        def generate_mock_fixture(
            self,
            external_service: str,
            methods: List[str]
        ) -> str:
            \"\"\"Generate mock fixture for external service.

            Example for OAuth provider:
            ```python
            @pytest.fixture
            def mock_oauth_provider(monkeypatch):
                async def mock_exchange(*args, **kwargs):
                    return {\"access_token\": \"test_token\", \"expires_in\": 3600}
                monkeypatch.setattr(OAuthService, \"exchange_code\", mock_exchange)
            ```
            \"\"\"

        def generate_test_data_factory(
            self,
            model: Dict[str, Any]
        ) -> str:
            \"\"\"Generate test data factory for model.

            Uses factory_boy patterns if available, otherwise simple fixture.
            \"\"\"

        def generate_api_client_fixture(
            self,
            routes: List[str]
        ) -> str:
            \"\"\"Generate FastAPI TestClient fixture.

            Example:
            ```python
            @pytest.fixture
            def api_client():
                from fastapi.testclient import TestClient
                from main import app
                return TestClient(app)
            ```
            \"\"\"
    ```

    Implementation requirements:
    - Follow existing Atom fixture patterns (see conftest.py)
    - Use SQLAlchemy rollback pattern for database fixtures
    - Use monkeypatch for mocking external services
    - Generate factory_boy fixtures for complex models
    - Minimum 80 lines for fixture generation

    Fixture patterns from conftest.py:
    - db_session: Database session with rollback
    - mock_llm_response: Mock LLM responses
    - client: TestClient for API testing
  </action>
  <verify>grep -n "class FixtureGenerator\|generate_db_fixture\|generate_mock_fixture" backend/core/test_generator_service.py</verify>
  <done>FixtureGenerator with database and API fixtures</done>
</task>

<task type="auto">
  <name>Task 5: Implement coverage analyzer and gap detector</name>
  <files>backend/core/test_generator_service.py</files>
  <action>
    Add coverage analysis methods:

    ```python
    class CoverageAnalyzer:
        \"\"\"Analyze coverage and identify test gaps.\"\"\"

        def __init__(self, project_root: str = "backend"):
            self.project_root = Path(project_root)

        async def analyze_coverage(
            self,
            source_file: str,
            test_file: str
        ) -> Dict[str, Any]:
            \"\"\"Run coverage analysis and identify gaps.

            Returns:
                {
                    \"coverage_percent\": float,
                    \"covered_lines\": [int],
                    \"uncovered_lines\": [int],
                    \"missing_branches\": [str],
                    \"suggested_tests\": [
                        {\"line\": int, \"reason\": str, \"test_name\": str}
                    ]
                }
            \"\"\"

        def generate_coverage_target_tests(
            self,
            coverage_gaps: List[Dict[str, Any]]
        ) -> List[str]:
            \"\"\"Generate tests to fill coverage gaps.
            Creates tests for uncovered branches and lines.
            \"\"\"

        def estimate_coverage_from_tests(
            self,
            test_cases: List[Dict[str, Any]]
        ) -> float:
            \"\"\"Estimate coverage from test case count.

            Heuristic: Each test covers ~10-15 lines on average.
            Adjusts based on test complexity.
            \"\"\"

        async def run_coverage_report(
            self,
            test_files: List[str]
        ) -> Dict[str, Any]:
            \"\"\"Run pytest with coverage and parse results.

            Executes: pytest --cov=source --cov-report=json
            Returns parsed coverage.json content.
            \"\"\"

        def check_coverage_target_met(
            self,
            coverage_percent: float,
            target_type: str  # \"unit\" or \"integration\"
        ) -> bool:
            \"\"\"Check if coverage meets target.
            Unit: 85%, Integration: 70%
            \"\"\"
    ```

    Implementation requirements:
    - Use pytest-cov for coverage measurement
    - Parse coverage.json for detailed analysis
    - Identify uncovered branches
    - Generate targeted tests for gaps
    - Minimum 80 lines for coverage analysis

    Coverage targets:
    - Unit tests: 85% line coverage
    - Integration tests: 70% line coverage
    - Property tests: Count toward coverage
  </action>
  <verify>grep -n "class CoverageAnalyzer\|analyze_coverage\|generate_coverage_target_tests" backend/core/test_generator_service.py</verify>
  <done>CoverageAnalyzer with gap detection</done>
</task>

<task type="auto">
  <name>Task 6: Implement main TestGeneratorService orchestration</name>
  <files>backend/core/test_generator_service.py</files>
  <action>
    Create main TestGeneratorService class:

    ```python
    class TestGeneratorService:
        \"\"\"Main service for automated test generation.\"\"\"

        def __init__(
            self,
            db: Session,
            byok_handler: BYOKHandler
        ):
            self.db = db
            self.byok_handler = byok_handler
            self.structure_generator = TestFileStructureGenerator()
            self.parametrized_generator = ParametrizedTestGenerator()
            self.property_generator = PropertyBasedTestGenerator()
            self.fixture_generator = FixtureGenerator()
            self.coverage_analyzer = CoverageAnalyzer()

        async def generate_tests(
            self,
            source_files: List[str],
            context: Dict[str, Any]
        ) -> Dict[str, Any]:
            \"\"\"Generate complete test suite for source files.

            Args:
                source_files: List of source file paths
                context: Implementation context from coder agent

            Returns:
                {
                    \"test_files\": [
                        {\"path\": str, \"code\": str, \"estimated_coverage\": float}
                    ],
                    \"fixtures\": [{\"name\": str, \"code\": str}],
                    \"total_estimated_coverage\": float,
                    \"test_count\": int
                }
            \"\"\"

        async def generate_tests_for_task(
            self,
            task: ImplementationTask,
            generated_code: Dict[str, str]
        ) -> Dict[str, Any]:
            \"\"\"Generate tests for a single implementation task.

            Analyzes generated code and creates comprehensive tests.
            \"\"\"

        def generate_test_file_content(
            self,
            structure: Dict[str, Any],
            fixtures: List[str],
            test_cases: List[str]
        ) -> str:
            \"\"\"Assemble complete test file content.
            Combines imports, fixtures, test cases into file.
            \"\"\"

        async def generate_until_coverage_target(
            self,
            source_file: str,
            target_coverage: float = 0.85
        ) -> Dict[str, Any]:
            \"\"\"Generate tests iteratively until coverage target met.

            Loop:
            1. Generate initial tests
            2. Run coverage
            3. Identify gaps
            4. Generate targeted tests for gaps
            5. Repeat until target met or max iterations (5)

            Returns final test suite and coverage achieved.
            \"\"\"

        async def refine_tests_with_llm(
            self,
            generated_tests: str,
            source_code: str
        ) -> str:
            \"\"\"Use LLM to refine and improve generated tests.
            Ask LLM to review for edge cases and missing scenarios.
            \"\"\"
    ```

    Orchestration requirements:
    - Coordinate all test generation components
    - Generate tests iteratively until coverage target met
    - Support both unit and integration test generation
    - Create fixtures automatically
    - Use LLM for test refinement
    - Minimum 100 lines for orchestration

    Total file size target: 350+ lines
  </action>
  <verify>grep -n "class TestGeneratorService\|generate_tests\|generate_until_coverage_target" backend/core/test_generator_service.py && wc -l backend/core/test_generator_service.py | awk '{print $1 " lines (target: 350+)"}'</verify>
  <done>TestGeneratorService with full orchestration</done>
</task>

<task type="auto">
  <name>Task 7: Create comprehensive tests for TestGeneratorService</name>
  <files>backend/tests/test_test_generator_service.py</files>
  <action>
    Create test file with 200+ lines covering:

    Test cases:
    1. test_structure_generator_extract_functions - Function extraction
    2. test_structure_generator_extract_classes - Class extraction
    3. test_structure_generator_infer_test_cases - Test case inference
    4. test_parametrized_generator_simple - Basic parametrized test
    5. test_parametrized_generator_combinatorial - Multiple parameters
    6. test_parametrized_generator_idfn - Test ID generation
    7. test_property_generator_strategy_inference - Strategy from types
    8. test_property_generator_invariants - Invariant generation
    9. test_property_generator_complete_test - Complete property test
    10. test_fixture_generator_db - Database fixture generation
    11. test_fixture_generator_mock_oauth - Mock fixture generation
    12. test_fixture_generator_api_client - API client fixture
    13. test_coverage_analyzer_analyze - Coverage analysis
    14. test_coverage_analyzer_gap_detection - Gap identification
    15. test_coverage_analyzer_target_check - Target validation
    16. test_generator_end_to_end - Full test generation workflow
    17. test_generator_iterative_coverage - Coverage iteration
    18. test_generator_llm_refinement - LLM-based refinement
    19. test_generated_tests_pass - Generated tests are valid
    20. test_coverage_target_achievement - 85% target reached

    Fixtures:
    - db_session (SQLAlchemy session)
    - mock_byok_handler (Mock BYOKHandler)
    - sample_source_code (Sample Python code)
    - test_generator_service (TestGeneratorService instance)

    Test approach:
    - Generate tests for sample functions
    - Run generated tests via subprocess
    - Verify pytest discovers and runs them
    - Check coverage improves with each iteration
    - Mock LLM for refinement testing

    Coverage target: >= 80% for TestGeneratorService
  </action>
  <verify>pytest backend/tests/test_test_generator_service.py -v --cov=backend/core/test_generator_service --cov-report=term-missing</verify>
  <done>All tests passing with 80%+ coverage</done>
</task>

</tasks>

<verification>
1. Run tests: pytest backend/tests/test_test_generator_service.py -v
2. Verify coverage: pytest --cov=backend/core/test_generator_service --cov-report=html
3. Test structure generation: Analyze sample service and verify test structure
4. Test parametrized tests: Generate and verify parametrize decorator syntax
5. Test property-based tests: Generate and verify Hypothesis strategies
6. Test fixtures: Generate db_session fixture and verify rollback works
7. Test coverage iteration: Generate tests until 85% target reached
8. Verify generated tests run: Generate tests and run via pytest
9. Check coverage improvement: Compare before/after coverage
</verification>

<success_criteria>
1. Test file structure accurately extracts functions and classes
2. Parametrized tests generate valid @pytest.mark.parametrize decorators
3. Property-based tests use correct Hypothesis strategies
4. Fixtures provide proper setup/teardown for database/API
5. Coverage analyzer identifies uncovered lines and branches
6. Iterative generation reaches 85% coverage target
7. Generated tests are syntactically valid and run successfully
8. LLM refinement improves test quality
9. TestGeneratorService coordinates all components
10. Test coverage >= 80% for TestGeneratorService
11. All tests passing with no flaky tests
</success_criteria>

<output>
After completion, create `.planning/phases/69-autonomous-coding-agents/69-05-SUMMARY.md` with:
- Files created/modified
- Lines of code added
- Test coverage achieved
- Example generated tests
- Coverage improvement metrics
- Next steps (Plan 69-06 depends on generated tests)
</output>
