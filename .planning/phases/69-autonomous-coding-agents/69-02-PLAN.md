---
phase: 69-autonomous-coding-agents
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/core/codebase_research_service.py
  - backend/core/requirement_parser_service.py
  - backend/tests/test_codebase_research_service.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Codebase is analyzed using AST parsing for pattern detection"
    - "Embedding search finds similar features using LanceDB"
    - "Import graph analysis identifies dependencies"
    - "API catalog is generated from existing routes"
    - "Conflicts are detected before code generation"
    - "Research results include file paths, similarity scores, and integration points"
  artifacts:
    - path: "backend/core/codebase_research_service.py"
      provides: "Codebase analysis with AST and embeddings"
      min_lines: 400
      exports: ["CodebaseResearchService", "analyze_codebase", "find_similar_features", "detect_conflicts"]
    - path: "backend/tests/test_codebase_research_service.py"
      provides: "Test coverage for research service"
      min_lines: 250
  key_links:
    - from: "backend/core/codebase_research_service.py"
      to: "core/embedding_service.py"
      via: "EmbeddingService for code similarity search"
      pattern: "from core.embedding_service import EmbeddingService"
    - from: "backend/core/codebase_research_service.py"
      to: "core/lancedb_handler.py"
      via: "LanceDB for vector storage and search"
      pattern: "from core.lancedb_handler import LanceDBHandler"
    - from: "backend/core/codebase_research_service.py"
      to: "backend/core/requirement_parser_service.py"
      via: "Shared Pydantic models for requirements"
      pattern: "from core.requirement_parser_service import"
---

<objective>
Implement Codebase Researcher Service that analyzes the Atom codebase using AST parsing, embedding search, and import graph analysis to find similar features, identify integration points, and detect conflicts before code generation.

Purpose: Provide autonomous agents with deep understanding of existing codebase patterns to avoid duplication and ensure compatibility.
Output: CodebaseResearchService with AST parsing, embedding search, conflict detection, comprehensive tests
</objective>

<execution_context>
@/Users/rushiparikh/.claude/get-shit-done/workflows/execute-plan.md
@/Users/rushiparikh/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/69-autonomous-coding-agents/69-RESEARCH.md
@.planning/STATE.md
@backend/core/embedding_service.py
@backend/core/episode_retrieval_service.py
@backend/core/lancedb_handler.py
@backend/api/auth_routes.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement AST parsing utilities for code analysis</name>
  <files>backend/core/codebase_research_service.py</files>
  <action>
    Create AST parsing utilities in CodebaseResearchService:

    ```python
    import ast
    import os
    from typing import List, Dict, Any, Optional, Set
    from pathlib import Path

    class ASTParser:
        \"\"\"AST-based code analysis utilities.\"\"\"

        def __init__(self, codebase_root: str = "backend"):
            self.codebase_root = Path(codebase_root)

        def parse_file(self, file_path: str) -> Optional[ast.AST]:
            \"\"\"Parse Python file into AST.\"\"\"

        def extract_functions(self, file_path: str) -> List[Dict[str, Any]]:
            \"\"\"Extract all functions with signatures and docstrings.
            Returns: [{\"name\": str, \"args\": list, \"returns\": str, \"docstring\": str, \"lineno\": int}]
            \"\"\"

        def extract_classes(self, file_path: str) -> List[Dict[str, Any]]:
            \"\"\"Extract all classes with methods and bases.
            Returns: [{\"name\": str, \"bases\": list, \"methods\": list, \"lineno\": int}]
            \"\"\"

        def extract_imports(self, file_path: str) -> Dict[str, List[str]]:
            \"\"\"Extract import statements.
            Returns: {\"from_imports\": [], \"direct_imports\": []}
            \"\"\"

        def find_api_routes(self, file_path: str) -> List[Dict[str, Any]]:
            \"\"\"Extract FastAPI route definitions.
            Returns: [{\"path\": str, \"method\": str, \"function\": str}]
            \"\"\"

        def extract_dependencies(self, file_path: str) -> Set[str]:
            \"\"\"Extract module dependencies from imports.\"\"\"
    ```

    Implementation requirements:
    - Use Python's built-in ast module (no external dependencies)
    - Handle parse errors gracefully (return None or empty dict)
    - Support both relative and absolute imports
    - Extract decorators (@router.get, @app.post) for API routes
    - Minimum 150 lines for AST utilities including error handling and docstrings
  </action>
  <verify>grep -n "class ASTParser" backend/core/codebase_research_service.py && wc -l backend/core/codebase_research_service.py</verify>
  <done>ASTParser class with all required extraction methods</done>
</task>

<task type="auto">
  <name>Task 2: Implement embedding-based similarity search</name>
  <files>backend/core/codebase_research_service.py</files>
  <action>
    Add embedding search methods to CodebaseResearchService:

    ```python
    class CodebaseResearchService:
        def __init__(
            self,
            db: Session,
            embedding_service: EmbeddingService,
            lancedb_handler: Optional[LanceDBHandler] = None
        ):
            self.db = db
            self.embedding_service = embedding_service
            self.lancedb = lancedb_handler
            self.ast_parser = ASTParser()

        async def index_codebase(self, force_refresh: bool = False) -> dict:
            \"\"\"Index all Python files in codebase for similarity search.
            Walks backend/ directory, extracts functions/classes,
            generates embeddings, stores in LanceDB.

            Returns: {\"indexed\": int, \"errors\": int, \"duration_seconds\": float}
            \"\"\"

        async def find_similar_features(
            self,
            query: str,
            top_k: int = 5,
            min_similarity: float = 0.7
        ) -> List[Dict[str, Any]]:
            \"\"\"Find similar code features using embedding search.

            Args:
                query: Natural language description or code snippet
                top_k: Maximum number of results
                min_similarity: Minimum similarity threshold (0-1)

            Returns:
                [{\"file\": str, \"name\": str, \"type\": \"function|class\",
                  \"similarity\": float, \"preview\": str, \"line\": int}]
            \"\"\"

        def _generate_code_embedding(self, code: str, metadata: dict) -> List[float]:
            \"\"\"Generate embedding for code snippet with metadata.\"\"\"

        def _search_by_keywords(self, keywords: List[str]) -> List[Dict[str, Any]]:
            \"\"\"Fallback: search files by keyword matching.\"\"\"
    ```

    Integration requirements:
    - Reuse EmbeddingService from backend/core/embedding_service.py
    - Use LanceDBHandler for vector storage (from episodic memory)
    - Store embeddings in table: code_embeddings (id, vector, metadata)
    - Generate embeddings for function/class docstrings + signatures
    - Cache embeddings in memory for fast repeated searches
    - Minimum 150 lines for embedding search functionality

    Error handling:
    - Handle embedding service failures gracefully (fallback to keyword search)
    - Log missing or unparseable files
    - Return empty list on search errors (don't block workflow)
  </action>
  <verify>grep -n "find_similar_features\|index_codebase" backend/core/codebase_research_service.py</verify>
  <done>Embedding search with LanceDB integration and fallback to keyword search</done>
</task>

<task type="auto">
  <name>Task 3: Implement import graph analysis</name>
  <files>backend/core/codebase_research_service.py</files>
  <action>
    Add import graph analysis methods:

    ```python
    class ImportGraphAnalyzer:
        \"\"\"Analyze import dependencies between modules.\"\"\"

        def __init__(self, codebase_root: str = "backend"):
            self.codebase_root = Path(codebase_root)
            self.graph: Dict[str, Set[str]] = {}

        def build_graph(self) -> Dict[str, Set[str]]:
            \"\"\"Build dependency graph: {module: [dependencies]}.
            Returns adjacency list of module dependencies.
            \"\"\"

        def find_dependents(self, module_name: str) -> List[str]:
            \"\"\"Find all modules that import the given module.
            Reverse dependency lookup.
            \"\"\"

        def detect_cycles(self) -> List[List[str]]:
            \"\"\"Detect circular import dependencies.
            Returns list of cycles (each cycle is list of modules).
            \"\"\"

        def get_execution_order(self, changed_modules: Set[str]) -> List[str]:
            \"\"\"Get topological order for testing affected modules.
            Returns modules in dependency order (leaves first).
            \"\"\"

        def estimate_impact(
            self,
            files_to_modify: List[str]
        ) -> Dict[str, Any]:
            \"\"\"Estimate impact of file modifications.
            Returns: {
                \"directly_affected\": [...],
                \"indirectly_affected\": [...],
                \"risk_level\": \"low|medium|high\"
            }
            \"\"\"
    ```

    Implementation requirements:
    - Use networkx if available (from skill composition), otherwise simple dict-based graph
    - Parse import statements from AST (not regex)
    - Handle both local imports (from core.xxx) and external imports
    - Detect circular dependencies using DFS cycle detection
    - Minimum 120 lines for import graph functionality

    Integration:
    - Used by planning agent to identify files needing updates
    - Used by conflict detector to find breaking changes
  </action>
  <verify>grep -n "class ImportGraphAnalyzer\|build_graph\|detect_cycles" backend/core/codebase_research_service.py</verify>
  <done>ImportGraphAnalyzer with cycle detection and impact estimation</done>
</task>

<task type="auto">
  <name>Task 4: Implement API catalog generator</name>
  <files>backend/core/codebase_research_service.py</files>
  <action>
    Add API catalog generation:

    ```python
    class APICatalogGenerator:
        \"\"\"Generate catalog of existing API endpoints.\"\"\"

        def __init__(self, codebase_root: str = "backend"):
            self.codebase_root = Path(codebase_root)
            self.ast_parser = ASTParser(codebase_root)

        def generate_catalog(self) -> Dict[str, Any]:
            \"\"\"Scan all API route files and generate endpoint catalog.

            Returns:
                {
                    \"endpoints\": [
                        {
                            \"path\": \"/api/auth/login\",
                            \"method\": \"POST\",
                            \"file\": \"backend/api/auth_routes.py\",
                            \"function\": \"login\",
                            \"auth_required\": true,
                            \"governance\": \"INTERN+\"
                        }
                    ],
                    \"routers\": [\"auth\", \"users\", \"workflows\", ...],
                    \"namespaces\": {\"/api/auth\": 5, \"/api/workflows\": 12}
                }
            \"\"\"

        def find_available_namespaces(self) -> List[str]:
            \"\"\"Find available API namespaces for new routes.
            Returns namespaces with <10 routes (opportunity for extension).
            \"\"\"

        def detect_route_conflicts(
            self,
            proposed_path: str,
            proposed_method: str
        ) -> Optional[Dict[str, Any]]:
            \"\"\"Check if proposed route conflicts with existing route.
            Returns conflict details or None if no conflict.
            \"\"\"
    ```

    Implementation requirements:
    - Scan backend/api/*.py for route definitions
    - Extract @router.get/post/put/delete decorators
    - Parse function signatures for request/response types
    - Identify auth requirements from function decorators
    - Minimum 100 lines for API catalog functionality

    Integration:
    - Used by planner to identify integration points
    - Used by conflict detector to prevent route collisions
  </action>
  <verify>grep -n "class APICatalogGenerator\|generate_catalog" backend/core/codebase_research_service.py</verify>
  <done>APICatalogGenerator with endpoint scanning and conflict detection</done>
</task>

<task type="auto">
  <name>Task 5: Implement conflict detection system</name>
  <files>backend/core/codebase_research_service.py</files>
  <action>
    Add conflict detection methods:

    ```python
    class ConflictDetector:
        \"\"\"Detect potential conflicts before code generation.\"\"\"

        def __init__(
            self,
            ast_parser: ASTParser,
            import_analyzer: ImportGraphAnalyzer,
            api_catalog: APICatalogGenerator
        ):
            self.ast_parser = ast_parser
            self.import_analyzer = import_analyzer
            self.api_catalog = api_catalog

        async def detect_conflicts(
            self,
            requirements: Dict[str, Any]
        ) -> Dict[str, Any]:
            \"\"\"Analyze requirements and detect all potential conflicts.

            Returns:
                {
                    \"conflicts\": [
                        {
                            \"type\": \"duplicate_feature|route_conflict|breaking_change\",
                            \"severity\": \"critical|high|medium|low\",
                            \"description\": str,
                            \"existing_file\": str,
                            \"suggestion\": str
                        }
                    ],
                    \"warnings\": [...],
                    \"safe_to_proceed\": bool
                }
            \"\"\"

        def detect_duplicate_features(
            self,
            feature_description: str
        ) -> List[Dict[str, Any]]:
            \"\"\"Check if feature already exists using similarity search.
            Returns duplicates with >0.8 similarity.
            \"\"\"

        def detect_breaking_changes(
            self,
            files_to_modify: List[Dict[str, str]]
        ) -> List[Dict[str, Any]]:
            \"\"\"Analyze proposed changes for breaking changes.
            Checks: removed APIs, renamed models, deleted functions.
            \"\"\"

        def detect_model_conflicts(
            self,
            proposed_models: List[Dict[str, Any]]
        ) -> List[Dict[str, Any]]:
            \"\"\"Check for SQLAlchemy model conflicts.
            Checks: duplicate table names, conflicting foreign keys.
            \"\"\"
    ```

    Implementation requirements:
    - Use similarity search for duplicate detection
    - Parse existing models for conflict checking
    - Check API catalog for route conflicts
    - Severity levels: critical (blocks implementation), high (requires review), medium (warn), low (info)
    - Minimum 120 lines for conflict detection

    Integration:
    - Called by researcher after codebase analysis
    - Results used by planner to adjust implementation plan
  </action>
  <verify>grep -n "class ConflictDetector\|detect_conflicts" backend/core/codebase_research_service.py</verify>
  <done>ConflictDetector with duplicate, breaking change, and model conflict detection</done>
</task>

<task type="auto">
  <name>Task 6: Implement main CodebaseResearchService orchestration</name>
  <files>backend/core/codebase_research_service.py</files>
  <action>
    Add main service class that orchestrates all research components:

    ```python
    class CodebaseResearchService:
        \"\"\"Main service for codebase research and analysis.\"\"\"

        def __init__(self, db: Session):
            self.db = db
            self.embedding_service = get_embedding_service()
            self.lancedb = get_lancedb_handler()
            self.ast_parser = ASTParser()
            self.import_analyzer = ImportGraphAnalyzer()
            self.api_catalog = APICatalogGenerator()
            self.conflict_detector = ConflictDetector(
                self.ast_parser,
                self.import_analyzer,
                self.api_catalog
            )

        async def analyze_codebase(
            self,
            requirements: Dict[str, Any],
            workspace_id: str
        ) -> Dict[str, Any]:
            \"\"\"Perform comprehensive codebase analysis.

            Args:
                requirements: Parsed requirements from RequirementParserService
                workspace_id: Workspace context

            Returns:
                {
                    \"similar_features\": [...],
                    \"integration_points\": [
                        {\"file\": str, \"reason\": str, \"type\": \"api|model|service\"}
                    ],
                    \"conflicts\": [...],
                    \"import_graph\": {...},
                    \"api_catalog\": {...},
                    \"recommendations\": [
                        \"Reuse SlackOAuth pattern for Google/GitHub\",
                        \"Use existing session management\",
                        \"Follow Atom governance patterns\"
                    ]
                }
            \"\"\"

        async def find_similar_features(
            self,
            query: str,
            top_k: int = 5
        ) -> List[Dict[str, Any]]:
            \"\"\"Find similar existing features.\"\"\"

        def get_integration_points(
            self,
            user_stories: List[Dict[str, Any]]
        ) -> List[Dict[str, Any]]:
            \"\"\"Identify files that need modification for integration.\"\"\"

        def generate_recommendations(
            self,
            similar_features: List[Dict],
            conflicts: List[Dict],
            integration_points: List[Dict]
        ) -> List[str]:
            \"\"\"Generate implementation recommendations based on analysis.\"\"\"
    ```

    Orchestration requirements:
    - Coordinate all sub-components (AST, embeddings, import graph, API catalog)
    - Combine results into comprehensive analysis
    - Generate actionable recommendations
    - Cache results for repeated queries
    - Minimum 80 lines for orchestration layer

    Total file size target: 400+ lines including all components
  </action>
  <verify>grep -n "class CodebaseResearchService\|analyze_codebase" backend/core/codebase_research_service.py && wc -l backend/core/codebase_research_service.py | awk '{print $1 " lines (target: 400+)"}'</verify>
  <done>CodebaseResearchService with full orchestration and 400+ lines</done>
</task>

<task type="auto">
  <name>Task 7: Create comprehensive tests for CodebaseResearchService</name>
  <files>backend/tests/test_codebase_research_service.py</files>
  <action>
    Create test file with 250+ lines covering:

    Test cases:
    1. test_ast_parser_extract_functions - Function extraction with signatures
    2. test_ast_parser_extract_classes - Class extraction with methods
    3. test_ast_parser_extract_imports - Import statement parsing
    4. test_ast_parser_find_api_routes - FastAPI route detection
    5. test_find_similar_features - Embedding search functionality
    6. test_index_codebase - Codebase indexing for search
    7. test_import_graph_build - Dependency graph construction
    8. test_import_graph_detect_cycles - Circular dependency detection
    9. test_import_graph_impact_estimate - Impact estimation for changes
    10. test_api_catalog_generate - Endpoint catalog generation
    11. test_api_catalog_namespace_detection - Available namespace finding
    12. test_conflict_detector_duplicates - Duplicate feature detection
    13. test_conflict_detector_breaking_changes - Breaking change detection
    14. test_conflict_detector_model_conflicts - Model conflict detection
    15. test_analyze_codebase_integration - Full analysis orchestration
    16. test_get_integration_points - Integration point identification
    17. test_generate_recommendations - Recommendation generation
    18. test_embedding_search_fallback - Fallback to keyword search
    19. test_error_handling_invalid_file - Invalid file handling
    20. test_cache_behavior - Result caching verification

    Fixtures:
    - db_session (SQLAlchemy session)
    - mock_embedding_service (Mock embeddings)
    - mock_lancedb (Mock LanceDB handler)
    - sample_codebase (Temporary directory with sample Python files)
    - research_service (CodebaseResearchService instance)

    Test approach:
    - Create temporary Python files for AST testing
    - Mock embedding responses with known vectors
    - Use real AST parsing (not mocked) for accuracy
    - Test with actual Atom codebase files when available
    - Parametrize tests for multiple file types

    Coverage target: >= 80% for CodebaseResearchService
    Follow Atom test patterns with clear organization.
  </action>
  <verify>pytest backend/tests/test_codebase_research_service.py -v --cov=backend/core/codebase_research_service --cov-report=term-missing</verify>
  <done>All tests passing with 80%+ coverage</done>
</task>

<task type="auto">
  <name>Task 8: Create code indexing CLI command for maintenance</name>
  <files>backend/cli/autonomous_coding_cli.py</files>
  <action>
    Create CLI module for maintenance operations:

    ```python
    # backend/cli/autonomous_coding_cli.py
    import asyncio
    import click
    from pathlib import Path

    @click.group()
    def autonomous():
        \"\"\"Autonomous coding maintenance commands.\"\"\"

    @autonomous.command()
    @click.option('--force', is_flag=True, help='Force full reindex')
    @click.option('--codebase', default='backend', help='Codebase root')
    def index_codebase(force, codebase):
        \"\"\"Index codebase for similarity search.
        Walks all Python files, generates embeddings, stores in LanceDB.
        \"\"\"
        async def _index():
            from core.codebase_research_service import CodebaseResearchService
            from core.database import SessionLocal

            db = SessionLocal()
            service = CodebaseResearchService(db)
            result = await service.index_codebase(force_refresh=force)
            click.echo(f\"Indexed {result['indexed']} files ({result['errors']} errors) in {result['duration_seconds']:.2f}s\")

        asyncio.run(_index())

    @autonomous.command()
    def analyze_imports():
        \"\"\"Analyze import dependencies and detect cycles.\"\"\"
        # Implementation for import analysis
    ```

    Integration:
    - Register with main CLI if exists (backend/cli/main.py)
    - Use click for CLI interface (consistent with Atom patterns)
    - Provide helpful commands for maintenance and debugging

    Minimum 80 lines including all commands and help text.
  </action>
  <verify>grep -n "index_codebase\|autonomous" backend/cli/autonomous_coding_cli.py 2>/dev/null || echo "CLI file created"</verify>
  <done>CLI commands for codebase indexing and maintenance</done>
</task>

</tasks>

<verification>
1. Run tests: pytest backend/tests/test_codebase_research_service.py -v
2. Verify coverage: pytest --cov=backend/core/codebase_research_service --cov-report=html
3. Test codebase indexing: python -m backend.cli.autonomous_coding_cli index-codebase
4. Verify AST parsing: Test with real Atom files (auth_routes.py, models.py)
5. Test similarity search: Query for "OAuth" feature and verify results
6. Check conflict detection: Create conflicting route and verify detection
7. Verify import graph: Build graph and check for cycles
</verification>

<success_criteria>
1. AST parsing extracts functions, classes, imports, and API routes accurately
2. Embedding search finds similar features with similarity scores
3. Import graph detects circular dependencies
4. API catalog generates complete endpoint listing
5. Conflict detection identifies duplicates, breaking changes, and model conflicts
6. Research service orchestrates all components and returns comprehensive analysis
7. CLI commands work for maintenance operations
8. Test coverage >= 80% for CodebaseResearchService
9. All tests passing with no flaky tests
</success_criteria>

<output>
After completion, create `.planning/phases/69-autonomous-coding-agents/69-02-SUMMARY.md` with:
- Files created/modified
- Lines of code added
- Test coverage achieved
- Codebase indexing statistics (files indexed, embeddings generated)
- Next steps (Plan 69-03 depends on this plan for integration points)
</output>
