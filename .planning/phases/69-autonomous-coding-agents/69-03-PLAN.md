---
phase: 69-autonomous-coding-agents
plan: 03
type: execute
wave: 2
depends_on: [69-01, 69-02]
files_modified:
  - backend/core/autonomous_planning_agent.py
  - backend/core/models.py
  - backend/tests/test_autonomous_planning_agent.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Features are decomposed into hierarchical task networks"
    - "DAG validation ensures no circular dependencies"
    - "Parallelization opportunities are identified for efficiency"
    - "Time and complexity estimates are accurate"
    - "File modification lists are generated"
    - "Test requirements are specified"
    - "Integration with NetworkX for DAG operations"
  artifacts:
    - path: "backend/core/autonomous_planning_agent.py"
      provides: "Implementation planning with DAG validation"
      min_lines: 350
      exports: ["PlanningAgent", "create_implementation_plan", "validate_dag", "estimate_complexity"]
    - path: "backend/tests/test_autonomous_planning_agent.py"
      provides: "Test coverage for planning agent"
      min_lines: 200
  key_links:
    - from: "backend/core/autonomous_planning_agent.py"
      to: "core/skill_composition_service.py"
      via: "Reuse DAG validation from skill composition"
      pattern: "from core.skill_composition_service import SkillCompositionEngine"
    - from: "backend/core/autonomous_planning_agent.py"
      to: "core/requirement_parser_service.py"
      via: "Parsed requirements as input"
      pattern: "RequirementParserService"
    - from: "backend/core/autonomous_planning_agent.py"
      to: "core/codebase_research_service.py"
      via: "Codebase context for planning"
      pattern: "CodebaseResearchService"
---

<objective>
Implement Implementation Planner Agent that breaks down feature requests into executable task sequences using Hierarchical Task Networks (HTN), validates dependencies with DAG analysis, identifies parallelization opportunities, and generates file modification lists.

Purpose: Transform high-level requirements into actionable implementation plans with validated dependencies and accurate time estimates.
Output: PlanningAgent with NetworkX DAG validation, HTN decomposition, parallelization analysis, comprehensive tests
</objective>

<execution_context>
@/Users/rushiparikh/.claude/get-shit-done/workflows/execute-plan.md
@/Users/rushiparikh/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/69-autonomous-coding-agents/69-RESEARCH.md
@.planning/STATE.md
@backend/core/skill_composition_service.py
@backend/core/workflow_engine.py
@backend/core/requirement_parser_service.py
@backend/core/codebase_research_service.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement Hierarchical Task Network decomposition</name>
  <files>backend/core/autonomous_planning_agent.py</files>
  <action>
    Create HTN decomposition methods in PlanningAgent:

    ```python
    from typing import List, Dict, Any, Optional, Set
    from dataclasses import dataclass
    from enum import Enum
    import networkx as nx

    class TaskComplexity(str, Enum):
        SIMPLE = "simple"        # <1 hour
        MODERATE = "moderate"    # 1-4 hours
        COMPLEX = "complex"      # 4-8 hours
        ADVANCED = "advanced"    # 1-2 days

    class AgentType(str, Enum):
        CODER_BACKEND = "coder-backend"
        CODER_FRONTEND = "coder-frontend"
        CODER_DATABASE = "coder-database"
        TESTER = "tester"
        DOCUMENTER = "documenter"

    @dataclass
    class ImplementationTask:
        \"\"\"Single task in implementation plan.\"\"\"
        id: str
        name: str
        agent_type: AgentType
        description: str
        dependencies: List[str]
        files_to_create: List[str]
        files_to_modify: List[str]
        estimated_time_minutes: int
        complexity: TaskComplexity
        can_parallelize: bool

    class HTNDecomposer:
        \"\"\"Hierarchical Task Network decomposition for feature planning.\"\"\"

        def decompose_feature(
            self,
            user_stories: List[Dict[str, Any]],
            research_context: Dict[str, Any]
        ) -> List[ImplementationTask]:
            \"\"\"Decompose feature into hierarchical task network.

            Args:
                user_stories: Parsed user stories from RequirementParserService
                research_context: Codebase analysis from CodebaseResearchService

            Returns:
                List of ImplementationTask in dependency order
            \"\"\"

        def _identify_task_types(
            self,
            user_stories: List[Dict[str, Any]]
        ) -> List[str]:
            \"\"\"Identify high-level task types (database, backend, frontend, testing).
            Returns: ['database_schema', 'backend_api', 'frontend_ui', 'testing']
            \"\"\"

        def _decompose_database_tasks(
            self,
            stories: List[Dict[str, Any]]
        ) -> List[ImplementationTask]:
            \"\"\"Generate database-related tasks (models, migrations).
            Examples: 'Create User model', 'Create OAuthSession model', 'Run migration'
            \"\"\"

        def _decompose_backend_tasks(
            self,
            stories: List[Dict[str, Any]],
            context: Dict[str, Any]
        ) -> List[ImplementationTask]:
            \"\"\"Generate backend tasks (services, API routes).
            Examples: 'Implement OAuthService', 'Create auth routes'
            \"\"\"

        def _decompose_frontend_tasks(
            self,
            stories: List[Dict[str, Any]]
        ) -> List[ImplementationTask]:
            \"\"\"Generate frontend tasks (UI components, state management).
            Examples: 'Create login buttons', 'Add auth context'
            \"\"\"

        def _decompose_testing_tasks(
            self,
            all_tasks: List[ImplementationTask]
        ) -> List[ImplementationTask]:
            \"\"\"Generate testing tasks based on implementation tasks.
            Examples: 'Unit tests for OAuthService', 'Integration tests for API'
            \"\"\"
    ```

    Implementation requirements:
    - Analyze user stories to identify feature components
    - Generate tasks with clear descriptions and file paths
    - Set default dependencies (database before backend, backend before frontend)
    - Estimate complexity based on story complexity and dependencies
    - Use research context to find similar features for time estimation
    - Minimum 120 lines for HTN decomposition

    Task naming convention:
    - Prefix with number: "task-001", "task-002", etc.
    - Descriptive names: "Create User model", "Implement Google OAuth flow"
  </action>
  <verify>grep -n "class HTNDecomposer\|decompose_feature" backend/core/autonomous_planning_agent.py</verify>
  <done>HTNDecomposer with feature decomposition into tasks</done>
</task>

<task type="auto">
  <name>Task 2: Implement DAG validation with NetworkX</name>
  <files>backend/core/autonomous_planning_agent.py</files>
  <action>
    Add DAG validation methods:

    ```python
    class DAGValidator:
        \"\"\"Validate task dependencies using NetworkX.\"\"\"

        def __init__(self):
            self.graph = nx.DiGraph()

        def build_dag(
            self,
            tasks: List[ImplementationTask]
        ) -> nx.DiGraph:
            \"\"\"Build directed acyclic graph from tasks.
            Nodes: task IDs
            Edges: dependencies (task_A depends on task_B = edge B->A)

            Returns NetworkX DiGraph for analysis.
            \"\"\"

        def validate_dag(
            self,
            dag: nx.DiGraph
        ) -> Dict[str, Any]:
            \"\"\"Validate DAG structure.

            Returns:
                {
                    \"is_valid\": bool,
                    \"has_cycles\": bool,
                    \"cycles\": [[...]],  # List of cycles if found
                    \"execution_order\": [...],  # Topological sort
                    \"parallelization_opportunities\": int
                }
            \"\"\"

        def detect_cycles(
            self,
            dag: nx.DiGraph
        ) -> List[List[str]]:
            \"\"\"Detect circular dependencies using NetworkX.
            Returns list of cycles (each cycle is list of task IDs).
            \"\"\"

        def get_execution_order(
            self,
            dag: nx.DiGraph
        ) -> List[str]:
            \"\"\"Get topological sort for execution order.
            Returns tasks in order they should execute (dependencies first).
            \"\"\"

        def identify_parallelization(
            self,
            dag: nx.DiGraph,
            execution_order: List[str]
        ) -> List[List[str]]:
            \"\"\"Identify tasks that can run in parallel.

            Returns list of waves:
            [[task1, task2], [task3], [task4, task5, task6]]
            Each sub-list is a wave of parallelizable tasks.
            \"\"\"

        def calculate_critical_path(
            self,
            dag: nx.DiGraph,
            tasks: Dict[str, ImplementationTask]
        ) -> Dict[str, Any]:
            \"\"\"Calculate critical path (longest path through DAG).
            Returns:
                {
                    \"critical_path\": [...],  # Task IDs on critical path
                    \"total_time_minutes\": int,
                    \"bottleneck_tasks\": [...]
                }
            \"\"\"
    ```

    Implementation requirements:
    - Use NetworkX for graph operations (already in Atom for skill composition)
    - Detect cycles and raise error if found (block invalid plans)
    - Generate topological sort for execution order
    - Identify waves of parallelizable tasks
    - Calculate critical path for time estimation
    - Minimum 100 lines for DAG validation

    Integration with SkillCompositionEngine:
    - Reuse DAG validation patterns from skill composition
    - Follow similar graph construction approach
  </action>
  <verify>grep -n "class DAGValidator\|validate_dag\|detect_cycles" backend/core/autonomous_planning_agent.py</verify>
  <done>DAGValidator with NetworkX integration and cycle detection</done>
</task>

<task type="auto">
  <name>Task 3: Implement complexity and time estimation</name>
  <files>backend/core/autonomous_planning_agent.py</files>
  <action>
    Add estimation methods:

    ```python
    class ComplexityEstimator:
        \"\"\"Estimate task complexity and duration.\"\"\"

        def __init__(self, research_context: Dict[str, Any]):
            self.research_context = research_context
            # Historical data: {feature_type: avg_time_minutes}
            self.historical_data = {
                \"database_model\": 30,
                \"api_route\": 45,
                \"service_layer\": 90,
                \"frontend_component\": 60,
                \"unit_tests\": 45,
                \"integration_tests\": 60
            }

        def estimate_complexity(
            self,
            task: ImplementationTask
        ) -> TaskComplexity:
            \"\"\"Estimate task complexity based on multiple factors.

            Factors:
            - Lines of code estimate
            - Number of integration points
            - Number of dependencies
            - Similar feature complexity (from research)

            Returns TaskComplexity enum value.
            \"\"\"

        def estimate_time(
            self,
            task: ImplementationTask,
            complexity: TaskComplexity
        ) -> int:
            \"\"\"Estimate task time in minutes.

            Base times from historical data adjusted by:
            - Complexity multiplier (1x, 2x, 4x, 8x)
            - Integration point count (+10 min per integration)
            - Dependency depth (+5 min per dependency level)

            Returns time in minutes.
            \"\"\"

        def calculate_complexity_score(
            self,
            user_stories: List[Dict[str, Any]],
            integration_points: List[Dict[str, Any]]
        ) -> float:
            \"\"\"Calculate overall complexity score.

            Formula from research:
            complexity_score = (
                lines_of_code * 0.3 +
                integration_points * 0.4 +
                test_count * 0.2 +
                dependency_depth * 0.1
            )

            Returns score used for complexity classification.
            \"\"\"

        def map_score_to_complexity(
            self,
            score: float
        ) -> TaskComplexity:
            \"\"\"Map numerical score to complexity category.
            <50: simple
            50-150: moderate
            150-300: complex
            >300: advanced
            \"\"\"

        def estimate_from_similar_features(
            self,
            task_description: str
        ) -> Optional[int]:
            \"\"\"Find similar features and use their actual time.
            Queries research context for similar implemented features.
            Returns average time of similar features or None.
            \"\"\"
    ```

    Implementation requirements:
    - Use formula from research document
    - Adjust estimates based on historical data
    - Consider integration points from research context
    - Minimum 100 lines for estimation logic

    Estimation accuracy targets:
    - Simple: +/- 15 minutes
    - Moderate: +/- 30 minutes
    - Complex: +/- 1 hour
    - Advanced: +/- 2 hours
  </action>
  <verify>grep -n "class ComplexityEstimator\|estimate_complexity\|estimate_time" backend/core/autonomous_planning_agent.py</verify>
  <done>ComplexityEstimator with multi-factor estimation</done>
</task>

<task type="auto">
  <name>Task 4: Implement file modification list generator</name>
  <files>backend/core/autonomous_planning_agent.py</files>
  <action>
    Add file list generation methods:

    ```python
    class FileListGenerator:
        \"\"\"Generate file modification lists for implementation.\"\"\"

        def __init__(self, codebase_root: str = "backend"):
            self.codebase_root = Path(codebase_root)

        def generate_file_lists(
            self,
            tasks: List[ImplementationTask],
            research_context: Dict[str, Any]
        ) -> Dict[str, List[str]]:
            \"\"\"Generate complete file modification lists.

            Returns:
                {
                    \"files_to_create\": [
                        \"backend/core/oauth_service.py\",
                        \"backend/api/oauth_routes.py\",
                        \"backend/tests/test_oauth_service.py\",
                        \"alembic/versions/xxx_add_oauth.py\"
                    ],
                    \"files_to_modify\": [
                        \"backend/core/models.py\",  # Add User model
                        \"backend/main.py\"          # Register routes
                    ],
                    \"files_to_delete\": []
                }
            \"\"\"

        def predict_files_to_create(
            self,
            task: ImplementationTask,
            user_stories: List[Dict[str, Any]]
        ) -> List[str]:
            \"\"\"Predict files that need to be created for a task.
            Based on task type and patterns from similar features.
            \"\"\"

        def predict_files_to_modify(
            self,
            task: ImplementationTask,
            integration_points: List[Dict[str, Any]]
        ) -> List[str]:
            \"\"\"Predict files that need to be modified.
            Based on integration points from research.
            \"\"\"

        def check_file_conflicts(
            self,
            file_lists: Dict[str, List[str]]
        ) -> List[Dict[str, Any]]:
            \"\"\"Check for file modification conflicts.
            Returns conflicts where multiple tasks modify the same file.
            \"\"\"

        def validate_file_existence(
            self,
            file_path: str
        ) -> bool:
            \"\"\"Check if file exists before suggesting modification.
            Returns True if file exists, False otherwise.
            \"\"\"

        def suggest_file_structure(
            self,
            feature_name: str,
            feature_type: str
        ) -> Dict[str, str]:
            \"\"\"Suggest file structure following Atom conventions.
            Returns {\"service\": \"core/xxx_service.py\", \"routes\": \"api/xxx_routes.py\"}
            \"\"\"
    ```

    Implementation requirements:
    - Follow Atom file naming conventions
    - Suggest paths based on feature type (service, routes, tests)
    - Check file existence before including in modification list
    - Detect conflicts (multiple tasks modifying same file)
    - Minimum 80 lines for file list generation

    File path patterns:
    - Services: backend/core/{feature}_service.py
    - Routes: backend/api/{feature}_routes.py
    - Models: backend/core/models.py (modify, don't create separate)
    - Tests: backend/tests/test_{feature}.py
    - Migrations: alembic/versions/{timestamp}_{description}.py
  </action>
  <verify>grep -n "class FileListGenerator\|generate_file_lists" backend/core/autonomous_planning_agent.py</verify>
  <done>FileListGenerator with conflict detection and Atom conventions</done>
</task>

<task type="auto">
  <name>Task 5: Implement test requirements generator</name>
  <files>backend/core/autonomous_planning_agent.py</files>
  <action>
    Add test requirements methods:

    ```python
    class TestRequirementsGenerator:
        \"\"\"Generate test requirements for implementation plan.\"\"\"

        def __init__(self):
            self.coverage_targets = {
                \"unit\": 0.85,      # 85% for unit tests
                \"integration\": 0.70,  # 70% for integration
                \"e2e\": 0.60        # 60% for E2E
            }

        def generate_test_requirements(
            self,
            tasks: List[ImplementationTask],
            user_stories: List[Dict[str, Any]]
        ) -> Dict[str, Any]:
            \"\"\"Generate test requirements for all tasks.

            Returns:
                {
                    \"test_files\": [
                        {
                            \"path\": \"tests/test_oauth_service.py\",
                            \"type\": \"unit\",
                            \"test_cases\": [
                                \"test_google_oauth_redirect\",
                                \"test_callback_success\",
                                \"test_callback_failure\"
                            ],
                            \"coverage_target\": 0.85
                        }
                    ],
                    \"overall_coverage_target\": 0.80,
                    \"test_types\": {\"unit\": 10, \"integration\": 2, \"e2e\": 0}
                }
            \"\"\"

        def generate_test_cases_for_task(
            self,
            task: ImplementationTask
        ) -> List[str]:
            \"\"\"Generate test case names based on task description.
            Uses template patterns for common task types.
            \"\"\"

        def suggest_test_type(
            self,
            task: ImplementationTask
        ) -> str:
            \"\"\"Suggest test type (unit, integration, e2e) based on task.
            - Service methods: unit
            - API endpoints: integration
            - Full workflows: e2e
            \"\"\"

        def estimate_test_count(
            self,
            task: ImplementationTask
        ) -> int:
            \"\"\"Estimate number of tests needed for task.
            Based on complexity and number of methods/endpoints.
            \"\"\"

        def generate_acceptance_test_cases(
            self,
            acceptance_criteria: List[str]
        ) -> List[Dict[str, str]]:
            \"\"\"Convert Gherkin acceptance criteria to test cases.
            Each Given/When/Then becomes a test method.
            \"\"\"
    ```

    Implementation requirements:
    - Generate test file paths following Atom conventions
    - Suggest test case names based on task
    - Set coverage targets (85% unit, 70% integration, 60% e2e)
    - Convert Gherkin criteria to test cases
    - Minimum 80 lines for test requirements

    Integration with Phase 69-05 (Test Generator):
    - Test requirements feed into test generator
    - Coverage targets guide test generation
  </action>
  <verify>grep -n "class TestRequirementsGenerator\|generate_test_requirements" backend/core/autonomous_planning_agent.py</verify>
  <done>TestRequirementsGenerator with coverage targets</done>
</task>

<task type="auto">
  <name>Task 6: Implement main PlanningAgent orchestration</name>
  <files>backend/core/autonomous_planning_agent.py</files>
  <action>
    Create main PlanningAgent class:

    ```python
    class PlanningAgent:
        \"\"\"Main planning agent for implementation planning.\"\"\"

        def __init__(
            self,
            db: Session,
            byok_handler: BYOKHandler
        ):
            self.db = db
            self.byok_handler = byok_handler
            self.htn_decomposer = HTNDecomposer()
            self.dag_validator = DAGValidator()
            self.complexity_estimator = ComplexityEstimator({})
            self.file_list_generator = FileListGenerator()
            self.test_generator = TestRequirementsGenerator()

        async def create_implementation_plan(
            self,
            requirements: Dict[str, Any],
            research_context: Dict[str, Any]
        ) -> Dict[str, Any]:
            \"\"\"Create complete implementation plan.

            Args:
                requirements: Parsed requirements from RequirementParserService
                research_context: Codebase analysis from CodebaseResearchService

            Returns:
                {
                    \"tasks\": [...],  # List of ImplementationTask
                    \"dag_valid\": bool,
                    \"execution_order\": [...],  # Topological sort
                    \"waves\": [[...]],  # Parallelization waves
                    \"files_to_create\": [...],
                    \"files_to_modify\": [...],
                    \"test_requirements\": {...},
                    \"estimated_duration_minutes\": int,
                    \"complexity\": \"moderate\",
                    \"parallelization_opportunities\": int
                }
            \"\"\"

        async def refine_plan_with_llm(
            self,
            initial_plan: Dict[str, Any]
        ) -> Dict[str, Any]:
            \"\"\"Use LLM to refine and validate the plan.
            Ask LLM to review for missing tasks, dependencies, or risks.
            \"\"\"

        def save_plan_to_workflow(
            self,
            workflow_id: str,
            plan: Dict[str, Any]
        ) -> None:
            \"\"\"Save implementation plan to AutonomousWorkflow record.\"\"\"

        def get_plan_summary(
            self,
            plan: Dict[str, Any]
        ) -> str:
            \"\"\"Generate human-readable plan summary.
            Returns formatted text with task list, estimates, warnings.
            \"\"\"
    ```

    Orchestration requirements:
    - Coordinate all sub-components
    - Call LLM for plan refinement (optional)
    - Save plan to database for tracking
    - Generate summary for human review
    - Minimum 80 lines for orchestration

    Total file size target: 350+ lines
  </action>
  <verify>grep -n "class PlanningAgent\|create_implementation_plan" backend/core/autonomous_planning_agent.py && wc -l backend/core/autonomous_planning_agent.py | awk '{print $1 " lines (target: 350+)"}'</verify>
  <done>PlanningAgent with full orchestration and 350+ lines</done>
</task>

<task type="auto">
  <name>Task 7: Create comprehensive tests for PlanningAgent</name>
  <files>backend/tests/test_autonomous_planning_agent.py</files>
  <action>
    Create test file with 200+ lines covering:

    Test cases:
    1. test_htn_decompose_simple_feature - Simple feature decomposition
    2. test_htn_decompose_complex_feature - Complex feature with multiple components
    3. test_dag_build - DAG construction from tasks
    4. test_dag_validate_no_cycles - Valid DAG validation
    5. test_dag_validate_with_cycles - Cycle detection
    6. test_dag_execution_order - Topological sort
    7. test_dag_parallelization - Parallelization wave identification
    8. test_dag_critical_path - Critical path calculation
    9. test_complexity_estimator_simple - Simple complexity estimation
    10. test_complexity_estimator_moderate - Moderate complexity
    11. test_complexity_estimator_advanced - Advanced complexity
    12. test_time_estimation_accuracy - Time estimation within tolerance
    13. test_file_list_generator_create - File creation predictions
    14. test_file_list_generator_modify - File modification predictions
    15. test_file_conflict_detection - Conflict detection
    16. test_test_requirements_generator - Test requirements generation
    17. test_planning_agent_end_to_end - Full planning workflow
    18. test_plan_refinement_with_llm - LLM-based plan refinement
    19. test_save_plan_to_workflow - Plan persistence
    20. test_plan_summary_generation - Summary formatting

    Fixtures:
    - db_session (SQLAlchemy session)
    - mock_byok_handler (Mock BYOKHandler)
    - sample_requirements (Sample parsed requirements)
    - sample_research_context (Sample codebase research results)
    - planning_agent (PlanningAgent instance)

    Test approach:
    - Test with real HTN decomposition (not mocked)
    - Use NetworkX for actual DAG validation
    - Mock LLM calls for refinement testing
    - Parametrize for different feature types

    Coverage target: >= 80% for PlanningAgent
  </action>
  <verify>pytest backend/tests/test_autonomous_planning_agent.py -v --cov=backend/core/autonomous_planning_agent --cov-report=term-missing</verify>
  <done>All tests passing with 80%+ coverage</done>
</task>

</tasks>

<verification>
1. Run tests: pytest backend/tests/test_autonomous_planning_agent.py -v
2. Verify coverage: pytest --cov=backend/core/autonomous_planning_agent --cov-report=html
3. Test DAG validation: Create plan with circular dependency and verify error
4. Test parallelization: Create plan with independent tasks and verify wave detection
5. Test time estimation: Compare estimated vs actual for sample features
6. Verify file lists: Check predicted files match Atom conventions
7. Test full workflow: Run end-to-end planning with sample requirements
</verification>

<success_criteria>
1. Features decompose into hierarchical task networks
2. DAG validation detects circular dependencies
3. Topological sort produces valid execution order
4. Parallelization opportunities are correctly identified
5. Time estimates are within tolerance (simple: +/-15min, moderate: +/-30min)
6. File lists follow Atom conventions
7. Test requirements include coverage targets
8. Plan can be saved to AutonomousWorkflow model
9. Test coverage >= 80% for PlanningAgent
10. All tests passing with no flaky tests
</success_criteria>

<output>
After completion, create `.planning/phases/69-autonomous-coding-agents/69-03-SUMMARY.md` with:
- Files created/modified
- Lines of code added
- Test coverage achieved
- Example implementation plan output
- Next steps (Plan 69-04 depends on this plan)
</output>
