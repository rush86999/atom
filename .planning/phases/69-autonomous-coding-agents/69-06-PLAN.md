---
phase: 69-autonomous-coding-agents
plan: 06
type: execute
wave: 3
depends_on: [69-04, 69-05]
files_modified:
  - backend/core/test_runner_service.py
  - backend/core/auto_fixer_service.py
  - backend/tests/test_test_runner_service.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Tests are executed and results are parsed for failures"
    - "Stack traces are analyzed to identify root causes"
    - "Auto-fixer generates and applies fixes for common failures"
    - "Fix iteration continues until tests pass or max retries reached"
    - "Coverage is measured and reported after each run"
    - "Test results are stored in AutonomousWorkflow model"
    - "Max 5 fix iterations to prevent infinite loops"
  artifacts:
    - path: "backend/core/test_runner_service.py"
      provides: "Test execution and result parsing"
      min_lines: 250
      exports: ["TestRunnerService", "run_tests", "parse_results", "measure_coverage"]
    - path: "backend/core/auto_fixer_service.py"
      provides: "Automated test failure fixing"
      min_lines: 300
      exports: ["AutoFixerService", "analyze_failure", "generate_fix", "apply_fix"]
    - path: "backend/tests/test_test_runner_service.py"
      provides: "Test coverage for test runner"
      min_lines: 200
  key_links:
    - from: "backend/core/test_runner_service.py"
      to: "core/test_generator_service.py"
      via: "Generated tests as input"
      pattern: "from core.test_generator_service import TestGeneratorService"
    - from: "backend/core/auto_fixer_service.py"
      to: "core/autonomous_coder_agent.py"
      via: "Apply fixes to generated code"
      pattern: "from core.autonomous_coder_agent import CodeGeneratorOrchestrator"
    - from: "backend/core/test_runner_service.py"
      to: "core/models.py"
      via: "Store results in AutonomousWorkflow"
      pattern: "AutonomousWorkflow"
---

<objective>
Implement Test Runner & Auto-Fixer Service that executes generated tests, analyzes failures with LLM-powered root cause detection, automatically applies fixes, and iterates until tests pass or max retries reached.

Purpose: Achieve 100% test pass rate automatically by analyzing failures, generating fixes, and re-running tests without human intervention.
Output: TestRunnerService with pytest integration, AutoFixerService with LLM fix generation, comprehensive tests
</objective>

<execution_context>
@/Users/rushiparikh/.claude/get-shit-done/workflows/execute-plan.md
@/Users/rushiparikh/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/69-autonomous-coding-agents/69-RESEARCH.md
@.planning/STATE.md
@backend/conftest.py
@backend/core/test_generator_service.py
@backend/core/autonomous_coder_agent.py
@backend/core/models.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement TestRunnerService for pytest execution</name>
  <files>backend/core/test_runner_service.py</files>
  <action>
    Create TestRunnerService with pytest execution:

    ```python
    import subprocess
    import asyncio
    import json
    from pathlib import Path
    from typing import List, Dict, Any, Optional
    import pytest
    from datetime import datetime

    class TestRunnerService:
        \"\"\"Execute tests and parse results.\"\"\"

        def __init__(
            self,
            db: Session,
            project_root: str = "backend"
        ):
            self.db = db
            self.project_root = Path(project_root)
            self.tests_root = self.project_root / "tests"

        async def run_tests(
            self,
            test_files: Optional[List[str]] = None,
            coverage: bool = True,
            verbose: bool = True
        ) -> Dict[str, Any]:
            \"\"\"Run pytest and capture results.

            Args:
                test_files: List of test files (None = all tests)
                coverage: Run with coverage reporting
                verbose: Verbose output

            Returns:
                {
                    \"passed\": int,
                    \"failed\": int,
                    \"skipped\": int,
                    \"total\": int,
                    \"duration_seconds\": float,
                    \"failures\": [
                        {
                            \"test_file\": str,
                            \"test_name\": str,
                            \"error_type\": str,
                            \"error_message\": str,
                            \"stack_trace\": str,
                            \"line_number\": int
                        }
                    ],
                    \"coverage\": {
                        \"percent\": float,
                        \"covered_lines\": int,
                        \"total_lines\": int
                    }
                }
            \"\"\"

        def _run_pytest(
            self,
            test_files: List[str],
            coverage: bool
        ) -> subprocess.CompletedProcess:
            \"\"\"Execute pytest subprocess.
            Runs: pytest -v --cov-report=json --cov-report=term
            Returns completed process with output.
            \"\"\"

        def parse_pytest_output(
            self,
            output: str
        ) -> Dict[str, Any]:
            \"\"\"Parse pytest terminal output for results.
            Extracts passed, failed, skipped counts.
            Parses FAILURE sections for error details.
            \"\"\"

        def parse_coverage_json(
            self,
            coverage_path: str
        ) -> Dict[str, Any]:
            \"\"\"Parse coverage.json file.
            Returns line coverage percentages by file.
            \"\"\"

        async def run_specific_test(
            self,
            test_file: str,
            test_name: str
        ) -> Dict[str, Any]:
            \"\"\"Run a single test case.
            Useful for re-running after fixes.
            \"\"\"

        def is_test_timeout(
            self,
            duration_seconds: float
        ) -> bool:
            \"\"\"Check if test exceeded timeout.
            Target: <30s per test file, <5s per test.
            \"\"\"
    ```

    Implementation requirements:
    - Use subprocess to run pytest
    - Parse both stdout and coverage.json
    - Extract failure details (test name, error, stack trace)
    - Measure execution time
    - Minimum 100 lines for test execution

    Pytest configuration:
    - Use existing pytest.ini
    - Enable verbose output (-v)
    - Coverage with json report
    - Timeout support if available
  </action>
  <verify>grep -n "class TestRunnerService\|run_tests\|parse_pytest_output" backend/core/test_runner_service.py</verify>
  <done>TestRunnerService with pytest execution and parsing</done>
</task>

<task type="auto">
  <name>Task 2: Implement stack trace analyzer for failure analysis</name>
  <files>backend/core/test_runner_service.py</files>
<action>
    Add failure analysis methods:

    ```python
    class StackTraceAnalyzer:
        \"\"\"Analyze test failure stack traces.\"\"\"

        def __init__(self):
            # Common error patterns
            self.error_patterns = {
                \"AssertionError\": r\"AssertionError: (.+)\",
                \"AttributeError\": r\"AttributeError: '(.+)' object has no attribute '(.+)'\",
                \"ImportError\": r\"ImportError: (.+)\",
                \"TypeError\": r\"TypeError: (.+)\",
                \"ValueError\": r\"ValueError: (.+)\",
                \"KeyError\": r\"KeyError: '(.+)'\",
                \"sqlalchemy.exc.IntegrityError\": r\"IntegrityError: (.+)\",
                \"sqlalchemy.orm.exc.*\": r\"(.+)Error: (.+)\",
            }

        def analyze_failure(
            self,
            failure: Dict[str, Any]
        ) -> Dict[str, Any]:
            \"\"\"Analyze test failure and identify root cause.

            Args:
                failure: Single failure from TestRunnerService

            Returns:
                {
                    \"error_type\": str,
                    \"error_category\": str,  # assertion, import, database, api, logic
                    \"root_cause\": str,
                    \"suggested_fix\": str,
                    \"file_to_fix\": str,
                    \"line_number\": int,
                    \"confidence\": float  # 0-1
                }
            \"\"\"

        def categorize_error(
            self,
            error_type: str,
            error_message: str
        ) -> str:
            \"\"\"Categorize error by type.
            Categories: assertion, import, database, api, logic, type_error
            \"\"\"

        def extract_location_from_trace(
            self,
            stack_trace: str
        ) -> Dict[str, str]:
            \"\"\"Extract file path and line number from stack trace.
            Returns {\"file\": str, \"line\": int}
            \"\"\"

        def identify_fix_strategy(
            self,
            error_category: str,
            root_cause: str
        ) -> str:
            \"\"\"Identify fix strategy based on error category.
            Strategies: add_import, fix_assertion, add_mock, fix_query, add_type_hint
            \"\"\"

        def generate_fix_suggestion(
            self,
            analysis: Dict[str, Any]
        ) -> str:
            \"\"\"Generate human-readable fix suggestion.
            Example: \"Add db.commit() after creating OAuthSession\"
            \"\"\"
    ```

    Implementation requirements:
    - Parse common error types with regex
    - Categorize errors for appropriate fix strategy
    - Extract file location from stack trace
    - Generate specific fix suggestions
    - Minimum 80 lines for stack trace analysis

    Error categories:
    - Assertion: Expected vs actual mismatch
    - Import: Missing or incorrect import
    - Database: Query, constraint, transaction issues
    - API: Endpoint, response, authentication issues
    - Logic: Business logic errors
    - Type: Type mismatch, None errors
  </action>
  <verify>grep -n "class StackTraceAnalyzer\|analyze_failure\|categorize_error" backend/core/test_runner_service.py</verify>
  <done>StackTraceAnalyzer with error categorization</done>
</task>

<task type="auto">
  <name>Task 3: Implement AutoFixerService for automated fixing</name>
  <files>backend/core/auto_fixer_service.py</files>
  <action>
    Create AutoFixerService with LLM-powered fixing:

    ```python
    from typing import List, Dict, Any, Optional
    import re
    from pathlib import Path

    class AutoFixerService:
        \"\"\"Automatically fix test failures using LLM.\"\"\"

        def __init__(
            self,
            db: Session,
            byok_handler: BYOKHandler
        ):
            self.db = db
            self.byok_handler = byok_handler
            self.max_iterations = 5
            self.fix_history = []

        async def fix_failures(
            self,
            failures: List[Dict[str, Any]],
            source_files: Dict[str, str]
        ) -> Dict[str, Any]:
            \"\"\"Fix multiple test failures.

            Args:
                failures: List of failures from TestRunnerService
                source_files: Current source code {path: content}

            Returns:
                {
                    \"fixes_applied\": [
                        {\"file\": str, \"original\": str, \"fixed\": str}
                    ],
                    \"remaining_failures\": int,
                    \"iterations\": int
                }
            \"\"\"

        async def fix_single_failure(
            self,
            failure: Dict[str, Any],
            source_code: str
        ) -> Optional[str]:
            \"\"\"Fix a single test failure.

            Analyzes failure, generates fix with LLM, applies fix.
            Returns fixed code or None if fix failed.
            \"\"\"

        async def generate_fix_with_llm(
            self,
            failure: Dict[str, Any],
            source_code: str
        ) -> str:
            \"\"\"Use LLM to generate code fix.

            Prompt includes:
            - Error message and stack trace
            - Source code context (10 lines before/after error)
            - Error analysis from StackTraceAnalyzer
            - Request for minimal fix that makes test pass

            Returns fixed code snippet.
            \"\"\"

        def apply_fix(
            self,
            source_code: str,
            fix: str,
            line_number: int
        ) -> str:
            \"\"\"Apply fix to source code.

            Replaces lines around line_number with fix.
            Preserves indentation and surrounding code.
            \"\"\"

        def validate_fix(
            self,
            original: str,
            fixed: str
        ) -> Dict[str, Any]:
            \"\"\"Validate fix is safe and minimal.

            Checks:
            - Fix doesn't remove too much code
            - Fix doesn't break syntax
            - Fix is minimal (only changes what's needed)

            Returns {\"valid\": bool, \"reason\": str}
            \"\"\"

        async def iterate_until_fixed(
            self,
            test_file: str,
            source_files: Dict[str, str],
            max_iterations: int = 5
        ) -> Dict[str, Any]:
            \"\"\"Run test, fix failures, repeat until pass or max iterations.

            Loop:
            1. Run tests
            2. If all pass, return success
            3. Analyze failures
            4. Generate fixes
            5. Apply fixes
            6. Repeat from 1

            Returns final status and fixes applied.
            \"\"\"
    ```

    Implementation requirements:
    - Use BYOK handler for LLM fix generation
    - Apply fixes at correct line numbers
    - Validate fixes before applying
    - Prevent infinite loops (max 5 iterations)
    - Track fix history for audit trail
    - Minimum 150 lines for auto-fixer

    LLM prompt strategy:
    - Include error context (type, message, stack trace)
    - Include source code around error line
    - Request minimal fix (don't refactor)
    - Ask for explanation of fix
    - Set temperature=0 for deterministic fixes
  </action>
  <verify>grep -n "class AutoFixerService\|fix_failures\|generate_fix_with_llm" backend/core/auto_fixer_service.py</verify>
  <done>AutoFixerService with LLM-powered fixes</done>
</task>

<task type="auto">
  <name>Task 4: Implement common fix patterns for quick fixes</name>
  <files>backend/core/auto_fixer_service.py</files>
  <action>
    Add common fix pattern methods:

    ```python
    class CommonFixPatterns:
        \"\"\"Common fix patterns for quick fixes without LLM.\"\"\"

        # Fix patterns: {error_pattern: fix_template}
        PATTERNS = {
            # Missing imports
            r\"NameError: name '(\w+)' is not defined\": \"from {module} import {name}\",
            # Missing db.commit()
            r\"assert None == (\w+)\": \"db.commit()\",
            # Missing await
            r\"coroutine '(\w+)' was never awaited\": \"await {call}\",
            # Wrong assertion
            r\"AssertionError: assert (.+) == None\": \"assert {value} is not None\",
            # AttributeError on None
            r\"AttributeError: 'NoneType' object has no attribute '(\w+)'\": \"if {obj} is not None:\",
        }

        @classmethod
        def find_pattern_match(
            cls,
            error_message: str
        ) -> Optional[Dict[str, Any]]:
            \"\"\"Find matching fix pattern for error.
            Returns pattern groups or None.
            \"\"\"

        @classmethod
        def apply_pattern_fix(
            cls,
            source_code: str,
            line_number: int,
            pattern: str,
            groups: Dict[str, str]
        ) -> Optional[str]:
            \"\"\"Apply pattern-based fix.
            Returns fixed code or None if pattern doesn't match.
            \"\"\"

        @classmethod
        def fix_missing_import(
            cls,
            source_code: str,
            missing_name: str
        ) -> Optional[str]:
            \"\"\"Fix missing import by adding import statement.
            Infers module from common patterns.
            \"\"\"

        @classmethod
        def fix_missing_db_commit(
            cls,
            source_code: str,
            line_number: int
        ) -> Optional[str]:
            \"\"\"Fix missing db.commit() after database operation.
            Adds commit after add() or merge().
            \"\"\"

        @classmethod
        def fix_missing_await(
            cls,
            source_code: str,
            line_number: int
        ) -> Optional[str]:
            \"\"\"Fix missing await for async function calls.
            Adds await before async call.
            \"\"\"
    ```

    Implementation requirements:
    - Pattern-based fixes for common errors
    - Faster than LLM for known patterns
    - Fallback to LLM for unknown patterns
    - Minimum 80 lines for pattern fixes

    Common patterns:
    - Missing import: Add import at top
    - Missing db.commit(): Add after db.add()
    - Missing await: Add await before async call
    - Wrong assertion: Fix assertion type
    - None attribute: Add None check
  </action>
  <verify>grep -n "class CommonFixPatterns\|find_pattern_match\|fix_missing_import" backend/core/auto_fixer_service.py</verify>
  <done>CommonFixPatterns with quick fix templates</done>
</task>

<task type="auto">
  <name>Task 5: Implement fix validation and safety checks</name>
  <files>backend/core/auto_fixer_service.py</files>
  <action>
    Add fix validation methods:

    ```python
    class FixValidator:
        \"\"\"Validate fixes are safe and correct.\"\"\"

        def __init__(self):
            self.max_lines_changed = 20  # Max lines a fix can change
            self.max_file_size_change = 0.5  # Max 50% size increase

        def validate_fix(
            self,
            original: str,
            fixed: str,
            line_number: int
        ) -> Dict[str, Any]:
            \"\"\"Validate fix is safe.

            Returns:
                {
                    \"valid\": bool,
                    \"reasons\": [str],  # Reasons if invalid
                    \"warnings\": [str]  # Warnings for risky fixes
                }
            \"\"\"

        def check_syntax(
            self,
            code: str
        ) -> Dict[str, Any]:
            \"\"\"Check Python syntax is valid.
            Uses ast.parse to validate.
            \"\"\"

        def check_size_change(
            self,
            original: str,
            fixed: str
        ) -> bool:
            \"\"\"Check fix doesn't change too much code.
            Prevents massive rewrites.
            \"\"\"

        def check_imports(
            self,
            fixed: str
        ) -> List[str]:
            \"\"\"Check imports are valid.
            Returns list of invalid imports (if any).
            \"\"\"

        def check_no_secrets(
            self,
            fixed: str
        ) -> bool:
            \"\"\"Check fix doesn't add secrets.
            Scans for API keys, tokens, passwords.
            \"\"\"

        def check_indentation(
            self,
            fixed: str
        ) -> bool:
            \"\"\"Check fix maintains proper indentation.
            Python indentation is critical.
            \"\"\"

        def estimate_fix_risk(
            self,
            fix: str,
            file_path: str
        ) -> str:
            \"\"\"Estimate fix risk level.
            Returns: \"low\", \"medium\", \"high\"
            \"\"\"
    ```

    Implementation requirements:
    - Validate Python syntax with ast.parse
    - Check fix size is reasonable
    - Prevent secrets from being added
    - Validate indentation
    - Estimate risk for governance
    - Minimum 80 lines for validation

    Risk levels:
    - Low: One-line fix, no behavior change
    - Medium: Multi-line fix, controlled behavior change
    - High: Significant logic change, new dependencies
  </action>
  <verify>grep -n "class FixValidator\|validate_fix\|check_syntax" backend/core/auto_fixer_service.py</verify>
  <done>FixValidator with safety checks</done>
</task>

<task type="auto">
  <name>Task 6: Implement test result storage and reporting</name>
  <files>backend/core/test_runner_service.py</files>
  <action>
    Add result storage methods:

    ```python
    class TestResultStorage:
        \"\"\"Store and report test results.\"\"\"

        def __init__(self, db: Session):
            self.db = db

        def save_results_to_workflow(
            self,
            workflow_id: str,
            results: Dict[str, Any]
        ) -> None:
            \"\"\"Save test results to AutonomousWorkflow.
            Updates test_results JSON field.
            \"\"\"

        def create_agent_log(
            self,
            workflow_id: str,
            agent_id: str,
            phase: str,
            action: str,
            input_data: Dict[str, Any],
            output_data: Dict[str, Any],
            status: str,
            error_message: Optional[str] = None
        ) -> AgentLog:
            \"\"\"Create AgentLog entry for test execution.
            Tracks test run for audit trail.
            \"\"\"

        def generate_test_report(
            self,
            results: Dict[str, Any]
        ) -> str:
            \"\"\"Generate human-readable test report.

            Returns formatted report with:
            - Summary (passed/failed/total)
            - Failure details
            - Coverage summary
            - Recommendations
            \"\"\"

        def calculate_coverage_delta(
            self,
            before: Dict[str, Any],
            after: Dict[str, Any]
        ) -> Dict[str, float]:
            \"\"\"Calculate coverage improvement.
            Returns {\"line_delta\": float, \"percent_delta\": float}
            \"\"\"
    ```

    Implementation requirements:
    - Save results to AutonomousWorkflow.test_results
    - Create AgentLog entries for audit
    - Generate readable reports
    - Calculate coverage improvements
    - Minimum 70 lines for storage

    Result format:
    - test_results: {passed, failed, coverage, duration, failures}
    - AgentLog: {agent_id, phase, action, status, duration}
  </action>
  <verify>grep -n "class TestResultStorage\|save_results_to_workflow\|generate_test_report" backend/core/test_runner_service.py</verify>
  <done>TestResultStorage with database persistence</done>
</task>

<task type="auto">
  <name>Task 7: Create comprehensive tests for services</name>
  <files>backend/tests/test_test_runner_service.py</files>
  <action>
    Create test file with 200+ lines covering:

    Test cases (TestRunnerService):
    1. test_run_tests_success - All tests passing
    2. test_run_tests_with_failures - Some tests failing
    3. test_parse_pytest_output - Output parsing
    4. test_parse_coverage_json - Coverage parsing
    5. test_run_specific_test - Single test execution
    6. test_is_test_timeout - Timeout detection

    Test cases (StackTraceAnalyzer):
    7. test_analyze_assertion_error - Assertion analysis
    8. test_analyze_attribute_error - Attribute error analysis
    9. test_analyze_import_error - Import error analysis
    10. test_categorize_error - Error categorization
    11. test_extract_location_from_trace - Location extraction
    12. test_identify_fix_strategy - Strategy identification

    Test cases (AutoFixerService):
    13. test_fix_single_failure - Single failure fix
    14. test_fix_multiple_failures - Multiple failures
    15. test_iterate_until_fixed - Iteration loop
    16. test_max_iterations_reached - Max iterations limit
    17. test_generate_fix_with_llm - LLM fix generation
    18. test_apply_fix - Fix application

    Test cases (CommonFixPatterns):
    19. test_fix_missing_import - Import fix
    20. test_fix_missing_db_commit - Commit fix
    21. test_fix_missing_await - Await fix

    Test cases (FixValidator):
    22. test_validate_fix_success - Valid fix
    23. test_validate_fix_syntax_error - Syntax error detection
    24. test_validate_fix_too_large - Size validation
    25. test_check_no_secrets - Secret detection

    Test cases (TestResultStorage):
    26. test_save_results_to_workflow - Result storage
    27. test_create_agent_log - Log creation
    28. test_generate_test_report - Report generation

    Fixtures:
    - db_session (SQLAlchemy session)
    - mock_byok_handler (Mock BYOKHandler)
    - sample_test_results (Sample test output)
    - sample_failures (Sample failure data)
    - runner_service (TestRunnerService instance)
    - fixer_service (AutoFixerService instance)

    Coverage target: >= 80% for both services
  </action>
  <verify>pytest backend/tests/test_test_runner_service.py -v --cov=backend/core/test_runner_service --cov=backend/core/auto_fixer_service --cov-report=term-missing</verify>
  <done>All tests passing with 80%+ coverage</done>
</task>

</tasks>

<verification>
1. Run tests: pytest backend/tests/test_test_runner_service.py -v
2. Verify coverage: pytest --cov=backend/core/test_runner_service --cov=backend/core/auto_fixer_service --cov-report=html
3. Test test execution: Run sample tests and verify parsing
4. Test failure analysis: Create sample failure and verify analysis
5. Test auto-fixing: Generate code with bug and verify fix applied
6. Test iteration: Create code with multiple bugs and verify iteration
7. Test validation: Apply unsafe fix and verify rejection
8. Test storage: Save results and verify database persistence
9. Verify max iterations: Create unfixable bug and verify 5 iteration limit
</verification>

<success_criteria>
1. TestRunnerService executes pytest and parses results
2. StackTraceAnalyzer categorizes errors correctly
3. AutoFixerService generates fixes using LLM
4. CommonFixPatterns apply quick fixes for known patterns
5. FixValidator rejects unsafe fixes
6. Iteration continues until pass or max retries
7. Results are stored in AutonomousWorkflow
8. AgentLog entries track all executions
9. Coverage is measured and reported
10. Test coverage >= 80% for both services
11. All tests passing with no flaky tests
</success_criteria>

<output>
After completion, create `.planning/phases/69-autonomous-coding-agents/69-06-SUMMARY.md` with:
- Files created/modified
- Lines of code added
- Test coverage achieved
- Auto-fix success rate metrics
- Iteration statistics
- Next steps (Plan 69-07 depends on test results)
</output>
