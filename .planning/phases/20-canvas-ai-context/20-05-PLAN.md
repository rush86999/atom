---
phase: 20-canvas-ai-context
plan: 05
type: execute
wave: 2
depends_on: ["20-03", "20-04"]
files_modified:
  - backend/tests/test_canvas_context_enrichment.py
  - backend/tests/test_canvas_aware_retrieval.py
autonomous: true

must_haves:
  truths:
    - "Tests exist for all 7 canvas types creating canvas context"
    - "Tests verify canvas_context enrichment on episode creation"
    - "Tests verify canvas-aware retrieval filters by canvas_type"
    - "Tests verify progressive detail levels (summary/standard/full)"
    - "Test coverage for episodic memory canvas integration reaches 50%+"
  artifacts:
    - path: "backend/tests/test_canvas_context_enrichment.py"
      provides: "Canvas context enrichment tests"
      contains: "test_canvas_context_extraction, test_7_canvas_types"
    - path: "backend/tests/test_canvas_aware_retrieval.py"
      provides: "Canvas-aware retrieval tests"
      contains: "test_retrieve_by_canvas_type, test_progressive_detail"
  key_links:
    - from: "backend/tests/test_canvas_context_enrichment.py"
      to: "backend/core/episode_segmentation_service.py"
      via: "Test coverage of _extract_canvas_context method"
      pattern: "def test_extract_canvas_context"
    - from: "backend/tests/test_canvas_aware_retrieval.py"
      to: "backend/core/episode_retrieval_service.py"
      via: "Test coverage of retrieve_canvas_aware method"
      pattern: "def test_retrieve_canvas_aware"
---

<objective>
Test canvas context enrichment across all 7 canvas types and verify canvas-aware episode retrieval with 50%+ coverage target for episodic memory services.

**Purpose**: Ensure canvas context integration works correctly for all canvas types and retrieval methods function as designed.

**Output**: Comprehensive test suite for canvas context enrichment and canvas-aware retrieval with 50%+ coverage.
</objective>

<execution_context>
@/Users/rushiparikh/.claude/get-shit-done/workflows/execute-plan.md
@/Users/rushiparikh/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/20-canvas-ai-context/20-RESEARCH.md
@.planning/phases/20-canvas-ai-context/20-03-SUMMARY.md
@.planning/phases/20-canvas-ai-context/20-04-SUMMARY.md
@.planning/ROADMAP.md
@.planning/STATE.md

@backend/core/episode_segmentation_service.py
@backend/core/episode_retrieval_service.py
@backend/tests/conftest.py
</context>

<tasks>

<task type="auto">
  <name>Create canvas context enrichment tests</name>
  <files>backend/tests/test_canvas_context_enrichment.py</files>
  <action>
    Create comprehensive test file for canvas context extraction and enrichment.

    Create `backend/tests/test_canvas_context_enrichment.py`:

    ```python
    """
    Canvas Context Enrichment Tests

    Tests for canvas context extraction from CanvasAudit records
    and enrichment of EpisodeSegment with canvas_context.
    """

    import pytest
    from datetime import datetime
    from sqlalchemy.orm import Session

    from core.models import (
        AgentRegistry,
        AgentStatus,
        CanvasAudit,
        ChatSession,
        Episode,
        EpisodeSegment,
        User,
    )
    from core.episode_segmentation_service import EpisodeSegmentationService


    # ============================================================================
    # Fixtures
    # ============================================================================

    @pytest.fixture
    def segmentation_service(db_session: Session) -> EpisodeSegmentationService:
        """Provide episode segmentation service"""
        return EpisodeSegmentationService(db_session)


    @pytest.fixture
    def test_user(db_session: Session) -> User:
        """Create test user"""
        user = User(
            id="test-user-canvas",
            email="canvas@example.com",
            first_name="Canvas",
            last_name="Tester"
        )
        db_session.add(user)
        db_session.commit()
        return user


    @pytest.fixture
    def test_agent(db_session: Session) -> AgentRegistry:
        """Create test agent"""
        agent = AgentRegistry(
            id="test-agent-canvas",
            name="CanvasTestAgent",
            description="Test agent for canvas context",
            status=AgentStatus.ACTIVE,
            maturity_level="INTERN",
            confidence_score=0.6
        )
        db_session.add(agent)
        db_session.commit()
        return agent


    @pytest.fixture
    def test_session(db_session: Session, test_user: User) -> ChatSession:
        """Create test chat session"""
        session = ChatSession(
            id="test-session-canvas",
            user_id=test_user.id,
            created_at=datetime.now()
        )
        db_session.add(session)
        db_session.commit()
        return session


    # ============================================================================
    # Canvas Context Extraction Tests (7 Canvas Types)
    # ============================================================================

    class TestCanvasContextExtraction:
        """Test canvas context extraction from CanvasAudit records"""

        def test_extract_generic_canvas_context(
            self,
            segmentation_service: EpisodeSegmentationService,
            test_session: ChatSession
        ):
            """Test extracting context from generic canvas (charts, forms)"""
            # Create canvas audit for generic canvas with chart
            audit = CanvasAudit(
                id="audit-generic-001",
                session_id=test_session.id,
                canvas_type="generic",
                action="present",
                component="line_chart",
                metadata_json={
                    "component": "line_chart",
                    "chart_type": "line",
                    "title": "User Growth",
                    "data_points": [
                        {"x": "Jan", "y": 100},
                        {"x": "Feb", "y": 150}
                    ]
                },
                created_at=datetime.now()
            )
            segmentation_service.db.add(audit)
            segmentation_service.db.commit()

            # Extract context
            context = segmentation_service._extract_canvas_context([audit])

            # Verify
            assert context is not None
            assert context["canvas_type"] == "generic"
            assert "presentation_summary" in context
            assert "line_chart" in context["visual_elements"] or "chart" in str(context["visual_elements"])

        def test_extract_docs_canvas_context(
            self,
            segmentation_service: EpisodeSegmentationService,
            test_session: ChatSession
        ):
            """Test extracting context from documentation canvas"""
            audit = CanvasAudit(
                id="audit-docs-001",
                session_id=test_session.id,
                canvas_type="docs",
                action="present",
                component="document_viewer",
                metadata_json={
                    "component": "document_viewer",
                    "title": "API Documentation",
                    "word_count": 2500
                },
                created_at=datetime.now()
            )
            segmentation_service.db.add(audit)
            segmentation_service.db.commit()

            context = segmentation_service._extract_canvas_context([audit])

            assert context["canvas_type"] == "docs"
            assert "presentation_summary" in context

        def test_extract_email_canvas_context(
            self,
            segmentation_service: EpisodeSegmentationService,
            test_session: ChatSession
        ):
            """Test extracting context from email canvas"""
            audit = CanvasAudit(
                id="audit-email-001",
                session_id=test_session.id,
                canvas_type="email",
                action="present",
                component="compose",
                metadata_json={
                    "component": "compose",
                    "subject": "Project Update",
                    "recipient": "team@example.com"
                },
                created_at=datetime.now()
            )
            segmentation_service.db.add(audit)
            segmentation_service.db.commit()

            context = segmentation_service._extract_canvas_context([audit])

            assert context["canvas_type"] == "email"
            assert "presentation_summary" in context

        def test_extract_sheets_canvas_context(
            self,
            segmentation_service: EpisodeSegmentationService,
            test_session: ChatSession
        ):
            """Test extracting context from spreadsheet canvas with business data"""
            audit = CanvasAudit(
                id="audit-sheets-001",
                session_id=test_session.id,
                canvas_type="sheets",
                action="present",
                component="sheet",
                metadata_json={
                    "component": "sheet",
                    "revenue": "$1.2M",
                    "amount": 1200000
                },
                created_at=datetime.now()
            )
            segmentation_service.db.add(audit)
            segmentation_service.db.commit()

            context = segmentation_service._extract_canvas_context([audit])

            assert context["canvas_type"] == "sheets"
            assert "critical_data_points" in context
            assert "revenue" in context["critical_data_points"]

        def test_extract_orchestration_canvas_context(
            self,
            segmentation_service: EpisodeSegmentationService,
            test_session: ChatSession
        ):
            """Test extracting context from orchestration canvas with workflow data"""
            audit = CanvasAudit(
                id="audit-orchestration-001",
                session_id=test_session.id,
                canvas_type="orchestration",
                action="submit",
                component="workflow_board",
                metadata_json={
                    "component": "workflow_board",
                    "workflow_id": "wf-123",
                    "approval_status": "approved"
                },
                created_at=datetime.now()
            )
            segmentation_service.db.add(audit)
            segmentation_service.db.commit()

            context = segmentation_service._extract_canvas_context([audit])

            assert context["canvas_type"] == "orchestration"
            assert "user_interaction" in context
            assert "approved" in context["user_interaction"].lower()
            assert "critical_data_points" in context
            assert context["critical_data_points"]["workflow_id"] == "wf-123"

        def test_extract_terminal_canvas_context(
            self,
            segmentation_service: EpisodeSegmentationService,
            test_session: ChatSession
        ):
            """Test extracting context from terminal canvas"""
            audit = CanvasAudit(
                id="audit-terminal-001",
                session_id=test_session.id,
                canvas_type="terminal",
                action="execute",
                component="terminal",
                metadata_json={
                    "component": "terminal",
                    "command": "npm install",
                    "exit_code": 0
                },
                created_at=datetime.now()
            )
            segmentation_service.db.add(audit)
            segmentation_service.db.commit()

            context = segmentation_service._extract_canvas_context([audit])

            assert context["canvas_type"] == "terminal"
            assert "critical_data_points" in context
            assert context["critical_data_points"]["command"] == "npm install"

        def test_extract_coding_canvas_context(
            self,
            segmentation_service: EpisodeSegmentationService,
            test_session: ChatSession
        ):
            """Test extracting context from coding canvas"""
            audit = CanvasAudit(
                id="audit-coding-001",
                session_id=test_session.id,
                canvas_type="coding",
                action="present",
                component="editor",
                metadata_json={
                    "component": "editor",
                    "file_path": "src/components/Header.tsx",
                    "language": "typescript"
                },
                created_at=datetime.now()
            )
            segmentation_service.db.add(audit)
            segmentation_service.db.commit()

            context = segmentation_service._extract_canvas_context([audit])

            assert context["canvas_type"] == "coding"
            assert "presentation_summary" in context


    # ============================================================================
    # Episode Segment Canvas Context Tests
    # ============================================================================

    class TestEpisodeSegmentCanvasContext:
        """Test EpisodeSegment canvas context enrichment"""

        def test_episode_segment_created_with_canvas_context(
            self,
            segmentation_service: EpisodeSegmentationService,
            test_agent: AgentRegistry,
            test_user: User,
            test_session: ChatSession
        ):
            """Test that EpisodeSegment includes canvas_context when canvas audits exist"""
            # Create chat messages for session
            from core.models import ChatMessage
            message = ChatMessage(
                id="msg-canvas-001",
                conversation_id=test_session.id,
                content="Show me the workflow approval",
                role="user",
                created_at=datetime.now()
            )
            segmentation_service.db.add(message)

            # Create canvas audit
            audit = CanvasAudit(
                id="audit-segment-001",
                session_id=test_session.id,
                canvas_type="orchestration",
                action="present",
                component="workflow_board",
                metadata_json={"workflow_id": "wf-456", "approval_status": "pending"},
                created_at=datetime.now()
            )
            segmentation_service.db.add(audit)
            segmentation_service.db.commit()

            # Create episode
            episode = await segmentation_service.create_episode_from_session(
                session_id=test_session.id,
                agent_id=test_agent.id,
                title="Workflow Approval Request",
                force_create=True
            )

            # Verify episode created
            assert episode is not None

            # Verify segment has canvas_context
            segments = segmentation_service.db.query(EpisodeSegment).filter(
                EpisodeSegment.episode_id == episode.id
            ).all()

            assert len(segments) > 0
            segment_with_canvas = [s for s in segments if s.canvas_context is not None]
            assert len(segment_with_canvas) > 0, "At least one segment should have canvas_context"

            # Verify canvas context structure
            canvas_ctx = segment_with_canvas[0].canvas_context
            assert "canvas_type" in canvas_ctx
            assert canvas_ctx["canvas_type"] == "orchestration"

        def test_episode_segment_without_canvas_audit_has_no_context(
            self,
            segmentation_service: EpisodeSegmentationService,
            test_agent: AgentRegistry,
            test_user: User,
            test_session: ChatSession
        ):
            """Test that EpisodeSegment without canvas_audit has null canvas_context"""
            from core.models import ChatMessage
            message = ChatMessage(
                id="msg-no-canvas-001",
                conversation_id=test_session.id,
                content="Hello agent",
                role="user",
                created_at=datetime.now()
            )
            segmentation_service.db.add(message)
            segmentation_service.db.commit()

            # Create episode without any canvas audits
            episode = await segmentation_service.create_episode_from_session(
                session_id=test_session.id,
                agent_id=test_agent.id,
                title="Simple Chat",
                force_create=True
            )

            assert episode is not None

            # Check segments - should not have canvas_context
            segments = segmentation_service.db.query(EpisodeSegment).filter(
                EpisodeSegment.episode_id == episode.id
            ).all()

            # All segments should have null canvas_context since no canvas audits exist
            for segment in segments:
                assert segment.canvas_context is None or segment.canvas_audit_id is None


    # ============================================================================
    # Progressive Detail Tests
    # ============================================================================

    class TestProgressiveDetailLevels:
        """Test canvas context progressive detail filtering"""

        def test_summary_level_returns_presentation_summary_only(
            self,
            segmentation_service: EpisodeSegmentationService
        ):
            """Test summary level returns only presentation_summary"""
            full_context = {
                "canvas_type": "orchestration",
                "presentation_summary": "Agent presented workflow approval form",
                "visual_elements": ["workflow_board", "approval_form"],
                "user_interaction": "User approved workflow",
                "critical_data_points": {
                    "workflow_id": "wf-123",
                    "approval_status": "approved"
                }
            }

            filtered = segmentation_service._filter_canvas_context_detail(
                full_context,
                "summary"
            )

            # Only presentation_summary should remain
            assert "presentation_summary" in filtered
            assert filtered["presentation_summary"] == "Agent presented workflow approval form"
            assert "visual_elements" not in filtered
            assert "critical_data_points" not in filtered

        def test_standard_level_includes_critical_data_points(
            self,
            segmentation_service: EpisodeSegmentationService
        ):
            """Test standard level includes summary + critical_data_points"""
            full_context = {
                "canvas_type": "sheets",
                "presentation_summary": "Agent presented revenue chart",
                "visual_elements": ["line_chart", "data_table"],
                "user_interaction": "User clicked export",
                "critical_data_points": {
                    "revenue": "$1.2M",
                    "amount": 1200000
                }
            }

            filtered = segmentation_service._filter_canvas_context_detail(
                full_context,
                "standard"
            )

            # Should have summary + critical_data_points
            assert "presentation_summary" in filtered
            assert "critical_data_points" in filtered
            assert filtered["critical_data_points"]["revenue"] == "$1.2M"
            assert "visual_elements" not in filtered

        def test_full_level_includes_all_fields(
            self,
            segmentation_service: EpisodeSegmentationService
        ):
            """Test full level includes all fields"""
            full_context = {
                "canvas_type": "generic",
                "presentation_summary": "Agent presented dashboard",
                "visual_elements": ["line_chart", "bar_chart"],
                "user_interaction": "User refreshed data",
                "critical_data_points": {"metrics": "1000 users"}
            }

            filtered = segmentation_service._filter_canvas_context_detail(
                full_context,
                "full"
            )

            # All fields should be present
            assert "presentation_summary" in filtered
            assert "visual_elements" in filtered
            assert "user_interaction" in filtered
            assert "critical_data_points" in filtered


    # ============================================================================
    # Coverage Tests
    # ============================================================================

    def test_canvas_context_coverage_50_percent_target():
        """
        Verify that canvas context tests contribute to 50%+ coverage target.
        This is a marker test - actual coverage measured by pytest-cov.
        """
        # This test documents the coverage target
        # Run with: pytest --cov=core.episode_segmentation_service --cov=core.episode_retrieval_service
        assert True, "Canvas context tests should contribute to episodic memory coverage"
    ```

    Run tests after creation:
    ```bash
    cd /Users/rushiparikh/projects/atom/backend
    pytest tests/test_canvas_context_enrichment.py -v --cov=core.episode_segmentation_service --cov-report=term-missing
    ```
  </action>
  <verify>
    1. test_canvas_context_enrichment.py created
    2. All 7 canvas types have extraction tests
    3. Progressive detail level tests exist (summary/standard/full)
    4. EpisodeSegment canvas context enrichment test exists
    5. Tests run without errors
  </verify>
  <done>
    Canvas context enrichment tests cover all 7 canvas types with extraction and progressive detail tests.
  </done>
</task>

<task type="auto">
  <name>Create canvas-aware retrieval tests</name>
  <files>backend/tests/test_canvas_aware_retrieval.py</files>
  <action>
    Create comprehensive test file for canvas-aware episode retrieval.

    Create `backend/tests/test_canvas_aware_retrieval.py`:

    ```python
    """
    Canvas-Aware Episode Retrieval Tests

    Tests for canvas-aware episode retrieval with canvas_type filtering
    and progressive detail levels.
    """

    import pytest
    from datetime import datetime, timedelta
    from sqlalchemy.orm import Session

    from core.models import (
        AgentRegistry,
        AgentStatus,
        Episode,
        EpisodeSegment,
        User,
    )
    from core.episode_retrieval_service import EpisodeRetrievalService


    # ============================================================================
    # Fixtures
    # ============================================================================

    @pytest.fixture
    def retrieval_service(db_session: Session) -> EpisodeRetrievalService:
        """Provide episode retrieval service"""
        return EpisodeRetrievalService(db_session)


    @pytest.fixture
    def test_agent(db_session: Session) -> AgentRegistry:
        """Create test agent with INTERN maturity (can use semantic search)"""
        agent = AgentRegistry(
            id="test-agent-retrieval",
            name="RetrievalTestAgent",
            description="Test agent for canvas-aware retrieval",
            status=AgentStatus.ACTIVE,
            maturity_level="INTERN",
            confidence_score=0.7
        )
        db_session.add(agent)
        db_session.commit()
        return agent


    @pytest.fixture
    def test_user(db_session: Session) -> User:
        """Create test user"""
        user = User(
            id="test-user-retrieval",
            email="retrieval@example.com",
            first_name="Retrieval",
            last_name="Tester"
        )
        db_session.add(user)
        db_session.commit()
        return user


    # ============================================================================
    # Canvas Type Filter Tests
    # ============================================================================

    class TestCanvasTypeFiltering:
        """Test canvas type filtering in episode retrieval"""

        @pytest.mark.asyncio
        async def test_retrieve_by_orchestration_canvas_type(
            self,
            retrieval_service: EpisodeRetrievalService,
            test_agent: AgentRegistry,
            test_user: User
        ):
            """Test retrieving episodes filtered by orchestration canvas type"""
            # Create episodes with different canvas types
            episode1 = Episode(
                id="ep-orchestration-001",
                agent_id=test_agent.id,
                user_id=test_user.id,
                title="Workflow Approval",
                started_at=datetime.now() - timedelta(days=1),
                status="active"
            )
            retrieval_service.db.add(episode1)

            # Add segment with orchestration canvas context
            segment1 = EpisodeSegment(
                id="seg-orchestration-001",
                episode_id=episode1.id,
                sequence_number=1,
                segment_type="canvas_interaction",
                content="Agent presented workflow board",
                timestamp=datetime.now() - timedelta(days=1),
                agent_id=test_agent.id,
                user_id=test_user.id,
                canvas_context={
                    "canvas_type": "orchestration",
                    "presentation_summary": "Agent presented workflow approval form",
                    "visual_elements": ["workflow_board"],
                    "critical_data_points": {
                        "workflow_id": "wf-123",
                        "approval_status": "approved"
                    }
                }
            )
            retrieval_service.db.add(segment1)

            # Create another episode with different canvas type
            episode2 = Episode(
                id="ep-terminal-001",
                agent_id=test_agent.id,
                user_id=test_user.id,
                title="Terminal Command",
                started_at=datetime.now() - timedelta(days=1),
                status="active"
            )
            retrieval_service.db.add(episode2)

            segment2 = EpisodeSegment(
                id="seg-terminal-001",
                episode_id=episode2.id,
                sequence_number=1,
                segment_type="canvas_interaction",
                content="Agent showed terminal output",
                timestamp=datetime.now() - timedelta(days=1),
                agent_id=test_agent.id,
                user_id=test_user.id,
                canvas_context={
                    "canvas_type": "terminal",
                    "presentation_summary": "Agent displayed command output"
                }
            )
            retrieval_service.db.add(segment2)
            retrieval_service.db.commit()

            # Retrieve by orchestration canvas type
            result = await retrieval_service.retrieve_canvas_aware(
                agent_id=test_agent.id,
                query="workflow",
                canvas_type="orchestration",
                canvas_context_detail="summary"
            )

            # Should only return orchestration episode
            assert result["count"] == 1
            assert len(result["episodes"]) == 1
            assert result["episodes"][0]["id"] == "ep-orchestration-001"

        @pytest.mark.asyncio
        async def test_retrieve_without_canvas_type_filter_returns_all(
            self,
            retrieval_service: EpisodeRetrievalService,
            test_agent: AgentRegistry,
            test_user: User
        ):
            """Test that omitting canvas_type returns all canvas types"""
            # Create episodes with different canvas types
            for canvas_type in ["orchestration", "terminal", "sheets"]:
                episode = Episode(
                    id=f"ep-{canvas_type}-002",
                    agent_id=test_agent.id,
                    user_id=test_user.id,
                    title=f"{canvas_type.title()} Activity",
                    started_at=datetime.now() - timedelta(hours=2),
                    status="active"
                )
                retrieval_service.db.add(episode)

                segment = EpisodeSegment(
                    id=f"seg-{canvas_type}-002",
                    episode_id=episode.id,
                    sequence_number=1,
                    segment_type="canvas_interaction",
                    content=f"Agent used {canvas_type}",
                    timestamp=datetime.now() - timedelta(hours=2),
                    agent_id=test_agent.id,
                    user_id=test_user.id,
                    canvas_context={
                        "canvas_type": canvas_type,
                        "presentation_summary": f"Agent presented {canvas_type}"
                    }
                )
                retrieval_service.db.add(segment)

            retrieval_service.db.commit()

            # Retrieve without canvas_type filter
            result = await retrieval_service.retrieve_canvas_aware(
                agent_id=test_agent.id,
                query="agent activity",
                canvas_type=None,  # No filter
                canvas_context_detail="summary"
            )

            # Should return all episodes
            assert result["count"] == 3


    # ============================================================================
    # Progressive Detail Tests
    # ============================================================================

    class TestProgressiveDetailInRetrieval:
        """Test progressive detail levels in retrieval results"""

        @pytest.mark.asyncio
        async def test_summary_detail_level_in_retrieval(
            self,
            retrieval_service: EpisodeRetrievalService,
            test_agent: AgentRegistry,
            test_user: User
        ):
            """Test that summary detail level returns minimal context"""
            episode = Episode(
                id="ep-detail-001",
                agent_id=test_agent.id,
                user_id=test_user.id,
                title="Detail Test",
                started_at=datetime.now() - timedelta(hours=1),
                status="active"
            )
            retrieval_service.db.add(episode)

            segment = EpisodeSegment(
                id="seg-detail-001",
                episode_id=episode.id,
                sequence_number=1,
                segment_type="canvas_interaction",
                content="Agent presented form",
                timestamp=datetime.now() - timedelta(hours=1),
                agent_id=test_agent.id,
                user_id=test_user.id,
                canvas_context={
                    "canvas_type": "generic",
                    "presentation_summary": "Agent presented approval form with 3 fields",
                    "visual_elements": ["form", "text_input", "submit_button"],
                    "user_interaction": "User clicked approve",
                    "critical_data_points": {
                        "form_id": "form-123",
                        "approval": "approved"
                    }
                }
            )
            retrieval_service.db.add(segment)
            retrieval_service.db.commit()

            # Retrieve with summary level
            result = await retrieval_service.retrieve_canvas_aware(
                agent_id=test_agent.id,
                query="approval",
                canvas_context_detail="summary"
            )

            # Check that only summary is in response
            assert result["count"] == 1
            episode_data = result["episodes"][0]
            segment_data = episode_data["segments"][0]
            canvas_ctx = segment_data["canvas_context"]

            # Only presentation_summary should be present
            assert "presentation_summary" in canvas_ctx
            assert "visual_elements" not in canvas_ctx
            assert "critical_data_points" not in canvas_ctx

        @pytest.mark.asyncio
        async def test_standard_detail_level_includes_critical_data(
            self,
            retrieval_service: EpisodeRetrievalService,
            test_agent: AgentRegistry,
            test_user: User
        ):
            """Test that standard detail level includes critical_data_points"""
            episode = Episode(
                id="ep-detail-002",
                agent_id=test_agent.id,
                user_id=test_user.id,
                title="Standard Detail Test",
                started_at=datetime.now() - timedelta(hours=1),
                status="active"
            )
            retrieval_service.db.add(episode)

            segment = EpisodeSegment(
                id="seg-detail-002",
                episode_id=episode.id,
                sequence_number=1,
                segment_type="canvas_interaction",
                content="Agent presented revenue data",
                timestamp=datetime.now() - timedelta(hours=1),
                agent_id=test_agent.id,
                user_id=test_user.id,
                canvas_context={
                    "canvas_type": "sheets",
                    "presentation_summary": "Agent presented revenue chart",
                    "visual_elements": ["line_chart"],
                    "critical_data_points": {
                        "revenue": "$1.5M",
                        "amount": 1500000
                    }
                }
            )
            retrieval_service.db.add(segment)
            retrieval_service.db.commit()

            # Retrieve with standard level
            result = await retrieval_service.retrieve_canvas_aware(
                agent_id=test_agent.id,
                query="revenue",
                canvas_context_detail="standard"
            )

            # Check that critical_data_points is included
            assert result["count"] == 1
            segment_data = result["episodes"][0]["segments"][0]
            canvas_ctx = segment_data["canvas_context"]

            assert "presentation_summary" in canvas_ctx
            assert "critical_data_points" in canvas_ctx
            assert canvas_ctx["critical_data_points"]["revenue"] == "$1.5M"
            assert "visual_elements" not in canvas_ctx


    # ============================================================================
    # Business Data Filter Tests
    # ============================================================================

    class TestBusinessDataFilters:
        """Test business data filtering in canvas context"""

        @pytest.mark.asyncio
        async def test_filter_by_approval_status(
            self,
            retrieval_service: EpisodeRetrievalService,
            test_agent: AgentRegistry,
            test_user: User
        ):
            """Test filtering episodes by approval status in critical_data_points"""
            # Create approved episode
            episode1 = Episode(
                id="ep-approved-001",
                agent_id=test_agent.id,
                user_id=test_user.id,
                title="Approved Workflow",
                started_at=datetime.now() - timedelta(days=1),
                status="active"
            )
            retrieval_service.db.add(episode1)

            segment1 = EpisodeSegment(
                id="seg-approved-001",
                episode_id=episode1.id,
                sequence_number=1,
                segment_type="canvas_interaction",
                content="Workflow approved",
                timestamp=datetime.now() - timedelta(days=1),
                agent_id=test_agent.id,
                user_id=test_user.id,
                canvas_context={
                    "canvas_type": "orchestration",
                    "presentation_summary": "Agent presented approval form",
                    "critical_data_points": {
                        "workflow_id": "wf-001",
                        "approval_status": "approved"
                    }
                }
            )
            retrieval_service.db.add(segment1)

            # Create rejected episode
            episode2 = Episode(
                id="ep-rejected-001",
                agent_id=test_agent.id,
                user_id=test_user.id,
                title="Rejected Workflow",
                started_at=datetime.now() - timedelta(days=1),
                status="active"
            )
            retrieval_service.db.add(episode2)

            segment2 = EpisodeSegment(
                id="seg-rejected-001",
                episode_id=episode2.id,
                sequence_number=1,
                segment_type="canvas_interaction",
                content="Workflow rejected",
                timestamp=datetime.now() - timedelta(days=1),
                agent_id=test_agent.id,
                user_id=test_user.id,
                canvas_context={
                    "canvas_type": "orchestration",
                    "presentation_summary": "Agent presented approval form",
                    "critical_data_points": {
                        "workflow_id": "wf-002",
                        "approval_status": "rejected"
                    }
                }
            )
            retrieval_service.db.add(segment2)
            retrieval_service.db.commit()

            # Filter by approved status
            result = await retrieval_service.retrieve_by_business_data(
                agent_id=test_agent.id,
                business_filters={"approval_status": "approved"}
            )

            # Should only return approved episode
            assert result["count"] == 1
            assert result["episodes"][0]["id"] == "ep-approved-001"


    # ============================================================================
    # Coverage Tests
    # ============================================================================

    def test_canvas_aware_retrieval_coverage_contribution():
        """
        Verify that canvas-aware retrieval tests contribute to episodic memory coverage.
        This is a marker test for coverage reporting.
        """
        assert True, "Canvas-aware retrieval tests should increase episodic memory coverage"
    ```

    Run tests after creation:
    ```bash
    cd /Users/rushiparikh/projects/atom/backend
    pytest tests/test_canvas_aware_retrieval.py -v --cov=core.episode_retrieval_service --cov-report=term-missing
    ```
  </action>
  <verify>
    1. test_canvas_aware_retrieval.py created
    2. Canvas type filter tests exist (orchestration, terminal, sheets)
    3. Progressive detail tests exist (summary/standard/full)
    4. Business data filter tests exist (approval_status, revenue, etc.)
    5. Tests run without errors
  </verify>
  <done>
    Canvas-aware retrieval tests cover filtering, progressive detail, and business data queries.
  </done>
</task>

<task type="auto">
  <name>Run coverage report and verify 50%+ target</name>
  <files>backend/tests/</files>
  <action>
    Run comprehensive coverage report for episodic memory services with canvas context features.

    Execute coverage command:
    ```bash
    cd /Users/rushiparikh/projects/atom/backend

    # Run coverage for episodic memory services
    pytest tests/test_canvas_context_enrichment.py \
           tests/test_canvas_aware_retrieval.py \
           tests/test_episode_segmentation.py \
           tests/test_episode_retrieval.py \
           -v \
           --cov=core.episode_segmentation_service \
           --cov=core.episode_retrieval_service \
           --cov=core.episode_lifecycle_service \
           --cov-report=term-missing \
           --cov-report=html:htmlcov \
           --cov-report=json:coverage_reports/canvas_context_coverage.json
    ```

    After tests complete, check coverage percentage:
    ```bash
    # View coverage summary from JSON
    cat coverage_reports/canvas_context_coverage.json | python -m json.tool
    ```

    Verify episodic memory services reach 50%+ coverage target combined.

    Create coverage summary document if target not met:
    ```bash
    # Generate detailed missing coverage report
    pytest tests/test_canvas_context_enrichment.py \
           tests/test_canvas_aware_retrieval.py \
           --cov=core.episode_segmentation_service \
           --cov=core.episode_retrieval_service \
           --cov-report=term-missing:skip-covered \
           | tee coverage_reports/canvas_context_missing_coverage.txt
    ```

    Document results in a coverage summary:
    ```bash
    # Create coverage summary
    cat > coverage_reports/CANVAS_CONTEXT_COVERAGE_SUMMARY.md << 'EOF'
    # Canvas Context Coverage Summary - Phase 20 Plan 05

    **Date**: $(date +%Y-%m-%d)
    **Target**: 50%+ coverage for episodic memory services with canvas context

    ## Coverage Results

    ### Service Coverage
    - episode_segmentation_service.py: [COVERAGE]%
    - episode_retrieval_service.py: [COVERAGE]%
    - episode_lifecycle_service.py: [COVERAGE]%

    ### Canvas Context Features
    - Canvas context extraction: [COVERED/NOT COVERED]
    - Progressive detail levels: [COVERED/NOT COVERED]
    - Canvas type filtering: [COVERED/NOT COVERED]
    - Business data filtering: [COVERED/NOT COVERED]

    ### Test Counts
    - test_canvas_context_enrichment.py: [N] tests
    - test_canvas_aware_retrieval.py: [N] tests
    - Total canvas context tests: [N] tests

    ## Next Steps

    [If below 50%: Document additional tests needed]
    [If above 50%: Target achieved, proceed to Plan 06]
    EOF
    ```
  </action>
  <verify>
    1. Coverage report generated successfully
    2. Coverage JSON file created with results
    3. HTML coverage report available in htmlcov directory
    4. Coverage summary document created
    5. Target achievement status documented (50%+ met or not)
  </verify>
  <done>
    Coverage report generated and 50%+ target status verified for episodic memory canvas integration.
  </done>
</task>

</tasks>

<verification>
After completion, verify:
1. All test files pass (pytest exit code 0)
2. Coverage report shows episodic memory coverage percentage
3. 50%+ coverage target achieved or gap documented
4. All 7 canvas types have extraction tests
5. Progressive detail levels tested (summary/standard/full)
</verification>

<success_criteria>
1. test_canvas_context_enrichment.py created with 7+ canvas type tests
2. test_canvas_aware_retrieval.py created with filtering and detail tests
3. Coverage report shows episodic memory services at 50%+ coverage
4. All tests pass without errors
5. Coverage summary document exists with results
</success_criteria>

<output>
After completion, create `.planning/phases/20-canvas-ai-context/20-05-SUMMARY.md`
</output>
