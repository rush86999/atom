---
phase: 67-ci-cd-pipeline-fixes
plan: 03
type: execute
wave: 2
depends_on: [67-01]
files_modified:
  - .github/workflows/deploy.yml
  - backend/alembic/versions/
  - backend/core/health_routes.py
autonomous: false
user_setup:
  - service: kubernetes
    why: "Deployment target for staging and production environments"
    env_vars:
      - name: KUBECONFIG_STAGING
        source: "Kubernetes cluster configuration for staging environment (base64 encoded)"
      - name: KUBECONFIG_PRODUCTION
        source: "Kubernetes cluster configuration for production environment (base64 encoded)"
    dashboard_config:
      - task: "Create Kubernetes secrets for deployment"
        location: "kubectl create secret generic atom-deploy-secrets --from-literal=kubeconfig=$(cat ~/.kube/config | base64)"

must_haves:
  truths:
    - "Smoke tests include proper authentication and fail on 401 errors"
    - "Automatic rollback triggered on smoke test failure"
    - "Error rate thresholds configured (<1% staging, <0.1% production)"
    - "Database connectivity verified in smoke tests"
    - "GitHub issue created automatically on failed deployment"
    - "Slack notification sent on rollback with commit details"
  artifacts:
    - path: .github/workflows/deploy.yml
      provides: "Deploy workflow with automatic rollback and proper smoke tests"
      min_lines: 400
      exports: ["deploy-staging", "deploy-production", "rollback-staging", "rollback-production"]
    - path: backend/core/health_routes.py
      provides: "Health check endpoints for liveness, readiness, database connectivity"
      min_lines: 200
      exports: ["/health/live", "/health/ready", "/health/db"]
  key_links:
    - from: ".github/workflows/deploy.yml"
      to: "backend/core/health_routes.py"
      via: "Smoke test calls /health/db endpoint"
      pattern: "/health/db"
    - from: ".github/workflows/deploy.yml"
      to: "backend/alembic/versions/XXXX_create_smoke_test_user.py"
      via: "Smoke test authentication with smoke_test user credentials"
      pattern: "SMOKE_TEST_"
    - from: ".github/workflows/deploy.yml"
      to: "GitHub Issues API"
      via: "Automatic issue creation on deployment failure"
      pattern: "github.rest.issues.create"
---

<objective>
Harden deployment safety with authenticated smoke tests, automatic rollback on failure, proper error thresholds, and comprehensive health checks.

Purpose: Current deployment workflow has smoke tests that pass with 401 errors (missing authentication), no automatic rollback on smoke test failure, and incorrect error rate thresholds (5% instead of <0.1%). This plan fixes critical deployment safety gaps.

Output: Automatic rollback on smoke test failure, proper error thresholds (<1% staging, <0.1% production), database connectivity verification, GitHub issue creation on failure.
</objective>

<execution_context>
@/Users/rushiparikh/.claude/get-shit-done/workflows/execute-plan.md
@/Users/rushiparikh/.claude/get-shit-done/templates/summary.md

@.planning/phases/67-ci-cd-pipeline-fixes/67-RESEARCH.md
@.github/workflows/deploy.yml
@backend/core/health_routes.py
@backend/alembic/versions/XXXX_create_smoke_test_user.py (from 67-01)

# Depends on 67-01 for smoke test user creation
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Research Context
Phase 67 research identified deployment safety gaps:

**Smoke Test Issues** (lines 157-169 in deploy.yml):
- No authentication tokens (will fail with 401)
- Hardcoded URLs (`staging.atom.example.com`) not configured
- Only 2 endpoints tested (missing database connectivity)
- No rollback trigger on smoke test failure

**Error Rate Threshold Issues** (lines 299-302):
- Current: 5% threshold (`if (( $(echo "$error_rate > 0.05" | bc -l) ))`)
- Should be: <1% for staging, <0.1% for production
- 5% allows 500 errors per 10k requests (far too high)

**Monitoring Issues**:
- No Prometheus reachability validation before querying
- Hardcoded Prometheus URL (`prometheus.example.com`)
- No retry logic for failed Prometheus queries
- Missing Grafana dashboard auto-update on deployment

**Key Decision**: Implement automatic rollback on smoke test failure (no manual approval). Use progressive canary deployment (10% ‚Üí 50% ‚Üí 100% traffic) to minimize blast radius.

**Standard Stack**: kubectl for Kubernetes deployment, GitHub Actions for CI/CD, actions/github-script@v7 for GitHub API integration.
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add database connectivity health check endpoint</name>
  <files>backend/core/health_routes.py</files>
  <action>
    Extend backend/core/health_routes.py with database connectivity check:

    Add new endpoint after /health/ready (line ~100):

    ```python
    @router.get("/db")
    async def check_database_connectivity(
        db: Session = Depends(get_db)
) -> Dict[str, Any]:
        """
    Check database connectivity and query performance.

    Returns 200 if database is accessible and responsive.
    Returns 503 if database is unreachable or slow.
    """
    start_time = time.time()

    try:
        # Test database connection with simple query
        result = db.execute(text("SELECT 1"))
        result.fetchone()

        query_time = (time.time() - start_time) * 1000  # Convert to ms

        # Check connection pool status
        pool_status = {
            "size": engine.pool.size(),
            "checked_in": engine.pool.checkedin(),
            "checked_out": engine.pool.checkedout(),
            "overflow": engine.pool.overflow(),
            "max_overflow": engine.pool.max_overflow()
        }

        health_status = {
            "status": "healthy",
            "timestamp": datetime.utcnow().isoformat(),
            "database": {
                "connected": True,
                "query_time_ms": round(query_time, 2),
                "pool_status": pool_status
            }
        }

        # Warn if query time >100ms
        if query_time > 100:
            health_status["database"]["warning"] = f"Slow query ({query_time:.2f}ms)"

        return health_status

    except Exception as e:
        logger.error(f"Database health check failed: {e}")
        raise HTTPException(
            status_code=503,
            detail={
                "status": "unhealthy",
                "timestamp": datetime.utcnow().isoformat(),
                "database": {
                    "connected": False,
                    "error": str(e)
                }
            }
        )
    ```

    Add imports at top if not present:
    ```python
    from sqlalchemy import text
    from sqlalchemy.engine import Engine
    from core.database import engine  # Adjust import based on actual location
    ```

    Update /health/ready endpoint to include database check:
    ```python
    @router.get("/ready")
    async def check_readiness(
        db: Session = Depends(get_db)
) -> Dict[str, Any]:
        """
    Readiness probe - checks if service is ready to handle requests.

    Checks database connectivity and critical dependencies.
    """
        checks = {
            "status": "ready",
            "timestamp": datetime.utcnow().isoformat(),
            "checks": {}
        }

        # Check database connectivity
        try:
            result = db.execute(text("SELECT 1"))
            result.fetchone()
            checks["checks"]["database"] = {"status": "ok"}
        except Exception as e:
            checks["status"] =not_ready"
            checks["checks"]["database"] = {"status": "failed", "error": str(e)}

        # Check other critical services (Redis, LanceDB, etc.)
        # Add similar checks for other dependencies

        if checks["status"] == "not_ready":
            raise HTTPException(status_code=503, detail=checks)

        return checks
    ```
  </action>
  <verify>
    Run: pytest backend/tests/test_health_routes.py -v -k "test_database_connectivity" (test passes)
    Run: curl -f http://localhost:8000/health/db (returns 200 with query_time_ms)
    Run: curl -f http://localhost:8000/health/ready (returns 200 if DB connected)
  </verify>
  <done>
    Database connectivity health check endpoint added. Smoke tests can verify database is reachable after deployment. Readiness probe includes database check.
  </done>
</task>

<task type="auto">
  <name>Task 2: Update smoke tests with authentication and database connectivity</name>
  <files>.github/workflows/deploy.yml</files>
  <action>
    Update deploy-staging job smoke tests (lines 157-169) with proper authentication:

    ```yaml
      - name: Run smoke tests
        env:
          SMOKE_TEST_USERNAME: ${{ secrets.SMOKE_TEST_USERNAME }}
          SMOKE_TEST_PASSWORD: ${{ secrets.SMOKE_TEST_PASSWORD }}
          STAGING_URL: ${{ secrets.STAGING_URL }}
        run: |
          set -euo pipefail

          echo "=== Waiting for rollout to complete ==="
          export KUBECONFIG=kubeconfig
          kubectl rollout status deployment/atom --timeout=5m

          echo "=== Waiting for pods to be ready ==="
          kubectl wait --for=condition=ready pod -l app=atom --timeout=300s

          echo "=== Running smoke tests ==="

          # Get staging URL from secret or environment variable
          STAGING_URL="${STAGING_URL:-https://staging.atom.example.com}"
          echo "Testing against: $STAGING_URL"

          # Login and get auth token
          echo "Step 1: Authenticate smoke test user..."
          LOGIN_RESPONSE=$(curl -s -X POST "$STAGING_URL/api/auth/login" \
            -H "Content-Type: application/json" \
            -d "{\"username\":\"${SMOKE_TEST_USERNAME}\",\"password\":\"${SMOKE_TEST_PASSWORD}\"}")

          TOKEN=$(echo "$LOGIN_RESPONSE" | jq -r '.access_token')

          if [ "$TOKEN" == "null" ] || [ -z "$TOKEN" ]; then
            echo "‚ùå Smoke test authentication failed"
            echo "Response: $LOGIN_RESPONSE"
            exit 1
          fi
          echo "‚úÖ Authentication successful"

          # Test health endpoints (no auth required)
          echo "Step 2: Test health endpoints..."
          curl -f "$STAGING_URL/health/live" || { echo "‚ùå Liveness probe failed"; exit 1; }
          echo "‚úÖ Liveness probe passed"

          curl -f "$STAGING_URL/health/ready" || { echo "‚ùå Readiness probe failed"; exit 1; }
          echo "‚úÖ Readiness probe passed"

          # Test database connectivity
          curl -f "$STAGING_URL/health/db" || { echo "‚ùå Database connectivity check failed"; exit 1; }
          echo "‚úÖ Database connectivity verified"

          # Test authenticated agent execution endpoint
          echo "Step 3: Test authenticated API endpoints..."
          AGENT_RESPONSE=$(curl -s -X POST "$STAGING_URL/api/agents/execute" \
            -H "Authorization: Bearer $TOKEN" \
            -H "Content-Type: application/json" \
            -d '{"agent_id": "test", "query": "hello"}')

          # Check for errors in response
          if echo "$AGENT_RESPONSE" | jq -e '.error' > /dev/null; then
            echo "‚ùå Agent execution failed: $AGENT_RESPONSE"
            exit 1
          fi
          echo "‚úÖ Agent execution endpoint passed"

          # Test authenticated canvas presentation endpoint
          CANVAS_RESPONSE=$(curl -s -X POST "$STAGING_URL/api/canvas/present" \
            -H "Authorization: Bearer $TOKEN" \
            -H "Content-Type: application/json" \
            -d '{"canvas_type": "generic", "content": "smoke test"}')

          if echo "$CANVAS_RESPONSE" | jq -e '.error' > /dev/null; then
            echo "‚ùå Canvas presentation failed: $CANVAS_RESPONSE"
            exit 1
          fi
          echo "‚úÖ Canvas presentation endpoint passed"

          # Test skills endpoint (coverage for additional API routes)
          echo "Step 4: Test skills endpoint..."
          SKILLS_RESPONSE=$(curl -s -X GET "$STAGING_URL/api/skills" \
            -H "Authorization: Bearer $TOKEN")

          if echo "$SKILLS_RESPONSE" | jq -e '.error' > /dev/null; then
            echo "‚ùå Skills endpoint failed: $SKILLS_RESPONSE"
            exit 1
          fi
          echo "‚úÖ Skills endpoint passed"

          echo "=== ‚úÖ All smoke tests passed ==="
    ```

    **Key improvements**:
    - Added authentication via smoke_test user credentials
    - Added STAGING_URL environment variable for flexible configuration
    - Added database connectivity check (/health/db endpoint)
    - Added skills endpoint test (broader API coverage)
    - Removed `|| true` (fail fast on any smoke test failure)
    - Added detailed error reporting with curl responses
    - Added step-by-step progress logging
  </action>
  <verify>
    Run: grep "SMOKE_TEST_USERNAME" .github/workflows/deploy.yml returns match
    Run: grep "/health/db" .github/workflows/deploy.yml returns match
    Run: grep -c "\-H \"Authorization: Bearer" .github/workflows/deploy.yml returns >= 3
  </verify>
  <done>
    Smoke tests updated with authentication, database connectivity check, and comprehensive endpoint coverage. Tests fail fast on any error (no silent failures).
  </done>
</task>

<task type="auto">
  <name>Task 3: Implement automatic rollback on smoke test failure</name>
  <files>.github/workflows/deploy.yml</files>
  <action>
    Add automatic rollback step after smoke tests in deploy-staging job (after line 230):

    ```yaml
      - name: Automatic rollback on smoke test failure
        if: failure()
        run: |
          set -euo pipefail
          export KUBECONFIG=kubeconfig

          echo "=== üö® Smoke tests failed - initiating automatic rollback ==="

          # Rollback to previous deployment
          echo "Step 1: Rolling back deployment..."
          kubectl rollout undo deployment/atom

          # Wait for rollback to complete
          echo "Step 2: Waiting for rollback to complete..."
          kubectl rollout status deployment/atom --timeout=5m

          # Verify rollback succeeded
          echo "Step 3: Verifying rollback..."
          kubectl rollout history deployment/atom --revision=0

          echo "=== ‚úÖ Rollback completed ==="

      - name: Send rollback notification
        if: failure()
        uses: actions/github-script@v7
        with:
          script: |
            const slackWebhook = '${{ secrets.SLACK_WEBHOOK_URL }}';

            // Send Slack notification
            if (slackWebhook) {
              await fetch(slackWebhook, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                  text: 'üö® Deployment rolled back due to smoke test failure',
                  blocks: [
                    {
                      type: 'section',
                      text: {
                        type: 'mrkdwn',
                        text: '*Automatic Rollback Triggered*\n' +
                              '*Commit:* `${{ github.sha }}`\n' +
                              '*Author:* ${{ github.actor }}\n' +
                              '*Environment:* Staging\n' +
                              '*Reason:* Smoke test failure\n' +
                              '*Action:* `kubectl rollout undo deployment/atom`\n' +
                              '*Workflow:* ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}'
                      }
                    },
                    {
                      type: 'section',
                      text: {
                        type: 'mrkdwn',
                        text: '_Please investigate the failure and fix the issue before retrying deployment._'
                      }
                    }
                  ]
                })
              });
            }

      - name: Create GitHub issue for investigation
        if: failure()
        uses: actions/github-script@v7
        with:
          script: |
            const issue = await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: 'üö® Deployment Rollback - Smoke Test Failed (Staging)',
              body: `
              ## Automatic Rollback Triggered

              **Commit:** ${{ github.sha }}
              **Author:** ${{ github.actor }}
              **Environment:** Staging
              **Workflow Run:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}

              ### Rollback Details
              - **Trigger:** Smoke test failure
              - **Action:** \`kubectl rollout undo deployment/atom\`
              - **Timestamp:** ${new Date().toISOString()}

              ### Next Steps
              1. Review smoke test failure logs in workflow run
              2. Identify root cause of failure
              3. Fix the issue in a new commit
              4. Re-run deployment after fix verified

              ### Relevant Files
              - Workflow: \`.github/workflows/deploy.yml\`
              - Health Routes: \`backend/core/health_routes.py\`
              - Smoke Test User: \`backend/alembic/versions/XXXX_create_smoke_test_user.py\`

              /assign @${{ github.actor }}
              /label "deployment"
              /label "rollback"
              /label "bug"
              `,
              labels: ['deployment', 'rollback', 'bug', 'staging']
            });

            console.log('Created issue:', issue.data.html_url);
    ```

    **Key improvements**:
    - Automatic rollback triggered on smoke test failure (no manual approval)
    - kubectl rollout undo command reverts to previous deployment
    - Rollback status wait ensures deployment is stable
    - Slack notification sent with full deployment context
    - GitHub issue created automatically with investigation details
    - Issue assigned to deploy author with relevant labels
  </action>
  <verify>
    Run: grep "Automatic rollback" .github/workflows/deploy.yml returns match
    Run: grep "kubectl rollout undo" .github/workflows/deploy.yml returns match
    Run: grep "github.rest.issues.create" .github/workflows/deploy.yml returns match
  </verify>
  <done>
    Automatic rollback implemented on smoke test failure. Rollback executes immediately, Slack notification sent, GitHub issue created for investigation.
  </done>
</task>

<task type="auto">
  <name>Task 4: Configure proper error rate thresholds and monitoring</name>
  <files>.github/workflows/deploy.yml</files>
  <action>
    Update deploy-staging job monitoring check (lines 285-307) with proper thresholds:

    ```yaml
      - name: Monitor deployment metrics
        run: |
          set -euo pipefail

          echo "=== Monitoring deployment metrics ==="

          # Wait for metrics to be available (30 second delay)
          echo "Waiting for Prometheus metrics to be available..."
          sleep 30

          # Configure Prometheus URL
          PROMETHEUS_URL="${{ secrets.PROMETHEUS_URL }}"
          if [ -z "$PROMETHEUS_URL" ]; then
            echo "‚ö†Ô∏è PROMETHEUS_URL not set, skipping metrics check"
            exit 0
          fi

          echo "Querying Prometheus at: $PROMETHEUS_URL"

          # Query error rate (errors per second over last 5 minutes)
          ERROR_RATE_QUERY='sum(rate(http_requests_total{status=~"5.."}[5m])) / sum(rate(http_requests_total[5m])) * 100'

          ERROR_RATE=$(curl -s -G "$PROMETHEUS_URL/api/v1/query" \
            --data-urlencode "query=$ERROR_RATE_QUERY" \
            | jq -r '.data.result[0].value[1] // "0"')

          echo "Error rate: $ERROR_RATE%"

          # Check staging threshold (<1% error rate)
          STAGING_THRESHOLD=1.0
          if (( $(echo "$ERROR_RATE > $STAGING_THRESHOLD" | bc -l) )); then
            echo "‚ùå Error rate too high: $ERROR_RATE% (threshold: $STAGING_THRESHOLD%)"

            # Send alert to Slack
            curl -X POST "${{ secrets.SLACK_WEBHOOK_URL }}" \
              -H 'Content-Type: application/json' \
              -d "{
                \"text\": \"‚ö†Ô∏è Staging deployment error rate high\",
                \"blocks\": [{
                  \"type\": \"section\",
                  \"text\": {
                    \"type\": \"mrkdwn\",
                    \"text\": \"*High Error Rate Detected*\\n*Error Rate:* $ERROR_RATE%\\n*Threshold:* $STAGING_THRESHOLD%\\n*Commit:* ${{ github.sha }}\"
                  }
                }]
              }"

            # Don't fail deployment, just warn
            echo "‚ö†Ô∏è Warning: Error rate exceeds threshold but deployment continues"
          else
            echo "‚úÖ Error rate within acceptable range: $ERROR_RATE% (threshold: $STAGING_THRESHOLD%)"
          fi

          # Query latency (P95 over last 5 minutes)
          LATENCY_QUERY='histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))'

          LATENCY_P95=$(curl -s -G "$PROMETHEUS_URL/api/v1/query" \
            --data-urlencode "query=$LATENCY_QUERY" \
            | jq -r '.data.result[0].value[1] // "0"')

          LATENCY_MS=$(echo "$LATENCY_P95 * 1000" | bc)

          echo "P95 Latency: ${LATENCY_MS}ms"

          # Check latency threshold (<500ms for staging)
          LATENCY_THRESHOLD=500
          if (( $(echo "$LATENCY_MS > $LATENCY_THRESHOLD" | bc -l) )); then
            echo "‚ö†Ô∏è Latency high: ${LATENCY_MS}ms (threshold: ${LATENCY_THRESHOLD}ms)"
          else
            echo "‚úÖ Latency within acceptable range: ${LATENCY_MS}ms (threshold: ${LATENCY_THRESHOLD}ms)"
          fi

          echo "=== ‚úÖ Monitoring check complete ==="
    ```

    **Add production deployment job with stricter thresholds** (after deploy-staging job):
    ```yaml
  deploy-production:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: [test, build, deploy-staging]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    environment:
      name: production
      url: https://atom.example.com

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up kubectl
        uses: azure/setup-kubectl@v4

      - name: Configure kubectl
        run: |
          echo "${{ secrets.KUBECONFIG_PRODUCTION }}" | base64 -d > kubeconfig
          export KUBECONFIG=kubeconfig

      - name: Update deployment image
        run: |
          export KUBECONFIG=kubeconfig
          kubectl set image deployment/atom atom=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }}

      - name: Wait for rollout
        run: |
          export KUBECONFIG=kubeconfig
          kubectl rollout status deployment/atom --timeout=5m

      - name: Run smoke tests (production)
        env:
          SMOKE_TEST_USERNAME: ${{ secrets.SMOKE_TEST_USERNAME }}
          SMOKE_TEST_PASSWORD: ${{ secrets.SMOKE_TEST_PASSWORD }}
          PRODUCTION_URL: ${{ secrets.PRODUCTION_URL }}
        run: |
          # Same smoke test logic as staging, but using PRODUCTION_URL
          # Copy smoke test steps from deploy-staging job
          # (Use PRODUCTION_URL instead of STAGING_URL)

      - name: Monitor production metrics (stricter thresholds)
        run: |
          # Same monitoring logic as staging, but with:
          # - Error rate threshold: <0.1% (not 1%)
          # - Latency threshold: <200ms P95 (not 500ms)
          # - Automatic rollback on threshold breach

      - name: Automatic rollback on production failure
        if: failure()
        run: |
          # Same rollback logic as staging
          # But sends urgent Slack notification: üö® PRODUCTION ROLLBACK üö®
    ```

    **Key improvements**:
    - Error rate threshold: 1% for staging, 0.1% for production (was 5%)
    - Latency threshold: 500ms P95 for staging, 200ms P95 for production
    - PROMETHEUS_URL from secrets (no hardcoded URL)
    - Graceful handling when PROMETHEUS_URL not set (skip check, don't fail)
    - Query error rate and latency from Prometheus HTTP API
    - Separate production deployment job with stricter thresholds
  </action>
  <verify>
    Run: grep "STAGING_THRESHOLD=1.0" .github/workflows/deploy.yml returns match
    Run: grep "LATENCY_THRESHOLD=500" .github/workflows/deploy.yml returns match
    Run: grep "PROMETHEUS_URL" .github/workflows/deploy.yml returns >= 3
  </verify>
  <done>
    Error rate thresholds configured: <1% staging, <0.1% production (was 5%). Latency thresholds: 500ms P95 staging, 200ms P95 production. Prometheus URL from secrets, graceful handling when not set.
  </done>
</task>

<task type="checkpoint:human-verify">
  <name>Task 5: Human verification of deployment safety</name>
  <what-built>Deployment safety hardening with authenticated smoke tests, automatic rollback, proper error thresholds, and database connectivity verification</what-built>
  <how-to-verify>
    1. **Test Smoke Test Authentication**:
           ```bash
           # Deploy to staging
           gh workflow run deploy.yml --ref main
           gh run watch --interval 10

           # Check smoke test output for "‚úÖ Authentication successful"
           gh run view --log | grep "Authentication"
           ```

    2. **Test Automatic Rollback**:
           ```bash
           # Intentionally break smoke test (e.g., delete smoke_test user)
           # Deploy to staging and verify automatic rollback triggers
           # Check Slack notification received
           # Check GitHub issue created with "rollback" label
           ```

    3. **Verify Database Connectivity Check**:
           ```bash
           # Deploy to staging
           curl https://staging.atom.example.com/health/db
           # Should return: {"status": "healthy", "database": {"connected": true, "query_time_ms": 5.23}}
           ```

    4. **Verify Error Rate Thresholds**:
           ```bash
           # Check Prometheus query in workflow log
           gh run view --log | grep "Error rate:"
           # Should show: "Error rate: 0.5%" (below 1% threshold)
           ```

    **Expected Results**:
    - Smoke tests authenticate successfully
    - Database connectivity check passes
    - Error rate <1% (staging) or <0.1% (production)
    - Rollback triggers automatically on smoke test failure
    - GitHub issue created on rollback
    - Slack notification sent on rollback
  </how-to-verify>
  <resume-signal>Type "approved" if all smoke tests pass and rollback works, or describe issues to fix</resume-signal>
</task>

</tasks>

<verification>
Overall phase verification steps:

1. **Smoke Test Authentication Verification**:
   ```bash
   # Verify smoke test user exists
   psql -c "SELECT username, is_smoke_test_user FROM users WHERE username='smoke_test'"

   # Test smoke test login
   curl -X POST https://staging.atom.example.com/api/auth/login \
     -d '{"username":"smoke_test","password":"smoke_test_password_change_in_prod"}' \
     | jq -r '.access_token'

   # Verify token works
   TOKEN="..."
   curl -H "Authorization: Bearer $TOKEN" https://staging.atom.example.com/api/agents/execute \
     -d '{"agent_id": "test", "query": "hello"}'
   ```

2. **Automatic Rollback Verification**:
   ```bash
   # Trigger intentional rollback (break smoke test)
   # Deploy to staging and check workflow log
   gh run view --log | grep "Automatic rollback"

   # Verify rollback commands executed
   gh run view --log | grep "kubectl rollout undo"

   # Verify Slack notification sent (check Slack channel)
   # Verify GitHub issue created (check repository Issues tab)
   ```

3. **Database Connectivity Verification**:
   ```bash
   # Test /health/db endpoint
   curl https://staging.atom.example.com/health/db | jq

   # Expected output:
   # {
   #   "status": "healthy",
   #   "database": {
   #     "connected": true,
   #     "query_time_ms": 5.23,
   #     "pool_status": {...}
   #   }
   # }
   ```

4. **Error Rate Thresholds Verification**:
   ```bash
   # Verify thresholds in workflow file
   grep "STAGING_THRESHOLD=" .github/workflows/deploy.yml
   grep "LATENCY_THRESHOLD=" .github/workflows/deploy.yml

   # Verify Prometheus query in workflow log
   gh run view --log | grep "Error rate:"
   gh run view --log | grep "P95 Latency:"
   ```

**Success Criteria**:
- [ ] Smoke tests include authentication and fail on 401 errors
- [ ] Automatic rollback triggered on smoke test failure
- [ ] Error rate thresholds: <1% staging, <0.1% production (was 5%)
- [ ] Database connectivity verified in smoke tests
- [ ] GitHub issue created automatically on rollback
- [ ] Slack notification sent on rollback with commit details
- [ ] Prometheus URL from secrets (not hardcoded)
</verification>

<success_criteria>
**Deployment Safety Hardening Complete When:**

1. **Smoke Test Authentication**: All smoke tests use proper auth tokens (no more 401 false positives)
2. **Automatic Rollback**: Rollback triggers on smoke test failure without manual approval
3. **Error Rate Thresholds**: <1% staging, <0.1% production (down from 5%)
4. **Database Connectivity**: /health/db endpoint tested in smoke tests
5. **Issue Creation**: GitHub issue created on rollback with investigation details
6. **Slack Notifications**: Rollback alerts sent with full deployment context
7. **Prometheus Integration**: Graceful handling when PROMETHEUS_URL not set

**Measurable Outcomes**:
- Smoke test failure rate: 0% (all tests pass with authentication)
- Rollback time: <60 seconds (automatic trigger + kubectl undo)
- Error rate in staging: <1% (90% reduction from 5% baseline)
- Error rate in production: <0.1% (98% reduction from 5% baseline)
- Database connectivity check: <50ms response time
</success_criteria>

<output>
After completion, create `.planning/phases/67-ci-cd-pipeline-fixes/67-03-SUMMARY.md` with:
- Smoke test authentication verification results (before vs after)
- Automatic rollback test results (rollback time, Slack notification, GitHub issue)
- Error rate metrics comparison (staging vs production, before vs after thresholds)
- Database connectivity health check response times
- Prometheus monitoring query examples
- Rollback procedure documentation
</output>
