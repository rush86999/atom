---
phase: 67-ci-cd-pipeline-fixes
plan: 04
type: execute
wave: 2
depends_on: [67-01]
files_modified:
  - .github/workflows/deploy.yml
  - backend/monitoring.py
  - .prometheus/alerts.yml
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Prometheus queries validated before use in deployment workflow"
    - "Grafana dashboard auto-updates on deployment"
    - "Progressive canary deployment strategy configured (10% → 50% → 100%)"
    - "Deployment metrics tracked (success rate, rollback rate, deployment frequency)"
    - "Prometheus alerting rules configured with proper thresholds"
    - "Monitoring check fails gracefully when Prometheus unreachable"
  artifacts:
    - path: .github/workflows/deploy.yml
      provides: "Deploy workflow with Prometheus validation, Grafana dashboard update, and canary deployment"
      min_lines: 450
      exports: ["deploy-staging", "deploy-production", "monitoring-check", "grafana-update"]
    - path: backend/monitoring.py
      provides: "Extended monitoring with deployment metrics (success rate, rollback rate, deployment frequency)"
      min_lines: 300
      exports: ["deployment_success_rate", "deployment_rollback_rate", "deployment_frequency"]
    - path: .prometheus/alerts.yml
      provides: "Prometheus alerting rules for deployment monitoring (error rate, latency, rollback detection)"
      min_lines: 150
      contains: "AlertmanagerConfig"
  key_links:
    - from: ".github/workflows/deploy.yml"
      to: "backend/monitoring.py"
      via: "Deployment metrics exported to Prometheus"
      pattern: "deployment_.*_total"
    - from: ".github/workflows/deploy.yml"
      to: ".prometheus/alerts.yml"
      via: "Prometheus alerting rules loaded on deployment"
      pattern: "promtool check rules"
    - from: ".github/workflows/deploy.yml"
      to: "Grafana API"
      via: "Dashboard auto-update on deployment"
      pattern: "grafana_api.*dashboard"
---

<objective>
Enhance monitoring and alerting with Prometheus query validation, Grafana dashboard auto-update, progressive canary deployment, and deployment metrics tracking.

Purpose: Current deploy.yml has hardcoded Prometheus URLs, no query validation before deployment, and no Grafana dashboard updates. This plan adds monitoring safety checks, automatic dashboard updates, and canary deployment strategy.

Output: Prometheus queries validated before use, Grafana dashboards auto-update on deployment, canary deployment strategy (10% → 50% → 100%), deployment metrics tracked.
</objective>

<execution_context>
@/Users/rushiparikh/.claude/get-shit-done/workflows/execute-plan.md
@/Users/rushiparikh/.claude/get-shit-done/templates/summary.md

@.planning/phases/67-ci-cd-pipeline-fixes/67-RESEARCH.md
@.github/workflows/deploy.yml
@backend/monitoring.py
@.prometheus/alerts.yml

# Depends on 67-01 for stable test infrastructure (monitoring needs passing tests)
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Research Context
Phase 67 research identified monitoring & alerting weaknesses:

**Current Issues** (from deploy.yml lines 285-307):
- Hardcoded Prometheus URL (`prometheus.example.com`) - not configurable
- No validation that Prometheus is reachable before querying
- No retry logic for failed Prometheus queries
- Missing Grafana dashboard auto-update on deployment
- No progressive canary deployment (100% traffic immediately)

**Monitoring Gaps**:
- No deployment metrics tracking (success rate, rollback rate, frequency)
- No Prometheus alerting rules for deployment anomalies
- No Grafana dashboard versioning or rollback capability
- No deployment visibility (are deployments succeeding?)

**Key Decision**: Implement progressive canary deployment (10% → 50% → 100% traffic) to minimize blast radius. Add Grafana dashboard auto-update via API calls. Validate Prometheus queries before deployment (fail gracefully if Prometheus unavailable).

**Standard Stack**: Prometheus for metrics collection, Alertmanager for alert routing, Grafana for visualization, promtool for rule validation.
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add deployment metrics to monitoring.py</name>
  <files>backend/monitoring.py</files>
  <action>
    Extend backend/monitoring.py with deployment metrics:

    Add new metrics after existing metrics (line ~100):

    ```python
    # Deployment metrics
    from prometheus_client import Counter, Histogram, Gauge

    # Track deployment attempts and outcomes
    deployment_total = Counter(
        'deployment_total',
        'Total number of deployments',
        ['environment', 'status']  # status: success, failed, rolled_back
    )

    deployment_duration_seconds = Histogram(
        'deployment_duration_seconds',
        'Deployment duration in seconds',
        ['environment'],
        buckets=[60, 120, 300, 600, 1800, 3600]  # 1min, 2min, 5min, 10min, 30min, 60min
    )

    deployment_rollback_total = Counter(
        'deployment_rollback_total',
        'Total number of deployment rollbacks',
        ['environment', 'reason']  # reason: smoke_test_failed, high_error_rate, manual
    )

    # Track deployment frequency (deployments per hour)
    deployment_frequency = Gauge(
        'deployment_frequency',
        'Deployment frequency (deployments per hour)',
        ['environment']
    )

    # Track canary deployment progress
    canary_traffic_percentage = Gauge(
        'canary_traffic_percentage',
        'Current canary traffic percentage',
        ['environment', 'deployment_id']
    )

    # Track smoke test results
    smoke_test_total = Counter(
        'smoke_test_total',
        'Total number of smoke tests run',
        ['environment', 'result']  # result: passed, failed
    )

    smoke_test_duration_seconds = Histogram(
        'smoke_test_duration_seconds',
        'Smoke test duration in seconds',
        ['environment'],
        buckets=[10, 30, 60, 120, 300]  # 10s, 30s, 1min, 2min, 5min
    )

    # Track Prometheus query success rate
    prometheus_query_total = Counter(
        'prometheus_query_total',
        'Total number of Prometheus queries',
        ['workflow', 'result']  # result: success, failed, timeout
    )

    prometheus_query_duration_seconds = Histogram(
        'prometheus_query_duration_seconds',
        'Prometheus query duration in seconds',
        ['workflow'],
        buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0]  # 100ms, 500ms, 1s, 2s, 5s, 10s
    )
    ```

    Add helper functions for metric tracking:

    ```python
    import time
    from contextlib import contextmanager

    @contextmanager
    def track_deployment(environment: str):
        """
    Context manager to track deployment metrics.
    Usage:
        with track_deployment('staging'):
            # deployment logic here
            pass
    """
        start_time = time.time()
        try:
            yield
            # Deployment succeeded
            deployment_total.labels(environment=environment, status='success').inc()
        except Exception as e:
            # Deployment failed
            deployment_total.labels(environment=environment, status='failed').inc()
            raise
        finally:
            # Record deployment duration
            duration = time.time() - start_time
            deployment_duration_seconds.labels(environment=environment).observe(duration)

    @contextmanager
    def track_smoke_test(environment: str):
        """
    Context manager to track smoke test metrics.
    Usage:
        with track_smoke_test('staging'):
            # smoke test logic here
            pass
    """
        start_time = time.time()
        try:
            yield
            # Smoke test passed
            smoke_test_total.labels(environment=environment, result='passed').inc()
        except Exception as e:
            # Smoke test failed
            smoke_test_total.labels(environment=environment, result='failed').inc()
            raise
        finally:
            # Record smoke test duration
            duration = time.time() - start_time
            smoke_test_duration_seconds.labels(environment=environment).observe(duration)

    def record_rollback(environment: str, reason: str):
        """Record a deployment rollback event."""
        deployment_rollback_total.labels(environment=environment, reason=reason).inc()

    def update_canary_traffic(environment: str, deployment_id: str, percentage: int):
        """Update canary traffic percentage."""
        canary_traffic_percentage.labels(
            environment=environment,
            deployment_id=deployment_id
        ).set(percentage)

    def record_prometheus_query(workflow: str, success: bool, duration: float):
        """Record a Prometheus query attempt."""
        result = 'success' if success else 'failed'
        prometheus_query_total.labels(workflow=workflow, result=result).inc()
        prometheus_query_duration_seconds.labels(workflow=workflow).observe(duration)
    ```

    Update existing metrics initialization if needed:
    ```python
    def initialize_metrics():
        """Initialize all Prometheus metrics."""
        # Start Prometheus HTTP server (if not already started)
        try:
            start_http_server(8001)  # Metrics on port 8001
            logger.info("Prometheus metrics server started on port 8001")
        except OSError as e:
            logger.warning(f"Prometheus metrics server already running: {e}")
    ```
  </action>
  <verify>
    Run: grep -c "deployment_" backend/monitoring.py returns >= 5
    Run: pytest backend/tests/test_monitoring.py -v -k "deployment" (tests pass)
    Run: curl http://localhost:8001/metrics | grep deployment_total (metric exposed)
  </verify>
  <done>
    Deployment metrics added to monitoring.py with counters for deployments, rollbacks, smoke tests, and Prometheus queries. Histograms track deployment and smoke test durations. Context managers simplify metric tracking.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create Prometheus alerting rules for deployment monitoring</name>
  <files>.prometheus/alerts.yml</files>
  <action>
    Create .prometheus/alerts.yml with deployment monitoring rules:

    ```yaml
    # Prometheus Alerting Rules for Atom Deployment Monitoring
    # Source: https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/
    #
    # Load these rules in Prometheus:
    #   promtool check rules .prometheus/alerts.yml
    #   prometheus --config.file=prometheus.yml --web.enable-lifecycle
    #   curl -X POST http://prometheus:9090/-/reload

    groups:
      - name: deployment_alerts
        interval: 30s
        rules:
          # High deployment error rate
          - alert: DeploymentHighErrorRate
            expr: |
              (
                sum(rate(deployment_total{status="failed"}[5m])) /
                sum(rate(deployment_total[5m]))
              ) > 0.1
            for: 2m
            labels:
              severity: warning
              environment: "{{ $labels.environment }}"
            annotations:
              summary: "High deployment error rate detected"
              description: "Deployment error rate is {{ $value | humanizePercentage }} (threshold: 10%) in environment {{ $labels.environment }}"

          # Deployment rollback detected
          - alert: DeploymentRollbackDetected
            expr: increase(deployment_rollback_total[5m]) > 0
            for: 0m
            labels:
              severity: critical
              environment: "{{ $labels.environment }}"
            annotations:
              summary: "Deployment rollback detected"
              description: "Deployment rollback triggered in {{ $labels.environment }}. Reason: {{ $labels.reason }}"

          # Smoke test failures
          - alert: SmokeTestFailing
            expr: |
              (
                sum(rate(smoke_test_total{result="failed"}[5m])) /
                sum(rate(smoke_test_total[5m]))
              ) > 0.05
            for: 3m
            labels:
              severity: warning
              environment: "{{ $labels.environment }}"
            annotations:
              summary: "Smoke test failure rate high"
              description: "Smoke test failure rate is {{ $value | humanizePercentage }} (threshold: 5%) in {{ $labels.environment }}"

          # High application error rate (staging)
          - alert: HighErrorRateStaging
            expr: |
              (
                sum(rate(http_requests_total{status=~"5..", environment="staging"}[5m])) /
                sum(rate(http_requests_total{environment="staging"}[5m]))
              ) > 0.01
            for: 2m
            labels:
              severity: warning
              environment: staging
            annotations:
              summary: "High error rate in staging"
              description: "Error rate is {{ $value | humanizePercentage }} (threshold: 1%) in staging"

          # High application error rate (production)
          - alert: HighErrorRateProduction
            expr: |
              (
                sum(rate(http_requests_total{status=~"5..", environment="production"}[5m])) /
                sum(rate(http_requests_total{environment="production"}[5m]))
              ) > 0.001
            for: 1m
            labels:
              severity: critical
              environment: production
            annotations:
              summary: "Critical error rate in production"
              description: "Error rate is {{ $value | humanizePercentage }} (threshold: 0.1%) in production"

          # High latency P95 (staging)
          - alert: HighLatencyStaging
            expr: |
              histogram_quantile(0.95,
                sum(rate(http_request_duration_seconds_bucket{environment="staging"}[5m])) by (le)
              ) > 0.5
            for: 5m
            labels:
              severity: warning
              environment: staging
            annotations:
              summary: "High latency in staging"
              description: "P95 latency is {{ $value | humanizeDuration }} (threshold: 500ms) in staging"

          # High latency P95 (production)
          - alert: HighLatencyProduction
            expr: |
              histogram_quantile(0.95,
                sum(rate(http_request_duration_seconds_bucket{environment="production"}[5m])) by (le)
              ) > 0.2
            for: 3m
            labels:
              severity: critical
              environment: production
            annotations:
              summary: "Critical latency in production"
              description: "P95 latency is {{ $value | humanizeDuration }} (threshold: 200ms) in production"

          # Deployment frequency anomaly (too many deployments)
          - alert: DeploymentFrequencyAnomaly
            expr: |
              (
                sum(rate(deployment_total[1h])) by (environment)
              ) > 2
            for: 5m
            labels:
              severity: info
              environment: "{{ $labels.environment }}"
            annotations:
              summary: "High deployment frequency detected"
              description: "Deployment frequency is {{ $value | humanize }} deployments/hour in {{ $labels.environment }} (threshold: 2/hour)"

          # Prometheus query failures (monitoring check failing)
          - alert: PrometheusQueryFailing
            expr: |
              (
                sum(rate(prometheus_query_total{result="failed"}[5m])) /
                sum(rate(prometheus_query_total[5m]))
              ) > 0.1
            for: 5m
            labels:
              severity: warning
              workflow: "{{ $labels.workflow }}"
            annotations:
              summary: "Prometheus query failure rate high"
              description: "Prometheus query failure rate is {{ $value | humanizePercentage }} in workflow {{ $labels.workflow }} (threshold: 10%)"
    ```

    Add validation script to test rules:
    ```bash
    #!/bin/bash
    # .prometheus/validate-alerts.sh

    echo "=== Validating Prometheus alerting rules ==="

    # Check if promtool is installed
    if ! command -v promtool &> /dev/null; then
      echo "❌ promtool not found. Install Prometheus CLI:"
      echo "   macOS: brew install prometheus"
      echo "   Linux: wget https://github.com/prometheus/prometheus/releases/download/v2.45.0/prometheus-2.45.0.linux-amd64.tar.gz"
      exit 1
    fi

    # Validate rules syntax
    promtool check rules .prometheus/alerts.yml

    if [ $? -eq 0 ]; then
      echo "✅ Prometheus alerting rules are valid"
    else
      echo "❌ Prometheus alerting rules have syntax errors"
      exit 1
    fi

    # Check rule count
    ALERT_COUNT=$(grep -c "alert:" .prometheus/alerts.yml)
    echo "Total alerts configured: $ALERT_COUNT"

    # List all alerts
    echo "=== Configured Alerts ==="
    grep "alert:" .prometheus/alerts.yml | sed 's/.*alert: //' | sed 's/ .*//' | sort
    ```

    Make script executable:
    ```bash
    chmod +x .prometheus/validate-alerts.sh
    ```
  </action>
  <verify>
    Run: .prometheus/validate-alerts.sh (validates rules and lists alerts)
    Run: grep -c "alert:" .prometheus/alerts.yml returns >= 9
    Run: promtool check rules .prometheus/alerts.yml (no errors)
  </verify>
  <done>
    Prometheus alerting rules created with 9+ alerts covering deployments, rollbacks, smoke tests, error rates, latency, and monitoring health. Validation script checks syntax before deployment.
  </done>
</task>

<task type="auto">
  <name>Task 3: Add Prometheus query validation to deploy workflow</name>
  <files>.github/workflows/deploy.yml</files>
  <action>
    Add Prometheus validation step to deploy-staging job (before monitoring check, line ~280):

    ```yaml
      - name: Validate Prometheus connectivity and queries
        id: prometheus_validation
        run: |
          set -euo pipefail

          echo "=== Validating Prometheus Configuration ==="

          # Configure Prometheus URL
          PROMETHEUS_URL="${{ secrets.PROMETHEUS_URL }}"

          if [ -z "$PROMETHEUS_URL" ]; then
            echo "⚠️ PROMETHEUS_URL not set"
            echo "Monitoring check will be skipped (graceful degradation)"
            echo "prometheus_reachable=false" >> $GITHUB_OUTPUT
            exit 0
          fi

          echo "Prometheus URL: $PROMETHEUS_URL"

          # Test Prometheus connectivity
          echo "Step 1: Testing Prometheus connectivity..."
          PROMETHEUS_START=$(date +%s)

          HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" "$PROMETHEUS_URL/-/healthy" || echo "000")

          PROMETHEUS_DURATION=$(($(date +%s) - PROMETHEUS_START))

          if [ "$HTTP_CODE" != "200" ]; then
            echo "❌ Prometheus unreachable (HTTP $HTTP_CODE)"
            echo "Monitoring check will be skipped (graceful degradation)"
            echo "prometheus_reachable=false" >> $GITHUB_OUTPUT
            exit 0
          fi

          echo "✅ Prometheus reachable (${PROMETHEUS_DURATION}s)"

          # Validate alerting rules
          echo "Step 2: Validating alerting rules..."
          if [ -f ".prometheus/alerts.yml" ]; then
            # Install promtool if not available
            if ! command -v promtool &> /dev/null; then
              echo "Installing promtool..."
              wget -q https://github.com/prometheus/prometheus/releases/download/v2.45.0/prometheus-2.45.0.linux-amd64.tar.gz
              tar -xzf prometheus-2.45.0.linux-amd64.tar.gz
              PATH="$PATH:$(pwd)/prometheus-2.45.0.linux-amd64"
            fi

            promtool check rules .prometheus/alerts.yml

            if [ $? -eq 0 ]; then
              echo "✅ Alerting rules valid"
            else
              echo "⚠️ Alerting rules have errors (non-blocking)"
            fi
          else
            echo "⚠️ .prometheus/alerts.yml not found (skipping validation)"
          fi

          # Test sample query
          echo "Step 3: Testing sample query..."
          QUERY_START=$(date +%s)

          TEST_QUERY='up{job="atom"}'
          QUERY_RESULT=$(curl -s -G "$PROMETHEUS_URL/api/v1/query" \
            --data-urlencode "query=$TEST_QUERY" \
            --max-time 10)

          QUERY_DURATION=$(($(date +%s) - QUERY_START))

          if echo "$QUERY_RESULT" | jq -e '.status == "success"' > /dev/null; then
            echo "✅ Sample query successful (${QUERY_DURATION}s)"
          else
            echo "⚠️ Sample query failed (non-blocking)"
          fi

          # Record Prometheus query metric
          echo "Step 4: Recording Prometheus query metrics..."
          METRICS_ENDPOINT="${{ secrets.METRICS_ENDPOINT }}"
          if [ -n "$METRICS_ENDPOINT" ]; then
            curl -s -X POST "$METRICS_ENDPOINT" \
              -H "Content-Type: plain/text" \
              --data "# TYPE prometheus_query_total counter\n\
    prometheus_query_total{workflow=\"deploy-staging\",result=\"success\"} 1\n\
    # TYPE prometheus_query_duration_seconds gauge\n\
    prometheus_query_duration_seconds{workflow=\"deploy-staging\"} ${PROMETHEUS_DURATION}" || true
          fi

          echo "prometheus_reachable=true" >> $GITHUB_OUTPUT
          echo "✅ Prometheus validation complete"

      - name: Monitor deployment metrics
        if: steps.prometheus_validation.outputs.prometheus_reachable == 'true'
        run: |
          # Existing monitoring check from Plan 03
          # ... (monitoring check code here)

      - name: Skip monitoring (Prometheus unreachable)
        if: steps.prometheus_validation.outputs.prometheus_reachable != 'true'
        run: |
          echo "⚠️ Monitoring check skipped (Prometheus unreachable)"
          echo "Deployment continues without Prometheus monitoring"
    ```

    **Key improvements**:
    - Test Prometheus connectivity before querying (`/-/healthy` endpoint)
    - Graceful degradation when PROMETHEUS_URL not set (skip monitoring, don't fail)
    - Validate alerting rules with promtool before deployment
    - Test sample query to verify Prometheus API works
    - Record Prometheus query metrics (duration, success/failure)
    - Set output variable `prometheus_reachable` for conditional monitoring
  </action>
  <verify>
    Run: grep "Validate Prometheus connectivity" .github/workflows/deploy.yml returns match
    Run: grep "prometheus_reachable" .github/workflows/deploy.yml returns >= 2
    Run: grep "/-/healthy" .github/workflows/deploy.yml returns match
  </verify>
  <done>
    Prometheus query validation added to deploy workflow. Connectivity tested before monitoring check. Graceful degradation when Prometheus unreachable. Alerting rules validated before deployment.
  </done>
</task>

<task type="auto">
  <name>Task 4: Add Grafana dashboard auto-update on deployment</name>
  <files>.github/workflows/deploy.yml</files>
  <action>
    Add Grafana dashboard update step to deploy-staging job (after monitoring check, line ~350):

    ```yaml
      - name: Update Grafana dashboards
        if: success()
        env:
          GRAFANA_URL: ${{ secrets.GRAFANA_URL }}
          GRAFANA_API_KEY: ${{ secrets.GRAFANA_API_KEY }}
        run: |
          set -euo pipefail

          echo "=== Updating Grafana Dashboards ==="

          # Check if Grafana credentials provided
          if [ -z "$GRAFANA_URL" ] || [ -z "$GRAFANA_API_KEY" ]; then
            echo "⚠️ Grafana URL or API key not set"
            echo "Dashboard update skipped (non-blocking)"
            exit 0
          fi

          echo "Grafana URL: $GRAFANA_URL"

          # List of dashboards to update
          DASHBOARDS=(
            "backend/monitoring/grafana/deployment-overview.json"
            "backend/monitoring/grafana/api-performance.json"
            "backend/monitoring/grafana/error-rate.json"
          )

          for DASHBOARD in "${DASHBOARDS[@]}"; do
            if [ ! -f "$DASHBOARD" ]; then
              echo "⚠️ Dashboard not found: $DASHBOARD (skipping)"
              continue
            fi

            echo "Processing dashboard: $DASHBOARD"

            # Extract dashboard UID from JSON
            DASHBOARD_UID=$(jq -r '.uid' "$DASHBOARD")

            if [ "$DASHBOARD_UID" == "null" ]; then
              echo "❌ Dashboard missing UID field"
              continue
            fi

            # Update dashboard via Grafana API
            echo "Updating dashboard: $DASHBOARD_UID"

            RESPONSE=$(curl -s -X POST "$GRAFANA_URL/api/dashboards/db" \
              -H "Authorization: Bearer $GRAFANA_API_KEY" \
              -H "Content-Type: application/json" \
              -d "{
                \"dashboard\": $(cat "$DASHBOARD"),
                \"overwrite\": true,
                \"message\": \"Updated via deploy.yml - Commit: ${{ github.sha }}\"
              }")

            # Check for API errors
            if echo "$RESPONSE" | jq -e '.status == "success"' > /dev/null; then
              echo "✅ Dashboard updated: $DASHBOARD_UID"
            else
              echo "❌ Dashboard update failed: $DASHBOARD_UID"
              echo "Response: $RESPONSE"
            fi
          done

          echo "✅ Grafana dashboard update complete"

      - name: Verify Grafana dashboard version
        if: success()
        env:
          GRAFANA_URL: ${{ secrets.GRAFANA_URL }}
          GRAFANA_API_KEY: ${{ secrets.GRAFANA_API_KEY }}
        run: |
          # Verify dashboard was updated with deployment metadata
          DASHBOARD_UID="atom-deployment-overview"
          GRAFANA_URL="${{ secrets.GRAFANA_URL }}"

          DASHBOARD_INFO=$(curl -s -X GET "$GRAFANA_URL/api/dashboards/uid/$DASHBOARD_UID" \
            -H "Authorization: Bearer ${{ secrets.GRAFANA_API_KEY }}")

          UPDATED=$(echo "$DASHBOARD_INFO" | jq -r '.dashboard.updatedAt')
          VERSION=$(echo "$DASHBOARD_INFO" | jq -r '.dashboard.version')

          echo "Dashboard: $DASHBOARD_UID"
          echo "Updated: $UPDATED"
          echo "Version: $VERSION"

          # Store version for rollback capability
          echo "grafana_dashboard_version=$VERSION" >> $GITHUB_OUTPUT

      - name: Rollback Grafana dashboard on failure
        if: failure()
        env:
          GRAFANA_URL: ${{ secrets.GRAFANA_URL }}
          GRAFANA_API_KEY: ${{ secrets.GRAFANA_API_KEY }}
        run: |
          # Rollback dashboard to previous version
          DASHBOARD_UID="atom-deployment-overview"
          PREVIOUS_VERSION=$((${{ steps.verify.outputs.grafana_dashboard_version }} - 1))

          echo "Rolling back dashboard to version: $PREVIOUS_VERSION"

          curl -s -X POST "$GRAFANA_URL/api/dashboards/db/$DASHBOARD_UID/revert/$PREVIOUS_VERSION" \
            -H "Authorization: Bearer $GRAFANA_API_KEY"

          echo "✅ Dashboard rolled back"
    ```

    **Create sample Grafana dashboard** (backend/monitoring/grafana/deployment-overview.json):
    ```json
    {
      "dashboard": {
        "id": null,
        "title": "Atom Deployment Overview",
        "tags": ["atom", "deployment"],
        "timezone": "browser",
        "schemaVersion": 36,
        "version": 1,
        "refresh": "30s",
        "panels": [
          {
            "id": 1,
            "title": "Deployment Success Rate",
            "type": "stat",
            "targets": [
              {
                "expr": "sum(rate(deployment_total{status=\"success\"}[5m])) / sum(rate(deployment_total[5m])) * 100"
              }
            ]
          },
          {
            "id": 2,
            "title": "Deployment Rollback Rate",
            "type": "graph",
            "targets": [
              {
                "expr": "sum(rate(deployment_rollback_total[5m])) by (environment)"
              }
            ]
          },
          {
            "id": 3,
            "title": "Smoke Test Pass Rate",
            "type": "stat",
            "targets": [
              {
                "expr": "sum(rate(smoke_test_total{result=\"passed\"}[5m])) / sum(rate(smoke_test_total[5m])) * 100"
              }
            ]
          }
        ]
      },
      "overwrite": true,
      "message": "Initial deployment monitoring dashboard"
    }
    ```

    **Key improvements**:
    - Dashboard update via Grafana API (POST /api/dashboards/db)
    - Automatic version tracking (store version for rollback)
    - Multiple dashboards supported (array of dashboard files)
    - Deployment metadata added to dashboard update message
    - Rollback capability on failure (revert to previous version)
    - Graceful handling when GRAFANA_URL or GRAFANA_API_KEY not set
  </action>
  <verify>
    Run: grep "Update Grafana dashboards" .github/workflows/deploy.yml returns match
    Run: grep "GRAFANA_URL" .github/workflows/deploy.yml returns >= 2
    Run: ls backend/monitoring/grafana/*.json returns >= 1
  </verify>
  <done>
    Grafana dashboard auto-update added to deploy workflow. Dashboards update automatically on successful deployment. Version tracking enables rollback capability. Graceful handling when Grafana credentials not set.
  </done>
</task>

<task type="auto">
  <name>Task 5: Implement progressive canary deployment strategy</name>
  <files>.github/workflows/deploy.yml</files>
  <action>
    Add canary deployment steps to deploy-production job (after smoke tests, line ~400):

    ```yaml
      - name: Progressive canary deployment
        if: success()
        env:
          CANARY_STEPS: "10,50,100"  # 10% → 50% → 100% traffic
          CANARY_WAIT_TIME: "300"    # 5 minutes between steps
        run: |
          set -euo pipefail

          echo "=== Progressive Canary Deployment ==="

          export KUBECONFIG=kubeconfig

          # Parse canary steps
          IFS=',' read -ra STEPS <<< "$CANARY_STEPS"

          DEPLOYMENT_ID="${{ github.sha }}"

          for PERCENTAGE in "${STEPS[@]}"; do
            echo "Step: Routing ${PERCENTAGE}% traffic to new deployment"

            # Update canary traffic percentage metric
            curl -s -X POST "${{ secrets.METRICS_ENDPOINT }}" \
              -H "Content-Type: plain/text" \
              --data "canary_traffic_percentage{environment=\"production\",deployment_id=\"$DEPLOYMENT_ID\"} $PERCENTAGE" || true

            # Update Kubernetes service to route traffic
            # Assuming blue-green deployment with two deployments: atom-blue, atom-green
            if [ "$PERCENTAGE" -eq 100 ]; then
              # Full traffic to new deployment
              kubectl patch svc atom-service -p '{"spec":{"selector":{"app":"atom","version":"${{ github.sha }}"}}}'
            else
              # Split traffic between old and new deployments
              # This requires Istio or similar service mesh for traffic splitting
              echo "⚠️ Traffic splitting requires service mesh (Istio, Linkerd)"
              echo "For basic canary, consider using Kubernetes deployment replicas:"
              echo "  - Scale new deployment to $PERCENTAGE% replicas"
              echo "  - Scale old deployment to $((100 - PERCENTAGE))% replicas"

              # Example: Replica-based canary (simple approach)
              TOTAL_REPLICAS=$(kubectl get deployment atom -o jsonpath='{.spec.replicas}')
              NEW_REPLICAS=$((TOTAL_REPLICAS * PERCENTAGE / 100))
              OLD_REPLICAS=$((TOTAL_REPLICAS - NEW_REPLICAS))

              echo "New deployment replicas: $NEW_REPLICAS"
              echo "Old deployment replicas: $OLD_REPLICAS"

              # Scale deployments (requires two deployments: atom-old, atom-new)
              # kubectl scale deployment atom-new --replicas=$NEW_REPLICAS
              # kubectl scale deployment atom-old --replicas=$OLD_REPLICAS
            fi

            # Wait for canary period
            if [ "$PERCENTAGE" -ne 100 ]; then
              echo "Waiting ${CANARY_WAIT_TIME}s for canary validation..."
              sleep "$CANARY_WAIT_TIME"
            fi

            # Check error rate during canary period
            if [ -n "${{ secrets.PROMETHEUS_URL }}" ]; then
              ERROR_RATE=$(curl -s -G "${{ secrets.PROMETHEUS_URL }}/api/v1/query" \
                --data-urlencode 'query=sum(rate(http_requests_total{status=~"5..",environment="production"}[5m])) / sum(rate(http_requests_total{environment="production"}[5m])) * 100' \
                | jq -r '.data.result[0].value[1] // "0"')

              echo "Error rate during canary: $ERROR_RATE%"

              # Rollback if error rate exceeds threshold
              if (( $(echo "$ERROR_RATE > 0.1" | bc -l) )); then
                echo "❌ Error rate too high during canary: $ERROR_RATE% (threshold: 0.1%)"
                echo "Triggering automatic rollback..."

                kubectl rollout undo deployment/atom
                exit 1
              fi
            fi

            echo "✅ Canary step ${PERCENTAGE}% complete"
          done

          echo "=== ✅ Canary deployment complete (100% traffic) ==="
    ```

    **Alternative: Istio-based traffic splitting** (if Istio available):
    ```yaml
      - name: Istio canary deployment
        if: success()
        env:
          CANARY_STEPS: "10,50,100"
          CANARY_WAIT_TIME: "300"
        run: |
          set -euo pipefail

          echo "=== Istio Canary Deployment ==="

          # Configure Istio VirtualService for traffic splitting
          for PERCENTAGE in 10 50 100; do
            echo "Routing ${PERCENTAGE}% traffic to new version"

            # Update Istio VirtualService
            kubectl apply -f - <<EOF
    apiVersion: networking.istio.io/v1beta1
    kind: VirtualService
    metadata:
      name: atom
    spec:
      http:
      - match:
        - headers:
            x-canary:
              exact: "true"
        route:
        - destination:
            host: atom
            subset: v2  # New version
          weight: ${PERCENTAGE}
        - destination:
            host: atom
            subset: v1  # Old version
          weight: $((100 - PERCENTAGE))
    EOF

            # Wait for canary period
            sleep "$CANARY_WAIT_TIME"

            # Check metrics, rollback if error rate high
            # ... (same error rate check as above)
          done
    ```

    **Key improvements**:
    - Progressive canary deployment (10% → 50% → 100% traffic)
    - Configurable canary steps and wait time
    - Error rate monitoring during canary period
    - Automatic rollback on error rate threshold breach
    - Support for both replica-based and Istio-based canary
    - Canary traffic metric tracking
  </action>
  <verify>
    Run: grep "Progressive canary deployment" .github/workflows/deploy.yml returns match
    Run: grep "CANARY_STEPS" .github/workflows/deploy.yml returns match
    Run: grep "canary_traffic_percentage" .github/workflows/deploy.yml returns match
  </verify>
  <done>
    Progressive canary deployment implemented with 10% → 50% → 100% traffic routing. Error rate monitored during canary period with automatic rollback on threshold breach. Support for both replica-based and Istio-based canary.
  </done>
</task>

</tasks>

<verification>
Overall phase verification steps:

1. **Deployment Metrics Verification**:
   ```bash
   # Verify metrics exposed via /metrics endpoint
   curl http://localhost:8001/metrics | grep deployment_total

   # Verify metric labels
   curl http://localhost:8001/metrics | grep 'deployment_total{environment="staging"'
   curl http://localhost:8001/metrics | grep 'deployment_rollback_total'
   ```

2. **Prometheus Alerting Rules Verification**:
   ```bash
   # Validate alerting rules syntax
   .prometheus/validate-alerts.sh

   # Count configured alerts
   grep -c "alert:" .prometheus/alerts.yml

   # List all alerts
   .prometheus/validate-alerts.sh | grep "Configured Alerts" -A 20
   ```

3. **Prometheus Query Validation Verification**:
   ```bash
   # Verify validation step in workflow
   grep "Validate Prometheus connectivity" .github/workflows/deploy.yml

   # Test graceful degradation (PROMETHEUS_URL not set)
   # Deploy with PROMETHEUS_URL secret unset, verify deployment succeeds
   ```

4. **Grafana Dashboard Update Verification**:
   ```bash
   # Verify Grafana dashboard update step
   grep "Update Grafana dashboards" .github/workflows/deploy.yml

   # Test dashboard update locally (if Grafana available)
   curl -X POST $GRAFANA_URL/api/dashboards/db \
     -H "Authorization: Bearer $GRAFANA_API_KEY" \
     -d @backend/monitoring/grafana/deployment-overview.json
   ```

5. **Canary Deployment Verification**:
   ```bash
   # Verify canary deployment step
   grep "Progressive canary deployment" .github/workflows/deploy.yml

   # Verify canary configuration
   grep "CANARY_STEPS" .github/workflows/deploy.yml
   grep "CANARY_WAIT_TIME" .github/workflows/deploy.yml
   ```

**Success Criteria**:
- [ ] Deployment metrics exposed (deployment_total, deployment_rollback_total, smoke_test_total)
- [ ] Prometheus alerting rules validated with promtool
- [ ] Prometheus connectivity tested before monitoring check
- [ ] Grafana dashboard auto-updates on deployment
- [ ] Canary deployment configured (10% → 50% → 100% traffic)
- [ ] Error rate monitored during canary period
- [ ] Automatic rollback on error rate threshold breach
</verification>

<success_criteria>
**Monitoring & Alerting Enhancement Complete When:**

1. **Deployment Metrics**: All deployment metrics tracked (success rate, rollback rate, frequency, duration)
2. **Alerting Rules**: 9+ Prometheus alerts configured for deployments, rollbacks, smoke tests, error rates, latency
3. **Query Validation**: Prometheus connectivity validated before monitoring check, graceful degradation when unreachable
4. **Grafana Integration**: Dashboards auto-update on deployment with version tracking and rollback capability
5. **Canary Deployment**: Progressive traffic routing (10% → 50% → 100%) with error rate monitoring
6. **Monitoring Safety**: No deployment failures due to Prometheus unavailability (graceful degradation)

**Measurable Outcomes**:
- Deployment success rate tracked: >95% (5+ deployments)
- Rollback rate tracked: <5% (automatic rollback on smoke test failure)
- Smoke test pass rate tracked: >98%
- Prometheus query success rate: >90%
- Grafana dashboard update success rate: 100%
- Canary deployment rollback rate: <2% (error rate threshold breach)
</success_criteria>

<output>
After completion, create `.planning/phases/67-ci-cd-pipeline-fixes/67-04-SUMMARY.md` with:
- List of all deployment metrics added (with metric names and labels)
- List of all Prometheus alerting rules configured (with thresholds and severities)
- Prometheus query validation results (connectivity, sample query, rule validation)
- Grafana dashboard update verification results (version tracking, rollback test)
- Canary deployment configuration (steps, wait times, error rate thresholds)
- Monitoring dashboard URLs (Prometheus, Grafana)
</output>
