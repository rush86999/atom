---
phase: 67-ci-cd-pipeline-fixes
plan: 05
type: execute
wave: 3
depends_on: [67-01, 67-02, 67-03, 67-04]
files_modified:
  - backend/docs/CI_CD_RUNBOOK.md
  - backend/docs/DEPLOYMENT_GUIDE.md
  - backend/docs/CI_CD_TROUBLESHOOTING.md
  - backend/.github/workflows/README.md
autonomous: true
user_setup: []

must_haves:
  truths:
    - "All CI/CD workflows documented with detailed diagrams"
    - "Runbook created for rollback procedures (staging and production)"
    - "Troubleshooting guide covers common failures with solutions"
    - "Quality gates documented with enforcement mechanisms"
    - "Workflow architecture diagram shows job dependencies"
    - "Environment variables documented with examples"
  artifacts:
    - path: backend/docs/CI_CD_RUNBOOK.md
      provides: "Comprehensive runbook for CI/CD operations (deployment, rollback, verification)"
      min_lines: 800
      exports: ["Deployment Runbook", "Rollback Procedures", "Verification Checklist"]
    - path: backend/docs/DEPLOYMENT_GUIDE.md
      provides: "Step-by-step deployment guide for staging and production environments"
      min_lines: 600
      exports: ["Deployment Guide", "Environment Setup", "Health Checks"]
    - path: backend/docs/CI_CD_TROUBLESHOOTING.md
      provides: "Troubleshooting guide for common CI/CD failures with solutions"
      min_lines: 500
      exports: ["Troubleshooting Guide", "Common Issues", "Solutions"]
    - path: .github/workflows/README.md
      provides: "Workflow documentation with architecture diagrams and job descriptions"
      min_lines: 400
      exports: ["Workflow Architecture", "Job Descriptions", "Dependencies"]
  key_links:
    - from: "backend/docs/CI_CD_RUNBOOK.md"
      to: ".github/workflows/deploy.yml"
      via: "Rollback procedures reference kubectl commands from workflow"
      pattern: "kubectl rollout undo"
    - from: "backend/docs/DEPLOYMENT_GUIDE.md"
      to: "backend/core/health_routes.py"
      via: "Deployment guide references /health/live, /health/ready, /health/db endpoints"
      pattern: "/health/"
    - from: "backend/docs/CI_CD_TROUBLESHOOTING.md"
      to: ".github/workflows/ci.yml"
      via: "Troubleshooting guide references CI workflow job names and error patterns"
      pattern: "backend-test-full|test-quality-gates"
---

<objective>
Create comprehensive documentation for CI/CD pipelines including runbook, deployment guide, troubleshooting guide, and workflow documentation with diagrams.

Purpose: CI/CD pipelines lack comprehensive documentation for operators. Runbook procedures, troubleshooting steps, and quality gate enforcement mechanisms need to be documented for operational excellence.

Output: Complete CI/CD documentation suite (runbook, deployment guide, troubleshooting guide, workflow README) with diagrams, procedures, and examples.
</objective>

<execution_context>
@/Users/rushiparikh/.claude/get-shit-done/workflows/execute-plan.md
@/Users/rushiparikh/.claude/get-shit-done/templates/summary.md

@.planning/phases/67-ci-cd-pipeline-fixes/67-RESEARCH.md
@.github/workflows/ci.yml
@.github/workflows/deploy.yml
@backend/docs/DEPLOYMENT_RUNBOOK.md (reference)

# Depends on all previous plans (01-04) for complete implementation context
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Research Context
Phase 67 research identified documentation gaps:

**Existing Documentation**:
- backend/docs/DEPLOYMENT_RUNBOOK.md exists but may be outdated
- No dedicated CI/CD troubleshooting guide
- No workflow architecture documentation
- Quality gates defined but not documented in runbook

**Documentation Requirements**:
1. **Runbook**: Deployment procedures, rollback procedures, verification checklists
2. **Deployment Guide**: Step-by-step instructions, environment setup, health checks
3. **Troubleshooting Guide**: Common failures, error patterns, solutions
4. **Workflow Documentation**: Job descriptions, dependencies, architecture diagrams

**Key Decision**: Create comprehensive documentation suite with ASCII diagrams, code examples, and step-by-step procedures. Include real examples from actual workflows (ci.yml, deploy.yml).

**Documentation Standards**:
- Use Mermaid diagrams for workflow visualization
- Include copy-paste code examples with real commands
- Provide environment variable templates
- Document all quality gates with pass/fail criteria
- Include escalation procedures for critical failures
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create comprehensive CI/CD runbook</name>
  <files>backend/docs/CI_CD_RUNBOOK.md</files>
  <action>
    Create backend/docs/CI_CD_RUNBOOK.md with operational procedures:

    ```markdown
    # Atom CI/CD Runbook

    **Purpose**: Operational procedures for Atom CI/CD pipelines
    **Last Updated**: 2026-02-20
    **Maintainer**: DevOps Team

    ## Table of Contents

    1. [Quick Reference](#quick-reference)
    2. [Deployment Procedures](#deployment-procedures)
    3. [Rollback Procedures](#rollback-procedures)
    4. [Verification Checklist](#verification-checklist)
    5. [Emergency Procedures](#emergency-procedures)
    6. [Quality Gates](#quality-gates)

    ---

    ## Quick Reference

    ### Workflow URLs
    - **CI Pipeline**: `.github/workflows/ci.yml`
    - **Deploy Pipeline**: `.github/workflows/deploy.yml`
    - **LanceDB Integration**: `.github/workflows/lancedb-integration.yml`

    ### Key Commands

    ```bash
    # Trigger deployment
    gh workflow run deploy.yml --ref main

    # Watch workflow run
    gh run watch --interval 10

    # View workflow logs
    gh run view --log

    # Cancel workflow run
    gh run cancel <run-id>

    # Retry failed workflow
    gh run rerun <run-id>
    ```

    ### Critical Environment Variables

    | Variable | Purpose | Example |
    |----------|---------|---------|
    | `DATABASE_URL` | Database connection | `postgresql://user:pass@host:5432/db` |
    | `PROMETHEUS_URL` | Monitoring endpoint | `http://prometheus:9090` |
    | `GRAFANA_URL` | Dashboard URL | `http://grafana:3000` |
    | `STAGING_URL` | Staging environment | `https://staging.atom.example.com` |
    | `PRODUCTION_URL` | Production environment | `https://atom.example.com` |

    ---

    ## Deployment Procedures

    ### Staging Deployment

    **Trigger**: Automatic on push to `main` branch
    **Approval**: None required (automatic deployment)
    **Duration**: 5-10 minutes

    **Procedure**:

    1. **Push to main branch**
       ```bash
       git checkout main
       git pull
       git merge feature-branch
       git push origin main
       ```

    2. **Monitor CI workflow**
       ```bash
       # Watch CI pipeline
       gh workflow view ci

       # Watch latest run
       gh run watch --interval 10
       ```

    3. **Verify staging deployment**
       ```bash
       # Check staging health
       curl https://staging.atom.example.com/health/live
       curl https://staging.atom.example.com/health/ready
       curl https://staging.atom.example.com/health/db

       # Check smoke test results
       gh run view --log | grep "Smoke test"
       ```

    4. **Verify deployment metrics**
       ```bash
       # Check Prometheus metrics
       curl "$PROMETHEUS_URL/api/v1/query?query=deployment_success_rate"

       # Check Grafana dashboard
       # Visit: https://grafana.example.com/d/atom-deployment-overview
       ```

    **Success Criteria**:
    - [ ] CI workflow passes (all tests, linting, type checking)
    - [ ] Docker image builds successfully
    - [ ] Deployment to staging completes
    - [ ] Smoke tests pass (authentication, health checks, API endpoints)
    - [ ] Error rate <1% (Prometheus query)
    - [ ] Dashboard updated in Grafana

    **Failure Handling**:
    - If CI fails: Fix test failures, push new commit
    - If build fails: Check Dockerfile syntax, dependencies
    - If deploy fails: Check kubectl configuration, cluster connectivity
    - If smoke tests fail: Automatic rollback triggered, investigate GitHub issue

    ### Production Deployment

    **Trigger**: Manual approval after staging deployment
    **Approval**: Required via GitHub Actions environment
    **Duration**: 10-15 minutes (including canary deployment)

    **Procedure**:

    1. **Verify staging deployment**
       ```bash
       # Run smoke tests against staging
       ./scripts/smoke-tests.sh staging

       # Check staging metrics
       curl "$PROMETHEUS_URL/api/v1/query?query=deployment_success_rate{environment=\"staging\"}"
       ```

    2. **Trigger production deployment**
       ```bash
       # Manual trigger via GitHub UI
       # Visit: https://github.com/org/repo/actions/workflows/deploy.yml
       # Click "Run workflow" â†’ Select "production" environment
       ```

       Or via CLI:
       ```bash
       gh workflow run deploy.yml --ref main -f environment=production
       ```

    3. **Monitor canary deployment**
       ```bash
       # Watch canary progress
       gh run watch --interval 30

       # Check canary traffic percentage
       curl "$PROMETHEUS_URL/api/v1/query?query=canary_traffic_percentage"
       ```

    4. **Verify production deployment**
       ```bash
       # Check production health
       curl https://atom.example.com/health/live
       curl https://atom.example.com/health/ready
       curl https://atom.example.com/health/db

       # Run smoke tests against production
       ./scripts/smoke-tests.sh production
       ```

    **Success Criteria**:
    - [ ] Staging deployment verified (smoke tests pass, metrics healthy)
    - [ ] Manual approval obtained
    - [ ] Production deployment completes
    - [ ] Canary deployment passes (10% â†’ 50% â†’ 100% traffic)
    - [ ] Smoke tests pass (production)
    - [ ] Error rate <0.1% (stricter than staging)
    - [ ] Latency P95 <200ms (production threshold)

    **Failure Handling**:
    - If staging fails: Do NOT proceed to production, fix issues first
    - If canary fails: Automatic rollback triggered, investigate error spike
    - If smoke tests fail: Automatic rollback triggered, GitHub issue created
    - If metrics degraded: Pause deployment, investigate, rollback if needed

    ---

    ## Rollback Procedures

    ### Automatic Rollback

    **Trigger Conditions**:
    - Smoke test failure (authentication, health checks, API endpoints)
    - Error rate exceeds threshold (>1% staging, >0.1% production)
    - Latency exceeds threshold (>500ms staging, >200ms production)
    - Canary deployment failure (error spike during canary period)

    **Automatic Rollback Process**:

    1. **Detection**: Smoke test or monitoring check detects failure
    2. **Trigger**: GitHub Actions workflow executes rollback step
    3. **Rollback**: `kubectl rollout undo deployment/atom` executed
    4. **Verification**: Rollback status wait (5-minute timeout)
    5. **Notification**: Slack alert sent, GitHub issue created

    **Example Rollback Log**:
    ```
    === ðŸš¨ Smoke tests failed - initiating automatic rollback ===
    Step 1: Rolling back deployment...
    deployment.apps/atom rolled back
    Step 2: Waiting for rollback to complete...
    rollback condition met (5s)
    === âœ… Rollback completed ===
    ```

    ### Manual Rollback

    **When to Use**:
    - Gradual performance degradation (no automatic trigger)
    - Customer-reported issues (not caught by automated checks)
    - Feature flag needs to be disabled
    - Database migration issues (post-deployment)

    **Procedure**:

    1. **Verify need for rollback**
       ```bash
       # Check error rate
       curl "$PROMETHEUS_URL/api/v1/query?query=error_rate"

       # Check customer reports
       # Check Slack alerts, PagerDuty, etc.
       ```

    2. **Execute rollback**
       ```bash
       # Option 1: Rollback to previous deployment
       kubectl rollout undo deployment/atom

       # Option 2: Rollback to specific revision
       kubectl rollout undo deployment/atom --to-revision=3

       # Wait for rollback to complete
       kubectl rollout status deployment/atom --timeout=5m
       ```

    3. **Verify rollback**
       ```bash
       # Check deployment status
       kubectl get deployments atom

       # Check pods are running
       kubectl get pods -l app=atom

       # Run smoke tests
       ./scripts/smoke-tests.sh production
       ```

    4. **Communicate rollback**
       ```bash
       # Send Slack notification
       curl -X POST "$SLACK_WEBHOOK_URL" \
         -H 'Content-Type: application/json' \
         -d '{"text": "ðŸš¨ Manual rollback executed for production"}'

       # Create GitHub issue
       gh issue create \
         --title "Production Rollback - Manual" \
         --body "Rollback executed manually due to..." \
         --label "production,rollback"
       ```

    **Rollback Verification Checklist**:
    - [ ] Deployment rolled back to previous version
    - [ ] Pods are healthy (no crash loops)
    - [ ] Smoke tests pass
    - [ ] Error rate returns to baseline (<0.1%)
    - [ ] Latency returns to baseline (P95 <200ms)
    - [ ] Slack notification sent
    - [ ] GitHub issue created for investigation

    ---

    ## Verification Checklist

    ### Post-Deployment Verification

    **Execute after EVERY deployment** (staging or production):

    1. **Health Checks**
       ```bash
       # Liveness probe
       curl -f https://staging.atom.example.com/health/live

       # Readiness probe
       curl -f https://staging.atom.example.com/health/ready

       # Database connectivity
       curl -f https://staging.atom.example.com/health/db
       ```

       **Expected Output**:
       ```json
       {"status": "healthy", "timestamp": "2026-02-20T10:30:00Z"}
       ```

    2. **Smoke Tests**
       ```bash
       # Run automated smoke tests
       ./scripts/smoke-tests.sh staging

       # Or manual smoke tests
       ./scripts/manual-smoke-tests.sh staging
       ```

       **Expected Results**:
       - Authentication: âœ… Success
       - Health endpoints: âœ… All pass
       - Agent execution: âœ… API responds
       - Canvas presentation: âœ… API responds
       - Skills endpoint: âœ… API responds

    3. **Metrics Verification**
       ```bash
       # Error rate
       curl "$PROMETHEUS_URL/api/v1/query?query=error_rate{environment=\"staging\"}"

       # Latency P95
       curl "$PROMETHEUS_URL/api/v1/query?query=latency_p95{environment=\"staging\"}"

       # Deployment success rate
       curl "$PROMETHEUS_URL/api/v1/query?query=deployment_success_rate"
       ```

       **Expected Thresholds**:
       - Error rate (staging): <1%
       - Error rate (production): <0.1%
       - Latency P95 (staging): <500ms
       - Latency P95 (production): <200ms
       - Deployment success rate: >95%

    4. **Dashboard Verification**
       - Visit Grafana: `https://grafana.example.com/d/atom-deployment-overview`
       - Verify no error spikes
       - Verify latency within threshold
       - Verify deployment success rate stable

    5. **Log Verification**
       ```bash
       # Check application logs
       kubectl logs -l app=atom --tail=100 | grep ERROR

       # Check for error patterns
       kubectl logs -l app=atom --tail=100 | grep -i "exception\|error\|failed"

       # Verify no crash loops
       kubectl get pods -l app=atom | grep -c "CrashLoopBackOff"
       # Expected: 0
       ```

    ---

    ## Emergency Procedures

    ### Emergency Rollback (<5 minutes)

    **Scenario**: Critical failure detected (error rate >5%, P0 incident)

    **Procedure**:
    1. **Execute immediate rollback**
       ```bash
       # Rollback to previous version (fastest method)
       kubectl rollout undo deployment/atom

       # Wait for rollback (with timeout)
       kubectl rollout status deployment/atom --timeout=3m
       ```

    2. **Verify rollback**
       ```bash
       # Check pods are restarting
       kubectl get pods -l app=atom

       # Check error rate decreasing
       curl "$PROMETHEUS_URL/api/v1/query?query=error_rate"
       ```

    3. **Communicate incident**
       ```bash
       # Send critical Slack alert
       curl -X POST "$SLACK_WEBHOOK_CRITICAL" \
         -H 'Content-Type: application/json' \
         -d '{"text": "ðŸš¨ðŸš¨ EMERGENCY ROLLBACK EXECUTED ðŸš¨ðŸš¨"}'

       # Declare incident (PagerDuty, etc.)
       # Follow incident response runbook
       ```

    4. **Create post-mortem ticket**
       ```bash
       gh issue create \
         --title "P0 Incident - Emergency Rollback" \
         --body "Emergency rollback executed due to..." \
         --label "incident,p0,critical"
       ```

    ### Incident Communication

    **Stakeholder Notification**:
    - **Engineering**: Slack #incident channel
    - **Product**: Email product@atom.example.com
    - **Support**: Email support@atom.example.com
    - **Customers** (if production outage): Status page update

    **Communication Template**:
    ```
    Subject: INCIDENT - Production Deployment Issue

    Summary: Automatic rollback triggered due to smoke test failure
    Impact: [X] users affected for [Y] minutes
    Timeline: [Start time] - [End time]
    Root Cause: [To be determined]
    Resolution: Rolled back to previous deployment
    Next Steps: Investigating root cause, will provide updates
    ```

    ---

    ## Quality Gates

    ### TQ-01: Test Independence

    **Purpose**: Validate tests run independently without ordering dependencies

    **Enforcement**: Random order test execution in CI

    **Command**:
    ```bash
    pytest tests/ --random-order --random-order-seed=random -v --maxfail=5
    ```

    **Pass Criteria**: All tests pass in random order

    **Failure Impact**: CI workflow fails, deployment blocked

    ### TQ-02: Test Pass Rate

    **Purpose**: Ensure 98% minimum test pass rate

    **Enforcement**: Pass rate calculated from pytest JSON output

    **Command**:
    ```bash
    pytest tests/ --json-report --json-report-file=pytest_report.json
    python tests/scripts/parse_pytest_output.py pytest_report.json
    ```

    **Pass Criteria**: Pass rate >=98%

    **Failure Impact**: CI workflow fails, deployment blocked

    ### TQ-03: Test Performance

    **Purpose**: Ensure test suite completes in reasonable time

    **Enforcement**: Test duration measured in CI

    **Pass Criteria**: Full test suite <60 minutes

    **Failure Impact**: Warning only, deployment continues

    ### TQ-04: Test Determinism

    **Purpose**: Ensure tests produce consistent results

    **Enforcement**: Flaky test detection with pytest-rerunfailures

    **Command**:
    ```bash
    pytest tests/ --reruns 2 --reruns-delay 1
    ```

    **Pass Criteria**: Tests pass after 2 retries

    **Failure Impact**: Test marked as flaky, investigation required

    ### TQ-05: Coverage Quality

    **Purpose**: Ensure adequate code coverage

    **Enforcement**: Coverage percentage measured in CI

    **Command**:
    ```bash
    pytest tests/ --cov=core --cov=api --cov=tools --cov-fail-under=25
    ```

    **Pass Criteria**: Coverage >=25% (current threshold)

    **Target**: Coverage >=50% (future goal)

    **Failure Impact**: Warning only, deployment continues

    ---

    ## Appendix

    ### Useful Commands

    ```bash
    # View workflow history
    gh run list --workflow=deploy.yml --limit 10

    # View specific workflow run
    gh run view <run-id>

    # Download workflow artifacts
    gh run download <run-id>

    # Retry failed workflow
    gh run rerun <run-id>

    # Cancel running workflow
    gh run cancel <run-id>

    # Check deployment status
    kubectl rollout status deployment/atom

    # View deployment history
    kubectl rollout history deployment/atom --revision=0

    # Check pod health
    kubectl get pods -l app=atom

    # View pod logs
    kubectl logs -l app=atom --tail=100 --follow

    # Execute command in pod
    kubectl exec -it <pod-name> -- /bin/bash
    ```

    ### Contacts

    - **DevOps On-Call**: devops@atom.example.com
    - **Engineering Lead**: eng-lead@atom.example.com
    - **Product Manager**: product@atom.example.com

    ### Related Documentation

    - [Deployment Guide](./DEPLOYMENT_GUIDE.md)
    - [Troubleshooting Guide](./CI_CD_TROUBLESHOOTING.md)
    - [Monitoring Setup](./MONITORING_SETUP.md)
    - [Docker Deployment](./DOCKER_DEPLOYMENT.md)
    ```

    **Key sections**:
    - Quick reference with critical commands and environment variables
    - Step-by-step deployment procedures for staging and production
    - Automatic and manual rollback procedures
    - Post-deployment verification checklist
    - Emergency procedures for critical failures
    - Quality gates documentation (TQ-01 through TQ-05)
    - Appendix with useful commands and contacts
  </action>
  <verify>
    Run: grep -c "##" backend/docs/CI_CD_RUNBOOK.md returns >= 20 (20+ sections)
    Run: wc -l backend/docs/CI_CD_RUNBOOK.md returns >= 800 (800+ lines)
    Run: grep "kubectl rollout undo" backend/docs/CI_CD_RUNBOOK.md returns match
  </verify>
  <done>
    CI/CD runbook created with comprehensive operational procedures. Includes deployment, rollback, verification, emergency procedures, and quality gates documentation.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create deployment guide with diagrams</name>
  <files>backend/docs/DEPLOYMENT_GUIDE.md</files>
  <action>
    Create backend/docs/DEPLOYMENT_GUIDE.md with step-by-step instructions:

    ```markdown
    # Atom Deployment Guide

    **Purpose**: Step-by-step guide for deploying Atom to staging and production
    **Last Updated**: 2026-02-20
    **Audience**: DevOps engineers, SREs

    ## Table of Contents

    1. [Prerequisites](#prerequisites)
    2. [Environment Setup](#environment-setup)
    3. [Staging Deployment](#staging-deployment)
    4. [Production Deployment](#production-deployment)
    5. [Health Checks](#health-checks)
    6. [Canary Deployment](#canary-deployment)
    7. [Monitoring](#monitoring)

    ---

    ## Prerequisites

    ### Required Tools

    - **kubectl**: 1.28+
    - **Docker**: 24.0+
    - **GitHub CLI**: 2.40+
    - **Python**: 3.11+
    - **Prometheus**: 2.45+ (for monitoring)

    ### Required Access

    - **GitHub**: Repository write access
    - **Kubernetes**: Cluster admin access
    - **Container Registry**: Push/pull access
    - **Prometheus**: Query access
    - **Grafana**: Dashboard update access

    ### Required Secrets

    Configure these secrets in GitHub repository settings (`Settings â†’ Secrets and variables â†’ Actions`):

    | Secret Name | Description | Example |
    |-------------|-------------|---------|
    | `KUBECONFIG_STAGING` | Staging cluster config (base64) | `base64 ~/.kube/config-staging` |
    | `KUBECONFIG_PRODUCTION` | Production cluster config (base64) | `base64 ~/.kube/config-production` |
    | `REGISTRY_USERNAME` | Container registry username | `dockerhub-user` |
    | `REGISTRY_PASSWORD` | Container registry password | `dckr_pat_xxxxx` |
    | `SMOKE_TEST_USERNAME` | Smoke test user | `smoke_test` |
    | `SMOKE_TEST_PASSWORD` | Smoke test password | `changeme123` |
    | `STAGING_URL` | Staging environment URL | `https://staging.atom.example.com` |
    | `PRODUCTION_URL` | Production environment URL | `https://atom.example.com` |
    | `PROMETHEUS_URL` | Monitoring endpoint | `http://prometheus:9090` |
    | `GRAFANA_URL` | Dashboard URL | `http://grafana:3000` |
    | `GRAFANA_API_KEY` | Grafana service account token | `eyJr...` |
    | `SLACK_WEBHOOK_URL` | Slack webhook for notifications | `https://hooks.slack.com/...` |

    ---

    ## Environment Setup

    ### Kubernetes Cluster Setup

    **Staging Cluster**:
    ```bash
    # Create namespace
    kubectl create namespace atom-staging

    # Create secrets
    kubectl create secret generic atom-secrets \
      --from-literal=database-url="postgresql://..." \
      --from-literal=byok-encryption-key="..." \
      --namespace=atom-staging

    # Deploy application (first time)
    kubectl apply -f k8s/staging/ --namespace=atom-staging
    ```

    **Production Cluster**:
    ```bash
    # Create namespace
    kubectl create namespace atom-production

    # Create secrets
    kubectl create secret generic atom-secrets \
      --from-literal=database-url="postgresql://..." \
      --from-literal=byok-encryption-key="..." \
      --namespace=atom-production

    # Deploy application (first time)
    kubectl apply -f k8s/production/ --namespace=atom-production
    ```

    ### Database Migration Setup

    **Run migrations manually** (before first deployment):
    ```bash
    # Activate virtual environment
    source venv/bin/activate

    # Run Alembic migrations
    alembic upgrade head

    # Verify migrations
    alembic current
    alembic history
    ```

    **Verify smoke test user**:
    ```bash
    # Check if smoke test user exists
    psql -c "SELECT username, is_smoke_test_user FROM users WHERE username='smoke_test'"

    # If not exists, create manually
    python -c "
    from backend.core.models import User
    from backend.core.security import get_password_hash
    from backend.core.database import SessionLocal

    db = SessionLocal()
    user = User(
        username='smoke_test',
        email='smoke-test@example.com',
        hashed_password=get_password_hash('smoke_test_password_change_in_prod'),
        is_smoke_test_user=True
    )
    db.add(user)
    db.commit()
    "
    ```

    ---

    ## Staging Deployment

    ### Deployment Workflow

    ```mermaid
    graph LR
    A[Push to main] --> B[CI Workflow]
    B --> C{Tests Pass?}
    C -->|No| D[Fail CI]
    C -->|Yes| E[Build Docker Image]
    E --> F[Push to Registry]
    F --> G[Deploy to Staging]
    G --> H{Smoke Tests Pass?}
    H -->|No| I[Automatic Rollback]
    H -->|Yes| J[Deployment Complete]
    I --> J
    ```

    ### Step-by-Step Procedure

    **Step 1: Push to main branch**
    ```bash
    # Ensure your branch is up to date
    git checkout main
    git pull origin main

    # Merge your feature branch
    git merge feature-branch

    # Push to trigger deployment
    git push origin main
    ```

    **Step 2: Monitor CI workflow**
    ```bash
    # Watch CI pipeline progress
    gh workflow view ci
    gh run watch --interval 10

    # Expected output:
    # âœ“ backend-test (3m 30s)
    # âœ“ frontend-build (1m 15s)
    # âœ“ backend-test-full (8m 45s)
    # âœ“ test-quality-gates (4m 20s)
    # âœ“ build-docker (2m 10s)
    ```

    **Step 3: Monitor deployment workflow**
    ```bash
    # Watch deployment progress
    gh workflow view deploy
    gh run watch --interval 30

    # Expected output:
    # âœ“ test (5m 10s)
    # âœ“ build (3m 20s)
    # â†’ deploy-staging (in progress)
    ```

    **Step 4: Verify deployment**
    ```bash
    # Check health endpoints
    curl -f https://staging.atom.example.com/health/live
    curl -f https://staging.atom.example.com/health/ready
    curl -f https://staging.atom.example.com/health/db

    # Expected output:
    # {"status":"healthy","timestamp":"2026-02-20T10:30:00Z"}
    ```

    **Step 5: Run manual smoke tests** (optional)
    ```bash
    # Run automated smoke tests
    ./scripts/smoke-tests.sh staging

    # Or manual smoke tests
    ./scripts/manual-smoke-tests.sh staging
    ```

    ---

    ## Production Deployment

    ### Deployment Workflow

    ```mermaid
    graph LR
    A[Staging Verified] --> B[Manual Approval]
    B --> C[Deploy to Production]
    C --> D[Canary 10%]
    D --> E{Error Rate OK?}
    E -->|No| F[Automatic Rollback]
    E -->|Yes| G[Canary 50%]
    G --> H{Error Rate OK?}
    H -->|No| F
    H -->|Yes| I[Canary 100%]
    I --> J{Smoke Tests Pass?}
    J -->|No| F
    J -->|Yes| K[Deployment Complete]
    ```

    ### Step-by-Step Procedure

    **Step 1: Verify staging deployment**
    ```bash
    # Run comprehensive smoke tests against staging
    ./scripts/smoke-tests.sh staging --comprehensive

    # Check staging metrics
    curl "$PROMETHEUS_URL/api/v1/query?query=deployment_success_rate{environment=\"staging\"}"
    curl "$PROMETHEUS_URL/api/v1/query?query=error_rate{environment=\"staging\"}"

    # Verify staging error rate <1%
    # Verify staging latency P95 <500ms
    ```

    **Step 2: Trigger production deployment**
    ```bash
    # Via GitHub UI:
    # 1. Visit: https://github.com/org/repo/actions/workflows/deploy.yml
    # 2. Click "Run workflow"
    # 3. Select "production" environment
    # 4. Click "Run workflow"

    # Or via CLI:
    gh workflow run deploy.yml --ref main -f environment=production
    ```

    **Step 3: Monitor canary deployment**
    ```bash
    # Watch canary progress
    gh run watch --interval 60

    # Expected output:
    # â†’ deploy-production (in progress)
    #   - Canary 10% traffic (5m wait)
    #   - Canary 50% traffic (5m wait)
    #   - Canary 100% traffic
    ```

    **Step 4: Verify production deployment**
    ```bash
    # Check health endpoints
    curl -f https://atom.example.com/health/live
    curl -f https://atom.example.com/health/ready
    curl -f https://atom.example.com/health/db

    # Run smoke tests against production
    ./scripts/smoke-tests.sh production
    ```

    ---

    ## Health Checks

    ### Liveness Probe

    **Endpoint**: `/health/live`
    **Purpose**: Check if application is alive
    **Method**: GET
    **Timeout**: 2 seconds
    **Interval**: 10 seconds

    **Request**:
    ```bash
    curl https://staging.atom.example.com/health/live
    ```

    **Response (200 OK)**:
    ```json
    {
      "status": "healthy",
      "timestamp": "2026-02-20T10:30:00.000Z"
    }
    ```

    **Response (503 Service Unavailable)**:
    ```json
    {
      "status": "unhealthy",
      "timestamp": "2026-02-20T10:30:00.000Z",
      "error": "Application not responding"
    }
    ```

    ### Readiness Probe

    **Endpoint**: `/health/ready`
    **Purpose**: Check if application is ready to handle requests
    **Method**: GET
    **Timeout**: 5 seconds
    **Interval**: 10 seconds

    **Request**:
    ```bash
    curl https://staging.atom.example.com/health/ready
    ```

    **Response (200 OK)**:
    ```json
    {
      "status": "ready",
      "timestamp": "2026-02-20T10:30:00.000Z",
      "checks": {
        "database": {"status": "ok"},
        "redis": {"status": "ok"},
        "lancedb": {"status": "ok"}
      }
    }
    ```

    **Response (503 Service Unavailable)**:
    ```json
    {
      "status": "not_ready",
      "timestamp": "2026-02-20T10:30:00.000Z",
      "checks": {
        "database": {"status": "failed", "error": "Connection refused"}
      }
    }
    ```

    ### Database Connectivity Check

    **Endpoint**: `/health/db`
    **Purpose**: Check database connectivity and query performance
    **Method**: GET
    **Timeout**: 5 seconds

    **Request**:
    ```bash
    curl https://staging.atom.example.com/health/db
    ```

    **Response (200 OK)**:
    ```json
    {
      "status": "healthy",
      "timestamp": "2026-02-20T10:30:00.000Z",
      "database": {
        "connected": true,
        "query_time_ms": 5.23,
        "pool_status": {
          "size": 10,
          "checked_in": 8,
          "checked_out": 2,
          "overflow": 0,
          "max_overflow": 10
        }
      }
    }
    ```

    **Response (503 Service Unavailable)**:
    ```json
    {
      "status": "unhealthy",
      "timestamp": "2026-02-20T10:30:00.000Z",
      "database": {
        "connected": false,
        "error": "Connection refused: port 5432"
      }
    }
    ```

    ---

    ## Canary Deployment

    ### Canary Strategy

    **Progressive Traffic Splitting**:
    - **10% traffic** (5 minutes): Initial canary, monitor error rate
    - **50% traffic** (5 minutes): Scale up, monitor for regressions
    - **100% traffic**: Full deployment, complete rollout

    **Error Rate Thresholds**:
    - **Staging**: <1% (10 errors per 1000 requests)
    - **Production**: <0.1% (1 error per 1000 requests)

    **Automatic Rollback**:
    - Triggered if error rate exceeds threshold during canary period
    - Rollback command: `kubectl rollout undo deployment/atom`
    - Rollback timeout: 5 minutes

    ### Monitoring Canary Deployment

    **Check canary traffic percentage**:
    ```bash
    curl "$PROMETHEUS_URL/api/v1/query?query=canary_traffic_percentage"
    ```

    **Check error rate during canary**:
    ```bash
    curl "$PROMETHEUS_URL/api/v1/query?query=error_rate{environment=\"production\"}"
    ```

    **Expected output**:
    ```
    # 10% canary
    {"result": "10.0"}

    # 50% canary
    {"result": "50.0"}

    # 100% canary
    {"result": "100.0"}
    ```

    ---

    ## Monitoring

    ### Prometheus Metrics

    **Key Metrics**:
    - `deployment_total`: Total deployments (success, failed, rolled_back)
    - `deployment_duration_seconds`: Deployment duration histogram
    - `deployment_rollback_total`: Total rollbacks by reason
    - `canary_traffic_percentage`: Current canary traffic percentage
    - `smoke_test_total`: Smoke test results (passed, failed)
    - `prometheus_query_total`: Prometheus query success rate

    **Query Examples**:
    ```bash
    # Deployment success rate
    curl "$PROMETHEUS_URL/api/v1/query?query=\
      sum(rate(deployment_total{status=\"success\"}[5m])) / \
      sum(rate(deployment_total[5m])) * 100"

    # Deployment rollback rate
    curl "$PROMETHEUS_URL/api/v1/query?query=\
      sum(rate(deployment_rollback_total[5m])) by (environment)"

    # Smoke test pass rate
    curl "$PROMETHEUS_URL/api/v1/query?query=\
      sum(rate(smoke_test_total{result=\"passed\"}[5m])) / \
      sum(rate(smoke_test_total[5m])) * 100"
    ```

    ### Grafana Dashboards

    **Deployment Overview Dashboard**:
    - URL: `https://grafana.example.com/d/atom-deployment-overview`
    - Panels:
      - Deployment Success Rate
      - Deployment Rollback Rate
      - Smoke Test Pass Rate
      - Deployment Duration
      - Canary Traffic Percentage

    **API Performance Dashboard**:
    - URL: `https://grafana.example.com/d/atom-api-performance`
    - Panels:
      - Request Rate
      - Error Rate
      - Latency P50, P95, P99
      - Endpoint Breakdown

    **Update Grafana Dashboards**:
    ```bash
    # Trigger manual dashboard update (if auto-update fails)
    curl -X POST "$GRAFANA_URL/api/dashboards/db" \
      -H "Authorization: Bearer $GRAFANA_API_KEY" \
      -H "Content-Type: application/json" \
      -d @backend/monitoring/grafana/deployment-overview.json
    ```

    ---

    ## Troubleshooting

    See [CI/CD Troubleshooting Guide](./CI_CD_TROUBLESHOOTING.md) for common deployment issues and solutions.

    ---

    ## Related Documentation

    - [CI/CD Runbook](./CI_CD_RUNBOOK.md)
    - [CI/CD Troubleshooting](./CI_CD_TROUBLESHOOTING.md)
    - [Monitoring Setup](./MONITORING_SETUP.md)
    - [Kubernetes Deployment](./KUBERNETES_DEPLOYMENT.md)
    ```

    **Key sections**:
    - Prerequisites (tools, access, secrets)
    - Environment setup (Kubernetes, database, smoke test user)
    - Step-by-step deployment procedures with Mermaid diagrams
    - Health check endpoints (liveness, readiness, database)
    - Canary deployment strategy with monitoring
    - Prometheus metrics and Grafana dashboards
  </action>
  <verify>
    Run: grep -c "```" backend/docs/DEPLOYMENT_GUIDE.md returns >= 20 (20+ code blocks)
    Run: wc -l backend/docs/DEPLOYMENT_GUIDE.md returns >= 600 (600+ lines)
    Run: grep "mermaid" backend/docs/DEPLOYMENT_GUIDE.md returns >= 2
  </verify>
  <done>
    Deployment guide created with step-by-step procedures, Mermaid diagrams, health check documentation, canary deployment strategy, and monitoring instructions.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create CI/CD troubleshooting guide</name>
  <files>backend/docs/CI_CD_TROUBLESHOOTING.md</files>
  <action>
    Create backend/docs/CI_CD_TROUBLESHOOTING.md with common issues and solutions:

    ```markdown
    # CI/CD Troubleshooting Guide

    **Purpose**: Common CI/CD failures and solutions
    **Last Updated**: 2026-02-20
    **Audience**: DevOps engineers, developers

    ## Table of Contents

    1. [CI Pipeline Failures](#ci-pipeline-failures)
    2. [Docker Build Failures](#docker-build-failures)
    3. [Deployment Failures](#deployment-failures)
    4. [Smoke Test Failures](#smoke-test-failures)
    5. [Monitoring Issues](#monitoring-issues)
    6. [Rollback Issues](#rollback-issues)

    ---

    ## CI Pipeline Failures

    ### Issue: Tests Failing in CI but Passing Locally

    **Symptoms**:
    - CI workflow fails at backend-test-full job
    - Tests pass when run locally: `pytest tests/`
    - Error logs show "UNIQUE constraint failed" or "database locked"

    **Root Causes**:
    1. **Test state leakage**: Tests sharing state via database or file system
    2. **Parallel test execution**: Tests not isolated for pytest-xdist
    3. **Missing fixtures**: CI-specific fixtures not available locally
    4. **Environment differences**: CI environment variables not set locally

    **Solutions**:

    **Solution 1: Check test isolation**
    ```bash
    # Run tests in random order (detects ordering dependencies)
    pytest tests/ --random-order -v

    # If tests fail, fix isolation issues:
    # - Use unique_resource_name fixture for all test resources
    # - Use db_session fixture with transaction rollback
    # - Avoid global variables in tests
    # - Prefer function-scoped fixtures over session/class-scoped
    ```

    **Solution 2: Run tests in sequential mode**
    ```bash
    # Run tests sequentially (disable parallel execution)
    pytest tests/ -v --maxfail=10

    # If sequential tests pass, parallel tests fail:
    # - Test has shared state issue
    # - Add unique_resource_name fixture
    # - Use file lock for session fixtures
    ```

    **Solution 3: Check CI environment variables**
    ```bash
    # View CI environment variables in workflow log
    gh run view --log | grep "DATABASE_URL"
    gh run view --log | grep "ATOM_"

    # Set missing variables locally
    export DATABASE_URL="sqlite:///:memory:"
    export ATOM_DISABLE_LANCEDB=true
    export ATOM_MOCK_DATABASE=true

    # Re-run tests locally
    pytest tests/ -v
    ```

    **Prevention**:
    - Always use `unique_resource_name` fixture for test resources
    - Use `db_session` fixture with automatic rollback
    - Run tests with `--random-order` before pushing
    - Add CI environment variables to local `.env` file

    ---

    ### Issue: Import Errors in CI

    **Symptoms**:
    - CI workflow fails at "Verify backend imports" step
    - Error: `ModuleNotFoundError: No module named 'xxx'`
    - Tests pass locally but imports fail in CI

    **Root Causes**:
    1. **Missing dependency**: Package not in requirements.txt
    2. **Import order**: Module imported before installation
    3. **Python path**: PYTHONPATH not set correctly in CI

    **Solutions**:

    **Solution 1: Check requirements.txt**
    ```bash
    # Verify missing package
    grep "package-name" backend/requirements.txt

    # Add missing package
    echo "package-name==1.0.0" >> backend/requirements.txt

    # Commit and push
    git add requirements.txt
    git commit -m "Add missing dependency: package-name"
    git push
    ```

    **Solution 2: Verify import order**
    ```bash
    # Check debug_ci_imports.py
    cat backend/debug_ci_imports.py

    # Ensure imports after pip install
    # - Import errors usually mean circular dependencies
    # - Use lazy imports (import inside function) if needed
    ```

    **Solution 3: Set PYTHONPATH in CI**
    ```yaml
    # In .github/workflows/ci.yml
    - name: Run tests
      env:
        PYTHONPATH: /Users/runner/work/atom/atom/backend
      run: |
        pytest tests/ -v
    ```

    ---

    ### Issue: Type Checking (MyPy) Failures

    **Symptoms**:
    - CI workflow fails at type checking step
    - Error: `error: Argument 1 to "function" has incompatible type`
    - MyPy passes locally but fails in CI

    **Root Causes**:
    1. **Missing type hints**: Function parameters not annotated
    2. **Import errors**: MyPy can't find imported modules
    3. **Version mismatch**: Different mypy version locally vs CI

    **Solutions**:

    **Solution 1: Add type hints**
    ```python
    # Before (missing type hints):
    def process_agent(agent_id, query):
        pass

    # After (with type hints):
    from typing import Dict, Any

    def process_agent(agent_id: str, query: str) -> Dict[str, Any]:
        pass
    ```

    **Solution 2: Ignore specific errors**
    ```python
    # Add type: ignore comment for false positives
    result = external_library.function(arg)  # type: ignore
    ```

    **Solution 3: Configure mypy**
    ```ini
    # In mypy.ini
    [mypy]
    ignore_missing_imports = True
    warn_return_any = False
    ```

    ---

    ## Docker Build Failures

    ### Issue: Docker Build Timeout

    **Symptoms**:
    - Build job fails after 10+ minutes
    - Error: "context canceled"
    - Build hangs at "COPY requirements.txt"

    **Root Causes**:
    1. **Large build context**: Too many files copied into Docker image
    2. **Slow network**: Pip downloads taking too long
    3. **Cache miss**: No layer caching, rebuilding everything

    **Solutions**:

    **Solution 1: Optimize .dockerignore**
    ```bash
    # Check build context size
    du -sh backend/

    # Add exclusions to .dockerignore
    echo "tests/" >> backend/.dockerignore
    echo "*.md" >> backend/.dockerignore
    echo ".git/" >> backend/.dockerignore

    # Re-run build
    docker build -t atom-backend:test ./backend
    ```

    **Solution 2: Use BuildKit cache**
    ```yaml
    # In .github/workflows/ci.yml
    - name: Build Docker image
      uses: docker/build-push-action@v5
      with:
        cache-from: type=gha,mode=max
        cache-to: type=gha,mode=max
    ```

    **Solution 3: Increase timeout**
    ```yaml
    # In .github/workflows/ci.yml
    - name: Build Docker image
      timeout-minutes: 30  # Increase from default 10
      uses: docker/build-push-action@v5
    ```

    ---

    ### Issue: Docker Layer Cache Not Working

    **Symptoms**:
    - Build takes same time every run (no improvement)
    - Docker logs show "CACHED" for 0 layers
    - cache-from step shows "Cache not found"

    **Root Causes**:
    1. **mode=min**: Only final layer cached
    2. **Cache key mismatch**: cache-from and cache-to keys don't match
    3. **BuildKit not enabled**: Using legacy builder

    **Solutions**:

    **Solution 1: Switch to mode=max**
    ```yaml
    # In .github/workflows/ci.yml
    cache-to: type=gha,mode=max  # Was: mode=min
    ```

    **Solution 2: Match cache keys**
    ```yaml
    # Ensure cache-from and cache-to match
    cache-from: type=gha,scope=buildx
    cache-to: type=gha,mode=max,scope=buildx
    ```

    **Solution 3: Enable BuildKit**
    ```yaml
    # In .github/workflows/ci.yml
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    ```

    ---

    ## Deployment Failures

    ### Issue: kubectl Command Failed

    **Symptoms**:
    - Deploy job fails at "Update deployment image" step
    - Error: "error: you must be logged in to the server"
    - kubectl commands fail with "Unauthorized"

    **Root Causes**:
    1. **Invalid kubeconfig**: KUBECONFIG secret expired or invalid
    2. **Wrong context**: kubectl pointing to wrong cluster
    3. **RBAC denied**: Service account lacks permissions

    **Solutions**:

    **Solution 1: Refresh kubeconfig secret**
    ```bash
    # Generate new kubeconfig
    base64 ~/.kube/config-staging > kubeconfig-staging.base64

    # Update GitHub secret
    gh secret set KUBECONFIG_STAGING < kubeconfig-staging.base64

    # Re-run deployment
    gh workflow run deploy.yml
    ```

    **Solution 2: Verify kubectl context**
    ```bash
    # Check current context
    kubectl config current-context

    # Set correct context
    kubectl config use-context staging-context

    # Verify cluster connectivity
    kubectl cluster-info
    ```

    **Solution 3: Check RBAC permissions**
    ```bash
    # Check service account permissions
    kubectl auth can-i update deployments/deploy
    kubectl auth can-i rollout undo deployment/atom

    # If denied, update RBAC rules
    kubectl apply -f k8s/rbac.yaml
    ```

    ---

    ### Issue: Smoke Tests Fail with 401 Unauthorized

    **Symptoms**:
    - Smoke tests fail immediately after deployment
    - Error: "HTTP/1.1 401 Unauthorized"
    - Login request returns null token

    **Root Causes**:
    1. **Missing smoke test user**: User not created in database
    2. **Wrong credentials**: SMOKE_TEST_PASSWORD mismatch
    3. **Database migration failed**: smoke_test user not created

    **Solutions**:

    **Solution 1: Create smoke test user**
    ```bash
    # Check if user exists
    psql -c "SELECT username FROM users WHERE username='smoke_test'"

    # If not exists, create migration
    alembic revision -m "create smoke test user"
    # Edit migration file to create user
    alembic upgrade head
    ```

    **Solution 2: Update GitHub secrets**
    ```bash
    # Set smoke test credentials
    gh secret set SMOKE_TEST_USERNAME <<< "smoke_test"
    gh secret set SMOKE_TEST_PASSWORD <<< "changeme123"

    # Re-run deployment
    gh workflow run deploy.yml
    ```

    **Solution 3: Verify user is active**
    ```bash
    # Check user status
    psql -c "SELECT username, is_active, is_smoke_test_user FROM users WHERE username='smoke_test'"

    # If inactive, activate
    psql -c "UPDATE users SET is_active=true WHERE username='smoke_test'"
    ```

    ---

    ## Monitoring Issues

    ### Issue: Prometheus Query Fails

    **Symptoms**:
    - Monitoring check step fails
    - Error: "Connection refused" or "timeout"
    - Smoke tests pass but deployment fails

    **Root Causes**:
    1. **Prometheus unreachable**: PROMETHEUS_URL not set or wrong
    2. **Query syntax error**: Invalid PromQL expression
    3. **Timeout**: Query takes too long (>10 seconds)

    **Solutions**:

    **Solution 1: Verify Prometheus URL**
    ```bash
    # Check if PROMETHEUS_URL secret is set
    gh secret list | grep PROMETHEUS_URL

    # Test Prometheus connectivity
    curl "$PROMETHEUS_URL/-/healthy"

    # If unreachable, skip monitoring (deployment continues)
    # Monitoring check is non-blocking (graceful degradation)
    ```

    **Solution 2: Validate PromQL query**
    ```bash
    # Test query in Prometheus UI
    # Visit: http://prometheus:9090/graph
    # Enter query: sum(rate(http_requests_total[5m]))
    # Click "Execute"

    # Fix query syntax errors
    # Common errors:
    # - Missing quotes: status=~"5.." (correct)
    # - Wrong function: rate(deployment_total) (correct: sum(rate(...)))
    ```

    **Solution 3: Increase query timeout**
    ```yaml
    # In .github/workflows/deploy.yml
    curl -s -G "$PROMETHEUS_URL/api/v1/query" \
      --max-time 30 \  # Increase from default 10
      --data-urlencode "query=$QUERY"
    ```

    ---

    ### Issue: Grafana Dashboard Update Fails

    **Symptoms**:
    - Dashboard update step fails
    - Error: "401 Unauthorized" or "404 Not Found"
    - Grafana API returns error

    **Root Causes**:
    1. **Invalid API key**: GRAFANA_API_KEY expired or wrong
    2. **Wrong dashboard UID**: Dashboard not found in Grafana
    3. **Permission denied**: Service account lacks dashboard update permission

    **Solutions**:

    **Solution 1: Refresh Grafana API key**
    ```bash
    # Create new service account token in Grafana
    # Visit: http://grafana:3000/org/serviceaccounts
    # Create service account â†’ Add token â†’ Copy token

    # Update GitHub secret
    gh secret set GRAFANA_API_KEY <<< "eyJr..."

    # Re-run deployment
    gh workflow run deploy.yml
    ```

    **Solution 2: Verify dashboard UID**
    ```bash
    # Check dashboard exists
    curl -X GET "$GRAFANA_URL/api/dashboards/uid/atom-deployment-overview" \
      -H "Authorization: Bearer $GRAFANA_API_KEY"

    # If 404, create dashboard first
    curl -X POST "$GRAFANA_URL/api/dashboards/db" \
      -H "Authorization: Bearer $GRAFANA_API_KEY" \
      -H "Content-Type: application/json" \
      -d @backend/monitoring/grafana/deployment-overview.json
    ```

    **Solution 3: Check service account permissions**
    ```bash
    # Verify service account has admin role
    curl -X GET "$GRAFANA_URL/api/serviceaccounts" \
      -H "Authorization: Bearer $GRAFANA_API_KEY" | jq

    # If role is Viewer or Editor, update to Admin
    # Visit: http://grafana:3000/org/serviceaccounts
    # Edit service account â†’ Role: Admin
    ```

    ---

    ## Rollback Issues

    ### Issue: Rollback Command Hangs

    **Symptoms**:
    - Rollback step starts but never completes
    - kubectl rollout status waits indefinitely
    - Deployment stuck in "Progressing" state

    **Root Causes**:
    1. **Pods not ready**: New pods failing to start
    2. **Missing replicas**: Deployment has 0 replicas
    3. **Resource limits**: Insufficient CPU/memory

    **Solutions**:

    **Solution 1: Check pod status**
    ```bash
    # List pods with status
    kubectl get pods -l app=atom

    # Describe failing pod
    kubectl describe pod <pod-name>

    # Check pod logs
    kubectl logs <pod-name>
    ```

    **Solution 2: Force rollback timeout**
    ```bash
    # Rollback with shorter timeout
    kubectl rollout undo deployment/atom --timeout=1m

    # If timeout fails, scale deployment manually
    kubectl scale deployment atom --replicas=0
    kubectl scale deployment atom --replicas=3
    ```

    **Solution 3: Check resource limits**
    ```bash
    # View deployment resource limits
    kubectl describe deployment atom | grep -A 5 "Limits"

    # Increase resource limits if needed
    kubectl set resources deployment atom \
      --limits=cpu=2000m,memory=4Gi \
      --requests=cpu=1000m,memory=2Gi
    ```

    ---

    ## Escalation Procedures

    **When to escalate**:
    - Issue not resolved after 30 minutes
    - Production deployment blocked
    - Customer impact (P0/P1 incident)

    **Escalation contacts**:
    - **DevOps Lead**: devops-lead@atom.example.com
    - **Engineering Manager**: eng-mgr@atom.example.com
    - **On-Call Engineer**: on-call@atom.example.com

    **Escalation template**:
    ```markdown
    ## CI/CD Issue Escalation

    **Issue**: [Brief description]
    **Impact**: [Production/Staging blocked]
    **Duration**: [X minutes]
    **Attempted Solutions**: [List steps tried]
    **Error Logs**: [Paste relevant logs]
    **Workflow Run**: [GitHub Actions link]

    **Request**: [Specific help needed]
    ```
    ```

    **Key sections**:
    - CI pipeline failures (test failures, import errors, type checking)
    - Docker build failures (timeout, cache issues)
    - Deployment failures (kubectl errors, smoke test failures)
    - Monitoring issues (Prometheus queries, Grafana updates)
    - Rollback issues (hangs, pods not ready)
    - Escalation procedures with templates
  </action>
  <verify>
    Run: grep -c "### Issue" backend/docs/CI_CD_TROUBLESHOOTING.md returns >= 10 (10+ issues)
    Run: wc -l backend/docs/CI_CD_TROUBLESHOOTING.md returns >= 500 (500+ lines)
    Run: grep "Solution" backend/docs/CI_CD_TROUBLESHOOTING.md | wc -l returns >= 20
  </verify>
  <done>
    CI/CD troubleshooting guide created with common issues and solutions. Covers CI failures, Docker build issues, deployment failures, smoke test failures, monitoring issues, and rollback problems.
  </done>
</task>

<task type="auto">
  <name>Task 4: Create workflow README with architecture diagrams</name>
  <files>.github/workflows/README.md</files>
  <action>
    Create .github/workflows/README.md with workflow documentation:

    ```markdown
    # GitHub Actions Workflows

    **Purpose**: Documentation for Atom CI/CD workflows
    **Last Updated**: 2026-02-20

    ## Workflow Overview

    Atom uses GitHub Actions for continuous integration and deployment. Workflows are defined in `.github/workflows/` directory.

    ---

    ## Workflows

    ### 1. CI Pipeline (ci.yml)

    **Purpose**: Run tests, linting, type checking, and build Docker images
    **Trigger**: Push to `main` or `develop` branches, pull requests
    **Duration**: 15-20 minutes

    **Jobs**:
    1. **backend-test**: Import verification (1 minute)
    2. **frontend-build**: Frontend build and artifact upload (2 minutes)
    3. **backend-test-full**: Full test suite with coverage (10 minutes)
    4. **test-quality-gates**: Test independence, pass rate, performance (5 minutes)
    5. **build-docker**: Docker image build with caching (3 minutes)

    **Architecture**:
    ```mermaid
    graph TB
      A[Push/PR] --> B[backend-test]
      A --> C[frontend-build]
      B --> D[backend-test-full]
      C --> E[Upload Artifacts]
      D --> F[test-quality-gates]
      D --> G[build-docker]
    ```

    **Quality Gates**:
    - **TQ-01**: Test independence (random order execution)
    - **TQ-02**: Test pass rate (>=98%)
    - **TQ-03**: Test performance (<60 minutes)
    - **TQ-04**: Test determinism (flaky test detection)
    - **TQ-05**: Coverage quality (>=25%)

    **Artifacts**:
    - Frontend build: `frontend-nextjs-build`
    - Coverage reports: `coverage-reports`

    ---

    ### 2. Deploy Pipeline (deploy.yml)

    **Purpose**: Deploy to staging and production environments
    **Trigger**: Push to `main` branch (staging auto, production manual)
    **Duration**: 10-15 minutes

    **Jobs**:
    1. **test**: Unit and integration tests (5 minutes)
    2. **build**: Docker image build and push (3 minutes)
    3. **deploy-staging**: Deploy to staging (5 minutes)
    4. **deploy-production**: Deploy to production with canary (10 minutes)

    **Architecture**:
    ```mermaid
    graph TB
      A[Push to main] --> B[test]
      B --> C[build]
      C --> D[deploy-staging]
      D --> E{Smoke Tests Pass?}
      E -->|No| F[Automatic Rollback]
      E -->|Yes| G[Manual Approval]
      G --> H[deploy-production]
      H --> I[Canary 10% â†’ 50% â†’ 100%]
      I --> J{Production Smoke Tests?}
      J -->|No| F
      J -->|Yes| K[Deployment Complete]
    ```

    **Deployment Environments**:
    - **Staging**: Automatic deployment on push to `main`
    - **Production**: Manual approval required (GitHub environment protection)

    **Canary Deployment**:
    - **10% traffic** (5 minutes): Monitor error rate
    - **50% traffic** (5 minutes): Scale up, monitor
    - **100% traffic**: Full rollout

    **Automatic Rollback**:
    - Triggered by: Smoke test failure, high error rate, high latency
    - Rollback command: `kubectl rollout undo deployment/atom`
    - Rollback timeout: 5 minutes

    ---

    ### 3. LanceDB Integration Tests (lancedb-integration.yml)

    **Purpose**: Run LanceDB integration tests with real service
    **Trigger**: Push to `main` or `develop` branches, pull requests, manual
    **Duration**: 15-20 minutes

    **Jobs**:
    1. **lancedb-integration**: LanceDB integration tests (15 minutes)

    **Architecture**:
    ```mermaid
    graph LR
      A[Push/PR] --> B[Start LanceDB Service]
      B --> C[Run Integration Tests]
      C --> D[Upload Test Results]
    ```

    **Service Container**:
    - **Image**: `lancedb/lancedb:latest`
    - **Port**: 8080
    - **Health check**: `curl -f http://localhost:8080/health`

    **Environment Variables**:
    - `LANCEDB_URI`: `sqlite:///tmp/lancedb_test.db`
    - `ATOM_DISABLE_LANCEDB`: `false` (use real service)

    ---

    ## Job Dependencies

    ### CI Workflow Dependencies

    ```
    backend-test (independent)
    frontend-build (independent)
       â†“
    backend-test-full (depends on backend-test)
       â†“
    test-quality-gates (depends on backend-test-full)
    build-docker (depends on backend-test-full)
    ```

    ### Deploy Workflow Dependencies

    ```
    test
       â†“
    build (depends on test)
       â†“
    deploy-staging (depends on build)
       â†“
    deploy-production (depends on deploy-staging)
    ```

    ---

    ## Environment Variables

    ### CI Workflow Variables

    | Variable | Purpose | Default |
    |----------|---------|---------|
    | `DATABASE_URL` | Test database | `sqlite:///:memory:` |
    | `BYOK_ENCRYPTION_KEY` | Encryption key | `test_key_for_ci_only` |
    | `ENVIRONMENT` | Environment name | `test` |
    | `ATOM_DISABLE_LANCEDB` | Disable LanceDB | `true` |
    | `ATOM_MOCK_DATABASE` | Mock database | `true` |

    ### Deploy Workflow Variables (Secrets)

    | Variable | Purpose | Required |
    |----------|---------|----------|
    | `KUBECONFIG_STAGING` | Staging cluster config | Yes |
    | `KUBECONFIG_PRODUCTION` | Production cluster config | Yes |
    | `REGISTRY_USERNAME` | Container registry username | Yes |
    | `REGISTRY_PASSWORD` | Container registry password | Yes |
    | `SMOKE_TEST_USERNAME` | Smoke test user | Yes |
    | `SMOKE_TEST_PASSWORD` | Smoke test password | Yes |
    | `STAGING_URL` | Staging URL | Yes |
    | `PRODUCTION_URL` | Production URL | Yes |
    | `PROMETHEUS_URL` | Monitoring endpoint | No |
    | `GRAFANA_URL` | Dashboard URL | No |
    | `GRAFANA_API_KEY` | Grafana API key | No |
    | `SLACK_WEBHOOK_URL` | Slack webhook | No |

    ---

    ## Quality Gates

    ### TQ-01: Test Independence

    **Purpose**: Validate tests run independently without ordering dependencies

    **Enforcement**: Random order test execution

    **Job**: `test-quality-gates` â†’ `TQ-01 Check test independence`

    **Pass Criteria**: All tests pass in random order

    ### TQ-02: Test Pass Rate

    **Purpose**: Ensure 98% minimum test pass rate

    **Enforcement**: Pass rate calculated from pytest JSON output

    **Job**: `test-quality-gates` â†’ `TQ-02 Check test pass rate`

    **Pass Criteria**: Pass rate >=98%

    ### TQ-03: Test Performance

    **Purpose**: Ensure test suite completes in reasonable time

    **Enforcement**: Test duration measured

    **Job**: `backend-test-full` â†’ Test execution time

    **Pass Criteria**: Full test suite <60 minutes

    ### TQ-04: Test Determinism

    **Purpose**: Ensure tests produce consistent results

    **Enforcement**: Flaky test detection with retries

    **Job**: `test-quality-gates` â†’ `TQ-02 Check test pass rate` (includes `--reruns 2`)

    **Pass Criteria**: Tests pass after 2 retries

    ### TQ-05: Coverage Quality

    **Purpose**: Ensure adequate code coverage

    **Enforcement**: Coverage percentage measured

    **Job**: `backend-test-full` â†’ Coverage report

    **Pass Criteria**: Coverage >=25%

    ---

    ## Troubleshooting

    See [CI/CD Troubleshooting Guide](../backend/docs/CI_CD_TROUBLESHOOTING.md) for common issues and solutions.

    ---

    ## Related Documentation

    - [CI/CD Runbook](../backend/docs/CI_CD_RUNBOOK.md)
    - [Deployment Guide](../backend/docs/DEPLOYMENT_GUIDE.md)
    - [Monitoring Setup](../backend/docs/MONITORING_SETUP.md)
    ```

    **Key sections**:
    - Workflow overview with architecture diagrams
    - Job descriptions and dependencies
    - Environment variables and secrets
    - Quality gates documentation
    - Links to detailed documentation
  </action>
  <verify>
    Run: grep -c "```" .github/workflows/README.md returns >= 10 (10+ code blocks)
    Run: wc -l .github/workflows/README.md returns >= 400 (400+ lines)
    Run: grep "mermaid" .github/workflows/README.md returns >= 2
  </verify>
  <done>
    Workflow README created with architecture diagrams, job descriptions, dependencies, environment variables, and quality gates documentation.
  </done>
</task>

</tasks>

<verification>
Overall phase verification steps:

1. **Documentation Completeness Verification**:
   ```bash
   # Verify all documentation files created
   ls -lh backend/docs/CI_CD_RUNBOOK.md
   ls -lh backend/docs/DEPLOYMENT_GUIDE.md
   ls -lh backend/docs/CI_CD_TROUBLESHOOTING.md
   ls -lh .github/workflows/README.md

   # Verify minimum line counts
   wc -l backend/docs/CI_CD_RUNBOOK.md  # Should be >=800
   wc -l backend/docs/DEPLOYMENT_GUIDE.md  # Should be >=600
   wc -l backend/docs/CI_CD_TROUBLESHOOTING.md  # Should be >=500
   wc -l .github/workflows/README.md  # Should be >=400
   ```

2. **Documentation Quality Verification**:
   ```bash
   # Verify Mermaid diagrams present
   grep -c "mermaid" backend/docs/DEPLOYMENT_GUIDE.md  # Should be >=2
   grep -c "mermaid" .github/workflows/README.md  # Should be >=2

   # Verify code blocks present
   grep -c "```" backend/docs/CI_CD_RUNBOOK.md  # Should be >=20
   grep -c "```" backend/docs/CI_CD_TROUBLESHOOTING.md  # Should be >=20

   # Verify command examples present
   grep -c "kubectl" backend/docs/CI_CD_RUNBOOK.md  # Should be >=10
   grep -c "curl" backend/docs/DEPLOYMENT_GUIDE.md  # Should be >=10
   ```

3. **Documentation Cross-References Verification**:
   ```bash
   # Verify cross-references between documents
   grep "CI_CD_TROUBLESHOOTING.md" backend/docs/CI_CD_RUNBOOK.md  # Should match
   grep "DEPLOYMENT_GUIDE.md" backend/docs/CI_CD_RUNBOOK.md  # Should match
   grep "CI_CD_RUNBOOK.md" backend/docs/DEPLOYMENT_GUIDE.md  # Should match

   # Verify workflow references
   grep "ci.yml" .github/workflows/README.md  # Should match
   grep "deploy.yml" .github/workflows/README.md  # Should match
   ```

4. **Documentation Accuracy Verification**:
   ```bash
   # Verify commands match actual workflows
   grep "kubectl rollout undo" backend/docs/CI_CD_RUNBOOK.md
   grep "kubectl rollout undo" .github/workflows/deploy.yml  # Should both exist

   # Verify environment variables documented
   grep "PROMETHEUS_URL" backend/docs/DEPLOYMENT_GUIDE.md
   grep "PROMETHEUS_URL" .github/workflows/deploy.yml  # Should both exist

   # Verify quality gates documented
   grep "TQ-01" backend/docs/CI_CD_RUNBOOK.md
   grep "TQ-01" .github/workflows/ci.yml  # Should both exist
   ```

**Success Criteria**:
- [ ] All 4 documentation files created (runbook, deployment guide, troubleshooting guide, workflow README)
- [ ] Minimum line counts met (800, 600, 500, 400)
- [ ] Mermaid diagrams included (>=2 per document with diagrams)
- [ ] Code blocks included (>=20 per document with code examples)
- [ ] Cross-references between documents
- [ ] Commands match actual workflows (kubectl, curl, etc.)
- [ ] Environment variables documented
- [ ] Quality gates documented
</verification>

<success_criteria>
**CI/CD Pipeline Documentation Complete When:**

1. **Runbook Created**: Comprehensive operational procedures (800+ lines) with deployment, rollback, verification, emergency procedures
2. **Deployment Guide Created**: Step-by-step instructions (600+ lines) with environment setup, health checks, canary deployment
3. **Troubleshooting Guide Created**: Common issues and solutions (500+ lines) covering CI, Docker, deployment, monitoring, rollback
4. **Workflow README Created**: Architecture diagrams (400+ lines) with job descriptions, dependencies, environment variables

**Measurable Outcomes**:
- Total documentation: 2,300+ lines across 4 documents
- Code examples: 60+ code blocks with copy-paste commands
- Diagrams: 4+ Mermaid diagrams for workflow visualization
- Cross-references: All documents link to related documentation
- Accuracy: Commands and variables verified against actual workflows
</success_criteria>

<output>
After completion, create `.planning/phases/67-ci-cd-pipeline-fixes/67-05-SUMMARY.md` with:
- List of all 4 documentation files created with line counts
- Number of code examples and diagrams in each document
- Cross-reference verification results
- Command accuracy verification results
- Documentation quality metrics (readability score, completeness percentage)
- Links to all documentation files
</output>
