---
phase: 67-ci-cd-pipeline-fixes
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - .github/workflows/ci.yml
  - backend/requirements-testing.txt
  - backend/tests/conftest.py
  - backend/alembic/versions/
autonomous: true
user_setup: []

must_haves:
  truths:
    - "All 14+ ignored test files are properly mocked and can run in CI"
    - "Test pass rate quality gate enforces 98% minimum threshold"
    - "Tests run in random order without failures (test independence validated)"
    - "Smoke tests include proper authentication and pass without 401 errors"
    - "LanceDB integration tests run in separate CI job with service container"
    - "Governance exam tests mock Knowledge Graph service dependencies"
    - "Analytics tests mock Prometheus/Grafana external dependencies"
  artifacts:
    - path: .github/workflows/ci.yml
      provides: "Updated CI workflow with removed test ignores and proper quality gates"
      min_lines: 400
      exports: ["backend-test-full", "test-quality-gates", "lancedb-integration-tests"]
    - path: .github/workflows/lancedb-integration.yml
      provides: "Separate workflow for LanceDB integration tests with service container"
      min_lines: 80
    - path: backend/tests/conftest.py
      provides: "Centralized fixtures for external service mocking (LanceDB, Knowledge Graph, Prometheus)"
      min_lines: 350
    - path: backend/requirements-testing.txt
      provides: "pytest-random-order and pytest-rerunfailures plugins for test stability"
      contains: "pytest-random-order"
    - path: backend/alembic/versions/XXXX_create_smoke_test_user.py
      provides: "Migration creating smoke_test user with known credentials"
      min_lines: 50
  key_links:
    - from: ".github/workflows/ci.yml"
      to: "backend/tests/conftest.py"
      via: "ATOM_MOCK_LANCEDB, ATOM_MOCK_KG environment variables"
      pattern: "ATOM_MOCK_.*=true"
    - from: "backend/tests/conftest.py"
      to: ".github/workflows/lancedb-integration.yml"
      via: "Service container configuration for LanceDB"
      pattern: "lancedb_service:"
    - from: ".github/workflows/ci.yml"
      to: "backend/alembic/versions/XXXX_create_smoke_test_user.py"
      via: "Smoke test authentication setup"
      pattern: "SMOKE_TEST_.*_CREDENTIALS"
---

<objective>
Stabilize test suite by removing all test ignores, implementing proper external service mocking, and enforcing 98% pass rate quality gate.

Purpose: CI pipeline currently ignores 14+ test files due to external dependencies, resulting in incomplete test coverage and unreliable quality gates. This plan fixes the root causes by mocking external services and creating dedicated integration test workflows.

Output: 100% of test files run in CI, 98% pass rate enforced, test independence validated via random order execution.
</objective>

<execution_context>
@/Users/rushiparikh/.claude/get-shit-done/workflows/execute-plan.md
@/Users/rushiparikh/.claude/get-shit-done/templates/summary.md

@.planning/phases/67-ci-cd-pipeline-fixes/67-RESEARCH.md
@.github/workflows/ci.yml
@backend/tests/conftest.py
@backend/requirements-testing.txt
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Research Context
Phase 67 research identified 14+ ignored test files in CI with root causes:
- **LanceDB Integration Tests** (4 files): External service not available in CI
- **Governance Exam Tests** (1 file): Missing Knowledge Graph or rule validation service
- **Analytics Tests** (4 files): Prometheus/Grafana external dependencies not mocked
- **Chat Interface Tests** (5 files): WebSocket server dependency not available in CI

Current quality gates (TQ-01 through TQ-05) are defined but mostly informational. Pass rate calculation has TODO comments indicating incomplete implementation. Coverage threshold at 25% is too low compared to TQ-05's 50% standard.

**Key Decision**: Use mocking over service containers for most external dependencies to keep CI fast. Only LanceDB gets dedicated service container workflow due to heavy integration testing requirements.

**Standard Stack**: pytest-random-order for test independence validation, pytest-rerunfailures for flaky test automatic retries, responses library for HTTP mocking, unittest.mock for service mocking.
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create centralized external service mocking fixtures</name>
  <files>backend/tests/conftest.py</files>
  <action>
    Add pytest fixtures for mocking external services:

    1. **mock_lancedb_client fixture** (lines 50-100):
       - Mock lancedb.connect() to return in-memory LanceDB client
       - Create mock tables with UUID-based table names for isolation
       - Use monkeypatch to replace lancedb.connect throughout test suite
       - Environment check: if ATOM_DISABLE_LANCEDB=true, use mock; else use real service

    2. **mock_knowledge_graph fixture** (lines 100-150):
       - Mock KnowledgeGraph validation service with constitutional rules
       - Return mock compliance results (passed: true, score: 0.95)
       - Mock get_constitutional_rules() to return test rule set
       - Use unittest.mock.MagicMock for flexible method mocking

    3. **mock_prometheus_client fixture** (lines 150-200):
       - Mock prometheus_client.Gauge(), Counter(), Histogram()
       - Track metric calls in memory dictionary for verification
       - Mock start_http_server() to skip port binding
       - Return mock registry for test assertions

    4. **mock_grafana_client fixture** (lines 200-250):
       - Mock Grafana API calls for dashboard updates
       - Use responses library to intercept HTTP requests
       - Return mock dashboard JSON for testing
       - Verify dashboard update calls in tests

    Add imports at top:
    ```python
    import pytest
    import unittest.mock
    from unittest.mock import MagicMock, AsyncMock, patch
    import responses
    ```

    Use unique_resource_name fixture (from Phase 01-test-infrastructure) for all test resource naming to prevent parallel test collisions.
  </action>
  <verify>
    Run: pytest backend/tests/test_lancedb_integration.py -v (should pass with mocks enabled)
    Run: pytest backend/tests/test_graduation_exams.py -v (should pass with Knowledge Graph mock)
    Run: pytest backend/tests/test_analytics_dashboard.py -v (should pass with Prometheus/Grafana mocks)
    Check: grep -c "def mock_" backend/tests/conftest.py returns >= 4
  </verify>
  <done>
    All external service fixtures created with proper mocking. Tests that previously failed with "Connection refused" now pass with mock services.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create LanceDB integration test workflow with service container</name>
  <files>.github/workflows/lancedb-integration.yml</files>
  <action>
    Create new GitHub Actions workflow for LanceDB integration tests:

    ```yaml
    name: LanceDB Integration Tests

    on:
      push:
        branches: [ main, develop ]
      pull_request:
        branches: [ main, develop ]
      workflow_dispatch:  # Manual trigger for debugging

    jobs:
      lancedb-integration:
        runs-on: ubuntu-latest
        services:
          lancedb:
            image: lancedb/lancedb:latest
            ports:
              - 8080:8080
            options: >-
              --health-cmd "curl -f http://localhost:8080/health || exit 1"
              --health-interval 10s
              --health-timeout 5s
              --health-retries 5

        steps:
          - uses: actions/checkout@v4

          - name: Set up Python
            uses: actions/setup-python@v5
            with:
              python-version: '3.11'

          - name: Cache pip packages
            uses: actions/cache@v4
            with:
              path: ~/.cache/pip
              key: ${{ runner.os }}-pip-lancedb-${{ hashFiles('backend/requirements*.txt') }}
              restore-keys: |
                ${{ runner.os }}-pip-lancedb-

          - name: Install dependencies
            working-directory: ./backend
            run: |
              python -m pip install --upgrade pip
              pip install -r requirements.txt
              pip install lancedb pytest pytest-asyncio

          - name: Run LanceDB integration tests
            working-directory: ./backend
            env:
              DATABASE_URL: "sqlite:///:memory:"
              LANCEDB_URI: "sqlite:///tmp/lancedb_test.db"
              BYOK_ENCRYPTION_KEY: test_key_for_ci_only
              ENVIRONMENT: test
              ATOM_DISABLE_LANCEDB: false  # Use real LanceDB service
            run: |
              pytest tests/integration/episodes/test_lancedb_integration.py \
                tests/integration/episodes/test_graduation_validation.py \
                tests/integration/episodes/test_episode_lifecycle_lancedb.py \
                -v --tb=short --maxfail=5

          - name: Upload test results
            uses: actions/upload-artifact@v4
            if: always()
            with:
              name: lancedb-test-results
              path: backend/test-results/
              retention-days: 7
    ```

    Key design decisions:
    - Service container uses LanceDB official image with health check
    - Separate job from main CI to avoid slowing down test workflow
    - Uses LANCEDB_URI environment variable for real service connection
    - ATOM_DISABLE_LANCEDB=false overrides CI default to use real service
  </action>
  <verify>
    Run: gh workflow view lancedb-integration (workflow should be visible in GitHub Actions)
    Run: gh workflow run lancedb-integration.yml (manual trigger should succeed)
    Check: grep -c "lancedb" .github/workflows/lancedb-integration.yml returns >= 5
  </verify>
  <done>
    LanceDB integration tests run in dedicated workflow with real service container. Tests previously ignored in main CI now execute in isolation.
  </done>
</task>

<task type="auto">
  <name>Task 3: Remove test ignores from main CI workflow and enforce quality gates</name>
  <files>.github/workflows/ci.yml</files>
  <action>
    Update .github/workflows/ci.yml to remove all --ignore flags and implement pass rate enforcement:

    **Step 1: Remove --ignore flags from backend-test-full job** (line 168):
    ```yaml
    # BEFORE:
    pytest tests/ -v -n auto \
      --ignore=tests/integration/episodes/test_lancedb_integration.py \
      --ignore=tests/integration/episodes/test_graduation_validation.py \
      # ... 12 more ignores

    # AFTER:
    pytest tests/ -v -n auto \
      --cov=core \
      --cov=api \
      --cov=tools \
      --cov-report=html:tests/coverage_reports/html \
      --cov-report=json:tests/coverage_reports/metrics/coverage.json \
      --cov-report=term-missing:skip-covered \
      --maxfail=10
    ```

    **Step 2: Implement pass rate calculation** (line 209-224):
    ```yaml
    - name: Check test pass rate (98%+ required)
      working-directory: ./backend
      if: always()
      run: |
        # Parse pytest output from previous step
        echo "=== Calculating Test Pass Rate ==="
        python tests/scripts/parse_pytest_output.py pytest_report.txt > pass_rate.txt
        PASS_RATE=$(cat pass_rate.txt)
        echo "Pass Rate: $PASS_RATE%"

        # Enforce 98% threshold
        if (( $(echo "$PASS_RATE < 98.0" | bc -l) )); then
          echo "❌ Pass rate below 98% threshold: $PASS_RATE%"
          exit 1
        fi
        echo "✅ Pass rate meets 98% threshold: $PASS_RATE%"
    ```

    **Step 3: Install pytest plugins for quality gates** (line 157):
    ```yaml
    - name: Install dependencies
      working-directory: ./backend
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-testing.txt
        pip install pytest-random-order pytest-rerunfailures pytest-json-report
    ```

    **Step 4: Enable JSON reporting for pass rate parsing** (line 168):
    ```yaml
    pytest tests/ -v -n auto \
      --json-report --json-report-file=pytest_report.json \
      --cov=core \
      --cov=api \
      --cov=tools \
      --cov-report=html:tests/coverage_reports/html \
      --cov-report=json:tests/coverage_reports/metrics/coverage.json \
      --cov-report=term-missing:skip-covered \
      --maxfail=10
    ```

    **Step 5: Update TQ-01 test independence check** (line 273):
    ```yaml
    # Remove all --ignore flags from TQ-01 random order test
    pytest tests/ --random-order --random-order-seed=random \
      -v --tb=short \
      --maxfail=5
    ```
  </action>
  <verify>
    Run: grep -c "\-\-ignore=" .github/workflows/ci.yml returns 0 (all ignores removed)
    Run: grep "pytest-random-order" backend/requirements-testing.txt returns match
    Run: pytest tests/ --json-report --json-report-file=test.json (generates JSON output)
  </verify>
  <done>
    All test files execute in main CI workflow. Test independence validated via random order execution. Pass rate quality gate enforces 98% minimum.
  </done>
</task>

<task type="auto">
  <name>Task 4: Create smoke test user migration and update smoke tests</name>
  <files>backend/alembic/versions/XXXX_create_smoke_test_user.py</files>
  <action>
    Create Alembic migration for smoke test user:

    ```python
    """create smoke test user

    Revision ID: XXXX
    Revises: YYYY
    Create Date: 2026-02-20

    """
    from alembic import op
    import sqlalchemy as sa
    from sqlalchemy.sql import table, column
    from passlib.context import CryptContext

    # revision identifiers, used by Alembic.
    revision = 'XXXX'
    down_revision = 'YYYY'
    branch_labels = None
    depends_on = None

    # Password hashing context
    pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")

    def upgrade() -> None:
        # Create smoke_test user with known credentials
        users_table = table('users',
            column('id', sa.String()),
            column('username', sa.String()),
            column('email', sa.String()),
            column('hashed_password', sa.String()),
            column('is_active', sa.Boolean()),
            column('is_smoke_test_user', sa.Boolean())
        )

        # Hash the password
        password = "smoke_test_password_change_in_prod"
        hashed_password = pwd_context.hash(password)

        op.execute(
            users_table.insert().values(
                id='smoke-test-user-uuid',
                username='smoke_test',
                email='smoke-test@example.com',
                hashed_password=hashed_password,
                is_active=True,
                is_smoke_test_user=True
            )
        )

    def downgrade() -> None:
        op.execute(
            sa.text("DELETE FROM users WHERE username = 'smoke_test'")
        )
    ```

    Update deploy.yml smoke tests to use authentication:

    ```yaml
    # Lines 157-169 in deploy.yml
    - name: Run smoke tests
      env:
        SMOKE_TEST_USERNAME: ${{ secrets.SMOKE_TEST_USERNAME }}
        SMOKE_TEST_PASSWORD: ${{ secrets.SMOKE_TEST_PASSWORD }}
      run: |
        # Login and get auth token
        TOKEN=$(curl -s -X POST https://staging.atom.example.com/api/auth/login \
          -H "Content-Type: application/json" \
          -d "{\"username\":\"${SMOKE_TEST_USERNAME}\",\"password\":\"${SMOKE_TEST_PASSWORD}\"}" \
          | jq -r '.access_token')

        if [ "$TOKEN" == "null" ] || [ -z "$TOKEN" ]; then
          echo "❌ Smoke test authentication failed"
          exit 1
        fi

        # Test health endpoints (no auth required)
        curl -f https://staging.atom.example.com/health/live || exit 1
        curl -f https://staging.atom.example.com/health/ready || exit 1

        # Test authenticated agent execution endpoint
        curl -f -X POST https://staging.atom.example.com/api/agents/execute \
          -H "Authorization: Bearer $TOKEN" \
          -H "Content-Type: application/json" \
          -d '{"agent_id": "test", "query": "hello"}' || exit 1

        # Test authenticated canvas presentation endpoint
        curl -f -X POST https://staging.atom.example.com/api/canvas/present \
          -H "Authorization: Bearer $TOKEN" \
          -H "Content-Type: application/json" \
          -d '{"canvas_type": "generic", "content": "smoke test"}' || exit 1

        echo "✅ All smoke tests passed"
    ```
  </action>
  <verify>
    Run: alembic upgrade head (migration applies successfully)
    Run: psql -c "SELECT username, is_smoke_test_user FROM users WHERE username='smoke_test'" (row exists)
    Run: curl -X POST /api/auth/login -d '{"username":"smoke_test","password":"smoke_test_password_change_in_prod"}' (returns JWT token)
  </verify>
  <done>
    Smoke test user exists in database with known credentials. Smoke tests authenticate properly and fail with 401 when credentials invalid. No more false-positive smoke test passes.
  </done>
</task>

<task type="auto">
  <name>Task 5: Create pytest output parser script for pass rate calculation</name>
  <files>backend/tests/scripts/parse_pytest_output.py</files>
  <action>
    Create Python script to parse pytest JSON output and calculate pass rate:

    ```python
    #!/usr/bin/env python3
    """
    Parse pytest JSON output and calculate pass rate.

    Usage: python parse_pytest_output.py pytest_report.json

    Output: Pass rate percentage to stdout (e.g., "98.5")
    """

    import json
    import sys

    def calculate_pass_rate(json_file):
        """Calculate pass rate from pytest JSON report."""
        with open(json_file, 'r') as f:
            data = json.load(f)

        summary = data.get('summary', {})
        total = summary.get('total', 0)
        passed = summary.get('passed', 0)
        failed = summary.get('failed', 0)
        errors = summary.get('error', 0)
        skipped = summary.get('skipped', 0)

        if total == 0:
            print("100.0")
            return 100.0

        # Pass rate = passed / (passed + failed + errors)
        # Skipped tests are excluded from pass rate calculation
        attempted = passed + failed + errors
        pass_rate = (passed / attempted * 100) if attempted > 0 else 100.0

        # Output to stdout for CI capture
        print(f"{pass_rate:.1f}")

        # Optional: detailed logging
        print(f"Total: {total}, Passed: {passed}, Failed: {failed}, Errors: {errors}, Skipped: {skipped}", file=sys.stderr)

        return pass_rate

    if __name__ == '__main__':
        if len(sys.argv) != 2:
            print("Usage: parse_pytest_output.py <pytest_json_file>", file=sys.stderr)
            sys.exit(1)

        json_file = sys.argv[1]
        try:
            pass_rate = calculate_pass_rate(json_file)
            # Exit with error if pass rate below 98%
            if pass_rate < 98.0:
                sys.exit(1)
        except Exception as e:
            print(f"Error parsing pytest output: {e}", file=sys.stderr)
            sys.exit(1)
    ```

    Add to requirements-testing.txt:
    ```
    pytest-json-report>=0.6.0
    ```

    Update .github/workflows/ci.yml to use parser:
    ```yaml
    - name: Check test pass rate (98%+ required)
      working-directory: ./backend
      if: always()
      run: |
        # Calculate pass rate from pytest JSON report
        python tests/scripts/parse_pytest_output.py pytest_report.json
        PASS_RATE=$?
        if [ $PASS_RATE -ne 0 ]; then
          echo "❌ Pass rate below 98% threshold"
          exit 1
        fi
        echo "✅ Pass rate meets 98% threshold"
    ```
  </action>
  <verify>
    Run: pytest tests/ --json-report --json-report-file=test.json && python tests/scripts/parse_pytest_output.py test.json (outputs pass rate)
    Run: echo $? (should be 0 if pass rate >=98%, 1 otherwise)
    Check: grep "pytest-json-report" backend/requirements-testing.txt returns match
  </verify>
  <done>
    Pass rate calculation script correctly parses pytest JSON output. Quality gate enforces 98% minimum pass rate. CI fails if pass rate falls below threshold.
  </done>
</task>

</tasks>

<verification>
Overall phase verification steps:

1. **Test Coverage Verification**:
   ```bash
   # All previously ignored tests now run
   pytest tests/integration/episodes/test_lancedb_integration.py -v
   pytest tests/integration/governance/test_graduation_exams.py -v
   pytest tests/test_analytics_dashboard.py -v

   # Verify no --ignore flags in CI workflow
   grep -c "\-\-ignore=" .github/workflows/ci.yml  # Should return 0
   ```

2. **Quality Gate Verification**:
   ```bash
   # Run tests with random order (test independence check)
   pytest tests/ --random-order -v --maxfail=5

   # Verify pass rate calculation
   pytest tests/ --json-report --json-report-file=report.json
   python backend/tests/scripts/parse_pytest_output.py report.json
   ```

3. **Smoke Test Authentication Verification**:
   ```bash
   # Verify smoke test user exists
   psql -c "SELECT username, is_smoke_test_user FROM users WHERE username='smoke_test'"

   # Test smoke test login
   curl -X POST /api/auth/login \
     -d '{"username":"smoke_test","password":"smoke_test_password_change_in_prod"}'
   ```

4. **LanceDB Integration Workflow Verification**:
   ```bash
   # Verify workflow exists
   gh workflow view lancedb-integration

   # Manually trigger workflow
   gh workflow run lancedb-integration.yml
   ```

**Success Criteria**:
- [ ] Zero --ignore flags in .github/workflows/ci.yml
- [ ] All 14+ previously ignored test files pass in CI
- [ ] Pass rate quality gate enforces 98% threshold
- [ ] Test independence validated (random order passes)
- [ ] Smoke tests include authentication and fail with invalid credentials
- [ ] LanceDB integration tests run in separate workflow with service container
- [ ] pytest-random-order and pytest-rerunfailures in requirements-testing.txt
</verification>

<success_criteria>
**Test Suite Stabilization Complete When:**

1. **Test File Execution**: 100% of test files run in CI (zero --ignore flags)
2. **Pass Rate Quality Gate**: 98% minimum threshold enforced via pytest-json-report
3. **Test Independence**: Random order execution passes consistently (no ordering dependencies)
4. **External Service Mocking**: All external dependencies properly mocked (LanceDB, Knowledge Graph, Prometheus, Grafana)
5. **Smoke Test Authentication**: Smoke tests use proper auth tokens and fail on 401 errors
6. **LanceDB Integration**: Dedicated workflow with service container for heavy integration tests

**Measurable Outcomes**:
- Test execution time: <5 minutes for main CI workflow (parallel execution)
- Pass rate: >=98% across 3 consecutive CI runs
- Flaky test rate: <2% (via pytest-rerunfailures automatic retry)
- Ignored test files: 0 (down from 14+)
</success_criteria>

<output>
After completion, create `.planning/phases/67-ci-cd-pipeline-fixes/67-01-SUMMARY.md` with:
- List of all 14+ previously ignored test files now running in CI
- Pass rate metrics from 3 consecutive CI runs
- Test independence validation results (random order execution)
- Smoke test authentication verification results
- Link to LanceDB integration workflow runs
</output>
