---
phase: 02-core-property-tests
plan: 08
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/tests/property_tests/llm/test_llm_invariants.py
  - backend/tests/property_tests/llm/test_streaming_invariants.py
  - backend/tests/property_tests/llm/test_token_counting_invariants.py
autonomous: true

must_haves:
  truths:
    - Provider fallback routes from OpenAI → Anthropic → DeepSeek → Gemini on failure
    - Token counting accuracy is within ±5% of actual usage
    - Streaming responses complete without truncation (all tokens received)
    - Cost calculation matches provider pricing (per-1k tokens)
    - Each invariant test documents bugs found with VALIDATED_BUG sections
  artifacts:
    - path: backend/tests/property_tests/llm/test_llm_invariants.py
      contains: VALIDATED_BUG
      min_lines: 350
    - path: backend/tests/property_tests/llm/test_streaming_invariants.py
      contains: VALIDATED_BUG
      min_lines: 200
  key_links:
    - from: backend/tests/property_tests/llm/
      to: backend/core/llm/byok_handler.py
      pattern: test_provider_fallback
---

<objective>
Implement LLM property tests with bug-finding evidence documentation for multi-provider routing, streaming, token management, and cost calculation.

Purpose: Address AR-04 requirement for "LLM Integration Coverage - Test multi-provider routing, streaming, token management, error handling" and roadmap deliverables for provider fallback, token counting accuracy, cost calculation, and streaming completion.

Output: LLM property tests with VALIDATED_BUG docstring sections for provider fallback, streaming completion, token counting (±5% accuracy), and cost calculation invariants.
</objective>

<execution_context>
@/Users/rushiparikh/.claude/get-shit-done/workflows/execute-plan.md
@/Users/rushiparikh/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md

@backend/tests/property_tests/README.md
@backend/core/llm/byok_handler.py
@backend/core/llm/streaming_handler.py
</context>

<tasks>

<task type="auto">
  <name>Add bug-finding evidence to provider fallback invariants</name>
  <files>backend/tests/property_tests/llm/test_llm_invariants.py</files>
  <action>
    Create provider fallback tests with bug-finding evidence:

    1. Add VALIDATED_BUG sections for provider routing bugs
    2. Document failover chain (OpenAI → Anthropic → DeepSeek → Gemini)
    3. Add tests for timeout handling and retry logic
    4. Add tests for provider unavailability

    Example pattern:
    ```python
    @given(
        primary_provider=st.sampled_from(['openai', 'anthropic', 'deepseek', 'gemini']),
        failure_mode=st.sampled_from(['timeout', 'rate_limit', 'error_500', 'invalid_key']),
        retry_count=st.integers(min_value=0, max_value=3)
    )
    @example(primary_provider='openai', failure_mode='rate_limit', retry_count=1)
    @settings(max_examples=100)
    def test_provider_fallback_chain(self, primary_provider, failure_mode, retry_count):
        """
        INVARIANT: Provider fallback routes to next available provider on failure.
        Fallback chain: OpenAI → Anthropic → DeepSeek → Gemini → Error.

        VALIDATED_BUG: OpenAI rate_limit did not fallback to Anthropic, returned error instead.
        Root cause was exception type mismatch (RateLimitError vs expected APIError).
        Fixed in commit xyz789 by adding broader exception catching in fallback loop.

        Fallback behavior: openai fails → try anthropic → try deepseek → try gemini → raise NoProviderAvailable.
        """
    ```

    Use max_examples=100 for provider fallback tests (availability is critical).
  </action>
  <verify>
    grep -c "VALIDATED_BUG" backend/tests/property_tests/llm/test_llm_invariants.py
  </verify>
  <done>
    At least 4 provider tests have VALIDATED_BUG sections
    Fallback chain documented with all 4 providers
    Timeout and retry logic tested
  </done>
</task>

<task type="auto">
  <name>Add bug-finding evidence to streaming completion invariants</name>
  <files>backend/tests/property_tests/llm/test_streaming_invariants.py</files>
  <action>
    Create streaming completion tests with bug-finding evidence:

    1. Add VALIDATED_BUG sections for truncation bugs
    2. Document chunk reassembly logic
    3. Add tests for connection mid-stream disconnect
    4. Add tests for empty chunk handling

    Example pattern:
    ```python
    @given(
        total_tokens=st.integers(min_value=10, max_value=4000),
        chunk_sizes=st.lists(st.integers(min_value=1, max_value=100), min_size=5, max_size=50)
    )
    @example(total_tokens=100, chunk_sizes=[20, 20, 20, 20, 20])
    @settings(max_examples=100)
    def test_streaming_completion(self, total_tokens, chunk_sizes):
        """
        INVARIANT: Streaming responses must complete with all tokens received.
        Chunk reassembly must produce complete response without truncation.

        VALIDATED_BUG: Last chunk dropped when chunk_size < expected, causing truncation.
        Root cause was treating small final chunk as error instead of completion signal.
        Fixed in commit abc456 by checking done flag instead of chunk size.

        Completion: sum(received_chunks) == total_tokens, no missing tokens.
        """
    ```

    Use max_examples=100 for streaming tests (completion is critical for UX).
  </action>
  <verify>
    grep -c "VALIDATED_BUG" backend/tests/property_tests/llm/test_streaming_invariants.py
  </verify>
  <done>
    At least 3 streaming tests have VALIDATED_BUG sections
    Truncation and disconnect bugs documented
    Chunk reassembly logic validated
  </done>
</task>

<task type="auto">
  <name>Add bug-finding evidence to token counting accuracy invariants</name>
  <files>backend/tests/property_tests/llm/test_token_counting_invariants.py</files>
  <action>
    Create token counting tests with bug-finding evidence:

    1. Add VALIDATED_BUG sections for token count deviation bugs
    2. Document ±5% accuracy requirement
    3. Add tests for different tokenizers (tiktoken, claude, gemini)
    4. Add tests for special token handling

    Example pattern:
    ```python
    @given(
        text=st.text(min_size=10, max_size=5000, alphabet='abc def ghi jkl mno pqr stu vwx yz'),
        tokenizer=st.sampled_from(['tiktoken', 'claude', 'gemini'])
    )
    @example(text="Hello world", tokenizer='tiktoken')
    @settings(max_examples=100)
    def test_token_counting_accuracy(self, text, tokenizer):
        """
        INVARIANT: Token counting must be within ±5% of actual usage.
        Estimated tokens should match provider billing (within tolerance).

        VALIDATED_BUG: tiktoken estimate was 120 but provider billed 127 (6% deviation).
        Root cause was using wrong encoding model (cl100k_base instead of o200k_base).
        Fixed in commit def123 by selecting encoding per provider.

        Accuracy: |estimated - actual| / actual <= 0.05 (5% tolerance).
        """
    ```

    Use max_examples=100 for token counting tests (billing accuracy is critical).
  </action>
  <verify>
    grep -c "VALIDATED_BUG" backend/tests/property_tests/llm/test_token_counting_invariants.py
  </verify>
  <done>
    At least 3 token counting tests have VALIDATED_BUG sections
    ±5% accuracy requirement validated
    Multiple tokenizers tested
  </done>
</task>

<task type="auto">
  <name>Add bug-finding evidence to cost calculation invariants</name>
  <files>backend/tests/property_tests/llm/test_token_counting_invariants.py</files>
  <action>
    Create cost calculation tests with bug-finding evidence:

    1. Add VALIDATED_BUG sections for billing calculation bugs
    2. Document per-1k token pricing models
    3. Add tests for input/output price differences
    4. Add tests for cost summation across requests

    Example pattern:
    ```python
    @given(
        input_tokens=st.integers(min_value=1, max_value=10000),
        output_tokens=st.integers(min_value=1, max_value=5000),
        provider=st.sampled_from(['openai', 'anthropic', 'deepseek', 'gemini'])
    )
    @example(input_tokens=1000, output_tokens=500, provider='openai')
    @settings(max_examples=100)
    def test_cost_calculation(self, input_tokens, output_tokens, provider):
        """
        INVARIANT: Cost calculation matches provider pricing (per-1k tokens).
        Total cost = (input_tokens / 1000 * input_price) + (output_tokens / 1000 * output_price).

        VALIDATED_BUG: Anthropic cost used input_price for output_tokens (wrong multiplier).
        Root cause was price dict lookup missing 'output' key, defaulting to 'input'.
        Fixed in commit ghi789 by adding explicit input/output price lookup.

        Cost accuracy: calculated_cost == expected_cost within rounding.
        """
    ```

    Use max_examples=100 for cost calculation tests (billing accuracy).
  </action>
  <verify>
    grep -c "cost\|pricing" backend/tests/property_tests/llm/test_token_counting_invariants.py
  </verify>
  <done>
    At least 3 cost calculation tests have VALIDATED_BUG sections
    Per-1k token pricing validated for all providers
    Input/output price differences tested
  </done>
</task>

<task type="auto">
  <name>Document LLM invariants in INVARIANTS.md</name>
  <files>backend/tests/property_tests/INVARIANTS.md</files>
  <action>
    Add LLM section to INVARIANTS.md:

    ```markdown
    ## LLM Integration Domain

    ### Provider Fallback Chain
    **Invariant**: Provider failure routes to next available (OpenAI → Anthropic → DeepSeek → Gemini).
    **Test**: test_provider_fallback_chain (test_llm_invariants.py)
    **Critical**: Yes (LLM availability is critical for agent functionality)
    **Bug Found**: RateLimitError did not trigger fallback to Anthropic
    **max_examples**: 100

    ### Streaming Completion
    **Invariant**: Streaming responses complete without truncation (all tokens received).
    **Test**: test_streaming_completion (test_streaming_invariants.py)
    **Critical**: Yes (truncation causes data loss)
    **Bug Found**: Small final chunk dropped, causing response truncation
    **max_examples**: 100

    ### Token Counting Accuracy
    **Invariant**: Token estimates within ±5% of actual provider usage.
    **Test**: test_token_counting_accuracy (test_token_counting_invariants.py)
    **Critical**: Yes (billing accuracy)
    **Bug Found**: 6% deviation from wrong encoding model
    **max_examples**: 100

    ### Cost Calculation
    **Invariant**: Cost matches provider pricing (per-1k tokens, input vs output).
    **Test**: test_cost_calculation
    **Critical**: Yes (billing accuracy)
    **Bug Found**: Output tokens billed at input price
    **max_examples**: 100
    ```

    Create new INVARIANTS.md if not exists, or append to existing.
    Include all LLM invariants with test locations.
  </action>
  <verify>
    grep -c "LLM Integration Domain" backend/tests/property_tests/INVARIANTS.md
  </verify>
  <done>
    LLM Integration section added to INVARIANTS.md
    At least 5 LLM invariants documented
    Provider fallback, streaming, token counting, cost calculation included
  </done>
</task>

</tasks>

<verification>
After completion:
1. Run LLM property tests: pytest backend/tests/property_tests/llm/ -v
2. Verify all tests pass with enhanced documentation
3. Check INVARIANTS.md has LLM section
4. Verify grep for VALIDATED_BUG returns at least 10 matches across LLM tests
</verification>

<success_criteria>
1. LLM property tests document bug-finding evidence (AR-04)
2. Provider fallback chain validated (OpenAI → Anthropic → DeepSeek → Gemini)
3. Token counting accuracy within ±5% validated
4. Cost calculation matches provider pricing
5. Streaming completion validated
6. INVARIANTS.md includes LLM section
</success_criteria>

<output>
After completion, create `.planning/phases/02-core-property-tests/02-08-SUMMARY.md`
</output>
