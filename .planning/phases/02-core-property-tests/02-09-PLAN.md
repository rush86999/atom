---
phase: 02-core-property-tests
plan: 09
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/tests/security/test_fuzzing_governance_inputs.py
  - backend/tests/security/test_owasp_injection.py
  - backend/tests/security/test_owasp_auth.py
  - backend/tests/security/test_dependency_scanning.py
autonomous: true

must_haves:
  truths:
    - Fuzz testing of governance inputs finds crashes and hangs (no uncaught exceptions)
    - SQL injection attempts are blocked (no SQL errors in output)
    - XSS payload attempts are sanitized (scripts don't execute)
    - Broken authentication attempts are rejected (weak passwords, session hijacking)
    - Dependency scanning identifies known vulnerabilities (pip-audit, bandit, safety)
  artifacts:
    - path: backend/tests/security/test_fuzzing_governance_inputs.py
      contains: "test_"
      min_lines: 200
    - path: backend/tests/security/test_owasp_injection.py
      contains: "test_"
      min_lines: 300
    - path: backend/tests/security/SECURITY_SCAN_RESULTS.md
      contains: "pip-audit"
      min_lines: 100
  key_links:
    - from: backend/tests/security/
      to: backend/core/
      pattern: test_fuzzing
---

<objective>
Implement comprehensive security test suite with fuzz testing, OWASP Top 10 coverage, and dependency scanning.

Purpose: Address AR-14 requirement for "Security Test Suite - Fuzz testing, OWASP Top 10, dependency scanning" and roadmap deliverables for fuzz testing governance inputs, OWASP Top 10 coverage (injection, broken auth, XSS, CSRF, security misconfiguration), and dependency scanning (pip-audit, bandit, safety).

Output: Security tests with fuzz testing for governance inputs, OWASP Top 10 coverage (injection, broken auth, XSS, CSRF, security misconfiguration), and dependency scanning results.
</objective>

<execution_context>
@/Users/rushiparikh/.claude/get-shit-done/workflows/execute-plan.md
@/Users/rushiparikh/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md

@backend/tests/security/README.md
@backend/core/agent_governance_service.py
@backend/core/models.py
</context>

<tasks>

<task type="auto">
  <name>Implement fuzz testing for governance inputs</name>
  <files>backend/tests/security/test_fuzzing_governance_inputs.py</files>
  <action>
    Create fuzz tests for governance input validation:

    1. Use hypothesis.fuzz() or atheris for fuzz testing
    2. Test governance decision inputs with random byte sequences
    3. Test maturity level inputs with invalid values
    4. Test confidence score inputs with out-of-range values
    5. Document any crashes or hangs found

    Example pattern:
    ```python
    from hypothesis import given, strategies as st
    import pytest

    @given(
        agent_id=st.binary(min_size=0, max_size=10000),
        maturity_level=st.binary(min_size=0, max_size=100),
        confidence=st.floats(min_value=-1000, max_value=1000)
    )
    def test_governance_input_fuzzing(self, agent_id, maturity_level, confidence):
        """
        FUZZ TEST: Governance service handles arbitrary inputs without crashing.

        Tests binary input for agent_id, invalid maturity levels, and extreme confidence values.
        Service should validate and reject invalid inputs, not crash.
        """
        # Decode binary to str for API simulation
        try:
            agent_id_str = agent_id.decode('utf-8', errors='ignore')
        except:
            agent_id_str = str(agent_id)

        # This should not crash - should return error or use defaults
        try:
            result = governance_service.check_permission(
                agent_id=agent_id_str,
                maturity_level=maturity_level if isinstance(maturity_level, str) else 'AUTONOMOUS',
                confidence=max(0.0, min(1.0, confidence))
            )
            # If we get here, input was handled (either validated or defaulted)
            assert result is not None or isinstance(result, bool)
        except Exception as e:
            # Only validation errors allowed, no crashes
            assert isinstance(e, (ValueError, TypeError, ValidationError))
    ```

    Use max_examples=200 for fuzz tests to find edge cases.
  </action>
  <verify>
    pytest backend/tests/security/test_fuzzing_governance_inputs.py -v --hypothesis-seed=0
  </verify>
  <done>
    Fuzz tests run without unhandled crashes
    At least 3 fuzz test functions created
    Input validation coverage for governance inputs
  </done>
</task>

<task type="auto">
  <name>Implement OWASP injection tests (SQL, XSS, path traversal)</name>
  <files>backend/tests/security/test_owasp_injection.py</files>
  <action>
    Create OWASP injection vulnerability tests:

    1. SQL injection payloads (admin'--, ' OR '1'='1, UNION SELECT)
    2. XSS payloads (<script>, javascript:, onerror=)
    3. Path traversal payloads (../../etc/passwd, ..%2F..%2F)
    4. Command injection payloads (; ls, | cat, `whoami`)

    Example pattern:
    ```python
    @pytest.mark.parametrize("payload", [
        "admin'--",
        "' OR '1'='1",
        "'; DROP TABLE agents; --",
        "1' UNION SELECT * FROM users--",
        "<script>alert('xss')</script>",
        "javascript:alert('xss')",
        "<img onerror='alert(1)' src=x>",
        "../../../etc/passwd",
        "..%2F..%2F..%2Fetc%2Fpasswd",
        "; ls -la",
        "| cat /etc/passwd",
        "`whoami`"
    ])
    def test_injection_payloads_blocked(self, payload):
        """
        OWASP A03:2021 - Injection
        Test that common injection payloads are blocked or sanitized.
        """
        # Test in API context
        response = client.post("/api/agents/query", json={"query": payload})
        # Should not return raw payload in error (information leak)
        # Should not cause SQL errors (database exposure)
        # Should not execute scripts (XSS)

        # Check for safe handling
        assert response.status_code in [400, 422]
        assert payload not in str(response.content) or "sanitized" in str(response.content)
    ```

    Document all tested payloads and results.
  </action>
  <verify>
    pytest backend/tests/security/test_owasp_injection.py -v
  </verify>
  <done>
    At least 12 injection payloads tested
    SQL injection payloads blocked
    XSS payloads sanitized
    Path traversal blocked
  </done>
</task>

<task type="auto">
  <name>Implement OWASP broken authentication tests</name>
  <files>backend/tests/security/test_owasp_auth.py</files>
  <action>
    Create OWASP broken authentication tests:

    1. Weak password acceptance (password, 123456, qwerty)
    2. Session fixation vulnerabilities
    3. Session timeout validation
    4. Password hashing (bcrypt, not plaintext)
    5. JWT token validation

    Example pattern:
    ```python
    @pytest.mark.parametrize("weak_password", [
        "password",
        "123456",
        "qwerty",
        "admin",
        "letmein",
        "welcome"
    ])
    def test_weak_passwords_rejected(self, weak_password):
        """
        OWASP A07:2021 - Identification and Authentication Failures
        Test that weak passwords are rejected during registration.
        """
        response = client.post("/api/auth/register", json={
            "email": "test@example.com",
            "password": weak_password
        })

        # Should reject weak passwords
        assert response.status_code == 400
        assert "weak" in response.json()["message"].lower() or "required" in response.json()["message"].lower()

    def test_session_timeout_enforced(self):
        """
        OWASP A07:2021 - Session Management
        Test that sessions expire after timeout.
        """
        # Create session
        login_response = client.post("/api/auth/login", json={
            "email": "test@example.com",
            "password": "SecurePass123!"
        })

        # Mock time passing beyond session timeout
        with freeze_time(time.time() + 3600 * 25):  # 25 hours later
            response = client.get("/api/agents", cookies={
                "session_token": login_response.cookies["session_token"]
            })
            # Should reject expired session
            assert response.status_code == 401
    ```

    Document all authentication security tests.
  </action>
  <verify>
    pytest backend/tests/security/test_owasp_auth.py -v
  </verify>
  <done>
    At least 6 weak passwords rejected
    Session timeout enforced
    JWT validation tested
    Password hashing verified (bcrypt)
  </done>
</task>

<task type="auto">
  <name>Run dependency scanning (pip-audit, bandit, safety)</name>
  <files>backend/tests/security/SECURITY_SCAN_RESULTS.md</files>
  <action>
    Run security scanning tools and document results:

    1. Run pip-audit for known vulnerabilities
    2. Run bandit for Python security issues
    3. Run safety check for dependency vulnerabilities
    4. Document findings in SECURITY_SCAN_RESULTS.md

    Commands to run:
    ```bash
    cd /Users/rushiparikh/projects/atom/backend
    pip-audit --format json > ../tests/security/pip-audit-results.json 2>&1 || true
    bandit -r . -f json -o ../tests/security/bandit-results.json 2>&1 || true
    safety check --json > ../tests/security/safety-results.json 2>&1 || true
    ```

    Create SECURITY_SCAN_RESULTS.md:
    ```markdown
    # Security Scan Results

    Run Date: 2026-02-17

    ## pip-audit (Known Vulnerabilities)

    ```bash
    pip-audit --format json
    ```

    ### Results
    [Paste results or summarize]

    ### Actions Required
    - [ ] Update vulnerable packages

    ## bandit (Python Security Issues)

    ```bash
    bandit -r . -f json
    ```

    ### Results
    [Paste results or summarize]

    ### Actions Required
    - [ ] Fix high-severity issues
    - [ ] Review medium-severity issues

    ## safety (Dependency Vulnerabilities)

    ```bash
    safety check --json
    ```

    ### Results
    [Paste results or summarize]

    ### Actions Required
    - [ ] Update flagged dependencies
    ```

    If tools not installed, add to requirements-dev.txt:
    ```
    pip-audit
    bandit
    safety
    ```
  </action>
  <verify>
    ls -la backend/tests/security/SECURITY_SCAN_RESULTS.md
    grep -c "##" backend/tests/security/SECURITY_SCAN_RESULTS.md
  </verify>
  <done>
    SECURITY_SCAN_RESULTS.md created with all 3 scan results
    Vulnerabilities documented with severity levels
    Actions required clearly listed
  </done>
</task>

<task type="auto">
  <name>Implement OWASP security misconfiguration tests</name>
  <files>backend/tests/security/test_owasp_misconfig.py</files>
  <action>
    Create OWASP security misconfiguration tests:

    1. Default credentials not exposed
    2. Debug mode off in production
    3. Error messages don't leak sensitive info
    4. Security headers present (CSP, X-Frame-Options, etc.)
    5. CORS configured correctly

    Example pattern:
    ```python
    def test_default_credentials_not_exposed(self):
        """
        OWASP A05:2021 - Security Misconfiguration
        Test that default credentials are not usable.
        """
        # Try common default credentials
        defaults = [
            ("admin", "admin"),
            ("admin", "password"),
            ("root", "root"),
            ("user", "password")
        ]

        for username, password in defaults:
            response = client.post("/api/auth/login", json={
                "email": f"{username}@example.com",
                "password": password
            })
            # Should not succeed with default credentials
            assert response.status_code == 401

    def test_error_messages_dont_leak_info(self):
        """
        OWASP A05:2021 - Security Misconfiguration
        Test that error messages don't expose internals.
        """
        response = client.get("/api/agents/nonexistent-id")
        # Should not expose stack traces, SQL errors, or file paths
        assert "Traceback" not in response.text
        assert "SQL" not in response.text
        assert "/app/" not in response.text
    ```

    Document all misconfiguration tests.
  </action>
  <verify>
    pytest backend/tests/security/test_owasp_misconfig.py -v
  </verify>
  <done>
    Default credentials tested and rejected
    Debug mode verified off
    Error messages checked for info leaks
    Security headers validated
  </done>
</task>

<task type="auto">
  <name>Document security test coverage in INVARIANTS.md</name>
  <files>backend/tests/property_tests/INVARIANTS.md</files>
  <action>
    Add security section to INVARIANTS.md:

    ```markdown
    ## Security Domain

    ### Injection Prevention
    **Invariant**: SQL, XSS, path traversal, command injection payloads are blocked.
    **Test**: test_injection_payloads_blocked (test_owasp_injection.py)
    **Critical**: Yes (OWASP A03:2021 - Injection)
    **Coverage**: 12+ injection payloads tested
    **max_examples**: N/A (parametrized tests)

    ### Broken Authentication Prevention
    **Invariant**: Weak passwords rejected, sessions timeout, tokens validated.
    **Test**: test_weak_passwords_rejected (test_owasp_auth.py)
    **Critical**: Yes (OWASP A07:2021)
    **Coverage**: 6+ weak passwords, session timeout, JWT validation
    **max_examples**: N/A (parametrized tests)

    ### Security Misconfiguration Prevention
    **Invariant**: Default credentials rejected, debug off, errors don't leak info.
    **Test**: test_default_credentials_not_exposed (test_owasp_misconfig.py)
    **Critical**: Yes (OWASP A05:2021)
    **Coverage**: Defaults, debug mode, error messages, security headers
    **max_examples**: N/A (parametrized tests)

    ### Dependency Security
    **Invariant**: No known vulnerabilities in dependencies (pip-audit, bandit, safety).
    **Test**: SECURITY_SCAN_RESULTS.md (automated scans)
    **Critical**: Yes (supply chain security)
    **Coverage**: pip-audit, bandit, safety scans
    **Frequency**: Run weekly or before releases
    ```

    Append to existing INVARIANTS.md.
  </action>
  <verify>
    grep -c "Security Domain" backend/tests/property_tests/INVARIANTS.md
  </verify>
  <done>
    Security Domain section added to INVARIANTS.md
    OWASP coverage documented
    Dependency scanning results referenced
  </done>
</task>

</tasks>

<verification>
After completion:
1. Run security tests: pytest backend/tests/security/ -v
2. Verify SECURITY_SCAN_RESULTS.md exists with scan results
3. Check INVARIANTS.md has security section
4. Verify all OWASP Top 10 categories covered
</verification>

<success_criteria>
1. Fuzz testing for governance inputs implemented (AR-14)
2. OWASP Top 10 coverage achieved (injection, broken auth, XSS, CSRF, misconfiguration)
3. Dependency scanning results documented (pip-audit, bandit, safety)
4. Security test suite passes with documented findings
5. INVARIANTS.md includes security section
</success_criteria>

<output>
After completion, create `.planning/phases/02-core-property-tests/02-09-SUMMARY.md`
</output>
