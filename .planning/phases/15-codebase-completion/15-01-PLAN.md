---
phase: 15-codebase-completion
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/conftest.py
  - tests/integration/conftest.py
  - tests/test_skill_integration.py
  - tests/test_skill_episodic_integration.py
  - backend/core/autonomous_supervisor_service.py
  - backend/core/skill_registry_service.py
autonomous: true

must_haves:
  truths:
    - "All 90 skill tests pass (currently 82/90, 8 failures due to fixture/async issues)"
    - "Test fixtures use consistent naming (db_session vs db inconsistency resolved)"
    - "Async tests use @pytest.mark.asyncio decorator properly"
    - "Production TODO comments evaluated (critical ones addressed, others documented)"
  artifacts:
    - path: "tests/conftest.py"
      provides: "Centralized database fixture with db_session naming"
      contains: "@pytest.fixture(scope=\"function\")\\ndef db_session"
    - path: "backend/core/autonomous_supervisor_service.py"
      provides: "Service with TODO comments evaluated"
      contains: "# TODO (evaluated)"
  key_links:
    - from: "tests/test_skill_integration.py"
      to: "tests/conftest.py"
      via: "db_session fixture reference"
      pattern: "def test_.*\\(db_session\\)"
    - from: "tests/test_skill_episodic_integration.py"
      to: "tests/conftest.py"
      via: "db_session fixture reference"
      pattern: "@pytest.mark.asyncio\\s+async def test_.*\\(db_session\\)"
---

<objective>
Fix test infrastructure issues and evaluate production TODO comments to achieve 95%+ test pass rate.

Purpose: Failing skill tests (82/90 passing) indicate fixture inconsistency and async test pattern issues that must be resolved for production readiness. TODO comments in production code need evaluation.

Output: Standardized test fixtures, fixed async tests, evaluated TODO comments with documentation or implementation.
</objective>

<execution_context>
@/Users/rushiparikh/.claude/get-shit-done/workflows/execute-plan.md
@/Users/rushiparikh/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/15-codebase-completion/15-RESEARCH.md
@.planning/STATE.md

# Key patterns from Phase 01-test-infrastructure
- Use function-scoped fixtures for test isolation
- UUID-based unique identifiers prevent test pollution
- Factory pattern for test data creation

# Key patterns from Phase 03-integration-security-tests
- Transaction rollback pattern for database test isolation
- FastAPI TestClient with dependency overrides for API testing
- pytest-asyncio with @pytest.mark.asyncio for async tests
</context>

<tasks>

<task type="auto">
  <name>Task 1: Standardize test fixtures and fix async tests</name>
  <files>tests/conftest.py, tests/integration/conftest.py, tests/test_skill_integration.py, tests/test_skill_episodic_integration.py</files>
  <action>
    1. Update tests/conftest.py to define standardized `db_session` fixture (function-scoped, tempfile-based SQLite)
    2. Remove duplicate/conflicting `db` fixtures from subdirectory conftest.py files
    3. Fix async tests in test_skill_episodic_integration.py:
       - Add @pytest.mark.asyncio decorator to all async test functions
       - Change `db` fixture references to `db_session`
       - Use AsyncMock for async method mocking (not Mock)
    4. Fix async tests in test_skill_integration.py (same pattern)
    5. Run skill tests: `pytest tests/test_skill_integration.py tests/test_skill_episodic_integration.py -v`

    Per 15-RESEARCH.md:
    - Use pytest-asyncio pattern: @pytest.mark.asyncio + async def test_xxx(db_session)
    - Standardize fixture name to `db_session` (not `db`)
    - Use tempfile.mkstemp for test database isolation
    - Cleanup test database files in fixture teardown
  </action>
  <verify>
    Command: `pytest tests/test_skill_integration.py tests/test_skill_episodic_integration.py -v --tb=short`
    Expected: All 90 skill tests pass (currently 82/90, fix 8 failing tests)
  </verify>
  <done>
    90/90 skill tests pass with zero fixture-related errors
  </done>
</task>

<task type="auto">
  <name>Task 2: Evaluate and address production TODO comments</name>
  <files>backend/core/autonomous_supervisor_service.py, backend/core/skill_registry_service.py, backend/docs/FUTURE_WORK.md</files>
  <action>
    1. Search for TODO/FIXME/XXX comments in production code (core/, api/, tools/ directories)
    2. Categorize each TODO:
       - Critical: Implement now (affects production functionality)
       - Future: Document in backend/docs/FUTURE_WORK.md
       - Delete: No longer relevant (remove comment)
    3. For critical TODOs in autonomous_supervisor_service.py and skill_registry_service.py:
       - Implement if simple (<30 min)
       - Create GitHub issue with deadline if complex
       - Add "# TODO (evaluated: [action_taken])" comment
    4. Create or update backend/docs/FUTURE_WORK.md with future TODOs
    5. Delete obsolete TODO comments

    Per 15-RESEARCH.md Pitfall 4:
    - Audit all TODO/FIXME/XXX comments in production code
    - Categorize: Critical (implement now), Future (document), Delete (obsolete)
    - Create GitHub issues for critical TODOs with deadline
  </action>
  <verify>
    Command: `grep -r "TODO\|FIXME\|XXX" backend/core/ backend/api/ backend/tools/ --include="*.py" | grep -v "test" | grep -v "evaluated" | wc -l`
    Expected: Zero unevaluated TODO comments in production code (all marked as evaluated or removed)
  </verify>
  <done>
    All production TODO comments evaluated: critical ones implemented or tracked, future ones documented, obsolete ones removed
  </done>
</task>

<task type="auto">
  <name>Task 3: Verify test suite pass rate and document fixes</name>
  <files>.planning/phases/15-codebase-completion/15-01-SUMMARY.md</files>
  <action>
    1. Run full skill test suite: `pytest tests/test_skill*.py -v --tb=short`
    2. Verify 90/90 tests pass (100% pass rate for skill tests)
    3. Create SUMMARY.md documenting:
       - Fixture standardization changes (db vs db_session)
       - Async test fixes (@pytest.mark.asyncio added)
       - TODO evaluation results (critical/future/delete counts)
       - Test pass rate improvement (82/90 â†’ 90/90)
       - Remaining work for next plans (type hints, error handling)
  </action>
  <verify>
    Command: `pytest tests/test_skill*.py --co -q | grep "test_" | wc -l`
    Expected: 90 tests collected with zero collection errors
  </verify>
  <done>
    SUMMARY.md created with 90/90 skill tests passing, TODO evaluation complete
  </done>
</task>

</tasks>

<verification>
1. Run skill test suite: `pytest tests/test_skill_integration.py tests/test_skill_episodic_integration.py -v`
2. Verify zero fixture-related errors (no "fixture 'db_session' not found")
3. Verify zero async-related errors (no "coroutine was never awaited")
4. Check TODO evaluation: `grep -r "TODO" backend/core/ --include="*.py" | grep -v "evaluated" | wc -l` should be 0
5. Read SUMMARY.md to confirm documentation of all fixes
</verification>

<success_criteria>
1. All 90 skill tests pass (100% pass rate achieved from 91%)
2. Test fixtures standardized to `db_session` naming across all conftest.py files
3. All async tests use @pytest.mark.asyncio decorator
4. Production TODO comments evaluated with documented decisions
5. SUMMARY.md created documenting all changes and remaining work
</success_criteria>

<output>
After completion, create `.planning/phases/15-codebase-completion/15-01-SUMMARY.md`
</output>
