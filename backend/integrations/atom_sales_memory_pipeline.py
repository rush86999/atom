"""
ATOM Sales Data Memory Pipeline
Background ingestion for HubSpot and Salesforce data into LanceDB for AI/RAG.
"""

import asyncio
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
import json
import os

from integrations.atom_communication_ingestion_pipeline import (
    LanceDBMemoryManager, 
    CommunicationData, 
    get_memory_manager
)
from core.websockets import manager
from integrations.hubspot_service import hubspot_service
# from integrations.salesforce_service import salesforce_service # Import when ready

logger = logging.getLogger(__name__)

class SalesMemoryPipeline:
    """
    Ingests Sales data (Deals, Contacts) into the shared LanceDB memory.
    Uses 'atom_communications' table but tagged with app_type='hubspot'/'salesforce'.
    """

    def __init__(self, workspace_id: Optional[str] = None):
        self.memory_manager = get_memory_manager(workspace_id)
        
    async def run_pipeline(self):
        """Main entry point for scheduled ingestion"""
        logger.info("Starting Sales Memory Pipeline...")
        
        # 1. Ingest HubSpot
        await self._ingest_hubspot()
        
        # 2. Ingest Salesforce (Placeholder)
        # await self._ingest_salesforce()
        
        logger.info("Sales Memory Pipeline Completed.")
        
        # Broadcast Status Update
        try:
            await manager.broadcast_event("communication_stats", "status_update", {
                "pipeline": "sales",
                "status": "completed",
                "timestamp": datetime.now().isoformat()
            })
        except Exception as e:
            logger.error(f"Failed to broadcast sales status: {e}")

    async def _ingest_hubspot(self):
        """Fetch newly modified deals from HubSpot and ingest"""
        try:
            # In a real scenario, we'd store cursor/last_sync_time in DB
            logger.info("Fetching HubSpot Deals for Memory Ingestion...")
            
            # Use existing service
            # Limit to 50 for this iteration to avoid rate limits
            token = os.getenv("HUBSPOT_ACCESS_TOKEN")
            if not token:
                logger.warning("Skipping HubSpot ingestion: No Access Token")
                return

            deals_response = hubspot_service.get_deals(token, limit=50) 
            deals = deals_response.get('results', [])
            
            count = 0
            for deal in deals:
                if self._ingest_deal("hubspot", deal):
                    count += 1
            
            logger.info(f"Successfully ingested {count} HubSpot deals into memory.")
            
        except Exception as e:
            logger.error(f"HubSpot Ingestion Failed: {e}")

    def _ingest_deal(self, source: str, deal_data: Dict[str, Any]) -> bool:
        """Map deal to CommunicationData structure and ingest"""
        try:
            props = deal_data.get('properties', {})
            deal_name = props.get('dealname', 'Unnamed Deal')
            amount = props.get('amount', '0')
            stage = props.get('dealstage', 'unknown')
            
            # Map Deal to a "Knowledge Record" format
            # We treat a Deal as a "message" from the CRM to the System
            content = f"Deal: {deal_name}\nAmount: {amount}\nStage: {stage}\nSource: {source.title()}"
            
            data = CommunicationData(
                id=f"{source}_deal_{deal_data.get('id')}",
                app_type=f"{source}_deal", # e.g. hubspot_deal
                timestamp=datetime.now(), # Ideally use last_modified from deal
                direction="inbound", 
                sender="system",
                recipient="atom",
                subject=f"Deal Update: {deal_name}",
                content=content,
                attachments=[],
                metadata={
                    "deal_id": deal_data.get('id'),
                    "amount": amount,
                    "stage": stage,
                    "raw_data": json.dumps(props)
                },
                status="active",
                priority="normal",
                tags=["sales", "deal", source],
                vector_embedding=None # Will be generated by manager
            )
            
            # Use the generic ingest method from manager
            return self.memory_manager.ingest_communication(data)
            
        except Exception as e:
            logger.error(f"Error mapping/ingesting deal {deal_data.get('id')}: {e}")
            return False

# Global Instance
sales_pipeline = SalesMemoryPipeline()

if __name__ == "__main__":
    # Test run
    logging.basicConfig(level=logging.INFO)
    asyncio.run(sales_pipeline.run_pipeline())
