---
phase: 10-fix-remaining-test-failures
plan: 05
type: execute
wave: 2
depends_on: [10-01, 10-02]
files_modified:
  - .planning/phases/10-fix-remaining-test-failures/10-05-performance-stability-report.md
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "Test suite execution time is under 60 minutes"
    - "No flaky tests detected across 3 runs"
    - "Performance and stability report exists"
  artifacts:
    - path: ".planning/phases/10-fix-remaining-test-failures/10-05-performance-stability-report.md"
      provides: "Performance and stability verification report"
      contains: "execution time|flaky tests|performance|stability"
  key_links:
    - from: "10-05-performance-stability-report.md"
      to: "pytest timing data"
      via: "Test execution time measurement"
      pattern: "duration|seconds|minutes"
---

<objective>
Verify performance (<60 minutes) and stability (no flaky tests) requirements.

Gap 3 from verification: No execution time measurement (TQ-03), no flaky test detection across multiple runs (TQ-04).

Purpose: Confirm test suite meets performance and stability requirements
Output: Performance and stability verification report
</objective>

<execution_context>
@/Users/rushiparikh/.claude/get-shit-done/workflows/execute-plan.md
@/Users/rushiparikh/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/10-fix-remaining-test-failures/10-01-SUMMARY.md
@.planning/phases/10-fix-remaining-test-failures/10-02-SUMMARY.md

TQ-03 requirement: Full test suite completes in <60 minutes
TQ-04 requirement: No flaky tests across multiple runs

Gap 3 from verification:
- No execution time measurement
- No flaky test detection across multiple runs
- No performance/stability report

Flaky test definition: Test that passes sometimes and fails sometimes on identical code
</context>

<tasks>

<task type="auto">
  <name>Measure test suite execution time (TQ-03)</name>
  <files>.planning/phases/10-fix-remaining-test-failures/10-05-performance-stability-report.md</files>
  <action>
  Run full test suite with timing measurement:

  ```bash
  echo "Starting test run at $(date -u +"%Y-%m-%dT%H:%M:%SZ")" > /tmp/perf_test_start.txt
  time PYTHONPATH=/Users/rushiparikh/projects/atom/backend pytest tests/ -v --tb=line 2>&1 | tee /tmp/perf_test_run.log
  echo "Finished test run at $(date -u +"%Y-%m-%dT%H:%M:%SZ")" > /tmp/perf_test_end.txt
  ```

  Create performance report with timing data:

  ```bash
  cat > .planning/phases/10-fix-remaining-test-failures/10-05-performance-stability-report.md << 'EOF'
  # Test Performance and Stability Verification Report
  ## Phase 10 Plan 05 - Gap Closure for TQ-03 and TQ-04

  **Generated:** 2026-02-15
  **Requirements:**
  - TQ-03: Full test suite completes in <60 minutes
  - TQ-04: No flaky tests across multiple runs

  ## TQ-03: Execution Time Measurement

  ### Run 1 Execution Time
  **Start:** $(cat /tmp/perf_test_start.txt)
  **End:** $(cat /tmp/perf_test_end.txt)
  **Duration:** [extract from time output]

  ### Test Count
  **Total tests:** $(grep -oE "[0-9]+ collected" /tmp/perf_test_run.log | grep -oE "[0-9]+")

  ### TQ-03 Requirement Met
  **Required:** <60 minutes
  **Achieved:** [minutes] minutes
  **Status:** [PASS/FAIL]

  EOF
  ```

  Extract duration from `time` output (real, user, sys) and convert to minutes.
  </action>
  <verify>grep -E "Execution Time|TQ-03 Requirement Met" .planning/phases/10-fix-remaining-test-failures/10-05-performance-stability-report.md</verify>
  <done>Execution time measured and documented in report</done>
</task>

<task type="auto">
  <name>Detect flaky tests across 3 runs (TQ-04)</name>
  <files>.planning/phases/10-fix-remaining-test-failures/10-05-performance-stability-report.md</files>
  <action>
  Run test suite 2 more times and compare results to detect flaky tests:

  ```bash
  # Run 2
  time PYTHONPATH=/Users/rushiparikh/projects/atom/backend pytest tests/ -v --tb=line 2>&1 | tee /tmp/perf_test_run2.log

  # Run 3
  time PYTHONPATH=/Users/rushiparikh/projects/atom/backend pytest tests/ -v --tb=line 2>&1 | tee /tmp/perf_test_run3.log
  ```

  Extract failed tests from each run:

  ```bash
  for run in 1 2 3; do
    grep "FAILED" /tmp/perf_test_run${run}.log | sed 's/^/Run '${run}': /' > /tmp/failed_run${run}.txt
  done
  ```

  Compare failures to identify flaky tests:

  ```bash
  # Find tests that failed in some runs but not others
  cat /tmp/failed_run*.txt | sort | uniq -c | grep -v "3 Run" > /tmp/flaky_tests.txt
  ```

  Append to report:

  ```bash
  cat >> .planning/phases/10-fix-remaining-test-failures/10-05-performance-stability-report.md << 'EOF'

  ## TQ-04: Flaky Test Detection

  ### Run 2 Execution Time
  **Duration:** [extract from time output]

  ### Run 3 Execution Time
  **Duration:** [extract from time output]

  ### Failure Comparison Across Runs

  | Test Name | Run 1 | Run 2 | Run 3 | Flaky? |
  |-----------|-------|-------|-------|--------|
  [List each test that failed in any run with status per run]

  ### Flaky Tests Detected
  ```
  if [ -s /tmp/flaky_tests.txt ]; then
    cat /tmp/flaky_tests.txt
  else
    echo "No flaky tests detected"
  fi
  ```
  ### TQ-04 Requirement Met
  **Required:** No flaky tests
  **Detected:** [count] flaky tests
  **Status:** [PASS/FAIL]

  EOF
  ```

  If flaky tests detected, list them and recommend fixes.
  </action>
  <verify>grep -E "Flaky Test Detection|TQ-04 Requirement Met" .planning/phases/10-fix-remaining-test-failures/10-05-performance-stability-report.md</verify>
  <done>Flaky test detection completed across 3 runs</done>
</task>

<task type="auto">
  <name>Generate performance summary and recommendations</name>
  <files>.planning/phases/10-fix-remaining-test-failures/10-05-performance-stability-report.md</files>
  <action>
  Calculate average execution time and append summary:

  ```bash
  cat >> .planning/phases/10-fix-remaining-test-failures/10-05-performance-stability-report.md << 'EOF'

  ## Summary

  ### Performance (TQ-03)
  | Metric | Run 1 | Run 2 | Run 3 | Average |
  |--------|-------|-------|-------|---------|
  | Duration (min) | [r1] | [r2] | [r3] | [avg] |

  **TQ-03 Status:** [PASS/FAIL]

  ### Stability (TQ-04)
  | Metric | Value |
  |--------|-------|
  | Flaky Tests | [count] |
  | Consistent Pass Rate | [yes/no] |

  **TQ-04 Status:** [PASS/FAIL]

  ### Overall Phase 10 Verification Status
  | Requirement | Status |
  |-------------|--------|
  | TQ-01: Tests collect successfully | PASS (from 10-01) |
  | TQ-02: 98%+ pass rate | [from 10-04] |
  | TQ-03: <60 minutes execution | [above] |
  | TQ-04: No flaky tests | [above] |

  ### Recommendations
  [If TQ-03 FAIL: Suggest test parallelization, test splitting, or optimization]
  [If TQ-04 FAIL: List specific flaky tests and recommended fixes]
  [If both PASS: Note success and next steps]

  EOF
  ```

  Fill in actual values from the timing data.
  </action>
  <verify>grep -E "Summary|Overall Phase 10" .planning/phases/10-fix-remaining-test-failures/10-05-performance-stability-report.md</verify>
  <done>Performance summary and recommendations documented</done>
</task>

</tasks>

<verification>
1. Execution time measured for all 3 runs
2. Average execution time < 60 minutes
3. Flaky test detection performed
4. No flaky tests found (or documented if found)
5. Report includes recommendations
</verification>

<success_criteria>
- Report document exists at 10-05-performance-stability-report.md
- 3 full test runs completed with timing
- TQ-03: Average execution time <60 minutes
- TQ-04: Zero flaky tests detected
- Recommendations documented (even if both pass)
</success_criteria>

<output>
After completion, create `.planning/phases/10-fix-remaining-test-failures/10-05-SUMMARY.md`
</output>
