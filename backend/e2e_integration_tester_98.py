#!/usr/bin/env python3
"""
E2E Integration Testing Framework for 98% Truth Validation
=================================================================

This framework provides comprehensive end-to-end testing to validate ATOM's
marketing claims with real API integrations and evidence collection.

Target: 98% validation truth score
Method: Evidence-based testing with real credentials
"""

import asyncio
from dataclasses import dataclass
from datetime import datetime
import json
import logging
import os
from pathlib import Path
import subprocess
import sys
import time
from typing import Any, Dict, List, Optional, Tuple
import requests

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class TestResult:
    """Test result data structure"""
    category: str
    test_name: str
    success: bool
    details: List[str]
    evidence: Dict[str, Any]
    execution_time: float

class E2EIntegrationTester:
    """
    Comprehensive E2E Integration Tester
    """

    def __init__(self):
        self.start_time = time.time()
        self.test_results = []
        self.evidence_collected = {}
        self.credentials = {}
        self.backend_url = "http://localhost:8000"
        self.backend_process = None

    async def collect_credentials_interactive(self) -> bool:
        """Collect credentials interactively from user"""
        print("\n" + "="*80)
        print("üîê ATOM E2E Integration Testing - Credential Collection")
        print("="*80)
        print("This framework will validate ATOM's marketing claims using real API integrations.")
        print("All credentials are stored temporarily in memory only and never saved to disk.\n")

        print("For 98% validation, we need to test real integrations.")
        print("You can skip any credential by pressing Enter.\n")

        credential_prompts = [
            ("OpenAI API Key", "sk-proj-", "For AI text generation and analysis"),
            ("Anthropic API Key", "sk-ant-", "For Claude AI integration"),
            ("DeepSeek API Key", "sk-", "For cost-effective AI analysis"),
            ("Slack Bot Token", "xoxb-", "For team communication workflows"),
            ("GitHub Token", "github_pat_", "For development automation"),
        ]

        for name, pattern, description in credential_prompts:
            print(f"üìã {name}")
            print(f"   Description: {description}")
            print(f"   Pattern: {pattern}*")

            # Use getpass for secure input
            try:
                import getpass
                credential = getpass.getpass(f"   Enter {name} (or press Enter to skip): ").strip()

                if credential and len(credential) > 10:
                    if pattern in credential:
                        self.credentials[name.replace(' ', '_').lower()] = credential
                        print(f"   ‚úÖ {name} collected successfully")
                    else:
                        print(f"   ‚ö†Ô∏è  {name} doesn't match expected pattern, skipping")
                else:
                    print(f"   ‚è≠Ô∏è  {name} skipped")
            except KeyboardInterrupt:
                print(f"\n   ‚ùå {name} collection cancelled")
                continue
            except Exception as e:
                print(f"   ‚ùå Error collecting {name}: {str(e)}")

            print()

        print(f"\nüìä Credentials Collected: {len(self.credentials)}/5")

        if len(self.credentials) >= 2:
            print("‚úÖ Sufficient credentials for comprehensive 98% validation testing")
            return True
        else:
            print("‚ö†Ô∏è  Limited credentials - will run simulated tests for missing integrations")
            print("   (Note: This may reduce validation accuracy below 98% target)")
            return True

    async def start_backend_server(self) -> bool:
        """Start ATOM backend server"""
        print("\nüöÄ Starting ATOM Backend Server...")

        try:
            # Check if server is already running
            try:
                response = requests.get(f"{self.backend_url}/health", timeout=5)
                if response.status_code == 200:
                    print("‚úÖ Backend server already running")
                    return True
            except:
                pass

            # Start backend server
            backend_file = "main_api_app.py"
            if os.path.exists(backend_file):
                self.backend_process = subprocess.Popen([
                    sys.executable, backend_file
                ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)

                # Wait for server to start
                for i in range(30):
                    try:
                        response = requests.get(f"{self.backend_url}/health", timeout=2)
                        if response.status_code == 200:
                            print("‚úÖ Backend server started successfully")
                            return True
                    except:
                        pass
                    time.sleep(1)

                print("‚ö†Ô∏è  Backend server started but health check failed")
                return True
            else:
                print("‚ùå Backend server file not found")
                return False

        except Exception as e:
            print(f"‚ùå Error starting backend server: {str(e)}")
            return False

    async def test_ai_integrations(self) -> TestResult:
        """Test AI provider integrations"""
        print("\nü§ñ Testing AI Provider Integrations...")

        start_time = time.time()
        details = []
        evidence = {}
        success_count = 0
        total_tests = 0

        ai_providers = {
            'openai': {
                'url': 'https://api.openai.com/v1/models',
                'headers': lambda: {'Authorization': f'Bearer {self.credentials.get("openai_api_key", "")}'},
                'test_prompt': 'What is ATOM?'
            },
            'anthropic': {
                'url': 'https://api.anthropic.com/v1/messages',
                'headers': lambda: {
                    'x-api-key': self.credentials.get("anthropic_api_key", ""),
                    'anthropic-version': '2023-06-01',
                    'content-type': 'application/json'
                },
                'test_prompt': 'What is ATOM?'
            },
            'deepseek': {
                'url': 'https://api.deepseek.com/v1/models',
                'headers': lambda: {'Authorization': f'Bearer {self.credentials.get("deepseek_api_key", "")}'},
                'test_prompt': 'What is ATOM?'
            }
        }

        for provider, config in ai_providers.items():
            total_tests += 1
            credential_key = f"{provider}_api_key"

            if credential_key not in self.credentials:
                details.append(f"‚è≠Ô∏è  {provider.title()}: Skipped (no credential)")
                continue

            try:
                # Test API connectivity
                if provider == 'anthropic':
                    # Test Anthropic messages API
                    data = {
                        'model': 'claude-3-haiku-20240307',
                        'max_tokens': 100,
                        'messages': [{'role': 'user', 'content': config['test_prompt']}]
                    }
                    response = requests.post(
                        config['url'],
                        headers=config['headers'](),
                        json=data,
                        timeout=10
                    )
                else:
                    # Test OpenAI/DeepSeek models API
                    response = requests.get(
                        config['url'],
                        headers=config['headers'](),
                        timeout=10
                    )

                if response.status_code in [200, 201]:
                    details.append(f"‚úÖ {provider.title()}: API connection successful")
                    evidence[f"{provider}_response"] = {
                        'status_code': response.status_code,
                        'response_time': response.elapsed.total_seconds(),
                        'sample_data': str(response.text[:200]) + "..." if len(response.text) > 200 else response.text
                    }
                    success_count += 1
                else:
                    details.append(f"‚ùå {provider.title()}: API error {response.status_code}")
                    evidence[f"{provider}_error"] = {
                        'status_code': response.status_code,
                        'error': response.text[:200]
                    }

            except Exception as e:
                details.append(f"‚ùå {provider.title()}: Connection failed - {str(e)}")
                evidence[f"{provider}_exception"] = str(e)

        # Test ATOM's own AI integration if backend is available
        try:
            atom_response = requests.post(
                f"{self.backend_url}/api/v1/nlp/analyze",
                json={'text': 'Test ATOM AI integration', 'analysis_type': 'intent'},
                timeout=10
            )
            if atom_response.status_code == 200:
                details.append("‚úÖ ATOM NLP API: Integration successful")
                evidence['atom_nlp_response'] = atom_response.json()
                success_count += 1
            else:
                details.append(f"‚ö†Ô∏è  ATOM NLP API: Error {atom_response.status_code}")
        except:
            details.append("‚ö†Ô∏è  ATOM NLP API: Backend not available")

        total_tests += 1  # ATOM NLP test

        success_rate = success_count / total_tests
        execution_time = time.time() - start_time

        details.append(f"üìä AI Integration Success Rate: {success_rate:.1%} ({success_count}/{total_tests})")

        return TestResult(
            category="AI Integration",
            test_name="Multi-Provider AI Testing",
            success=success_rate >= 0.8,
            details=details,
            evidence=evidence,
            execution_time=execution_time
        )

    async def test_workflow_automation(self) -> TestResult:
        """Test workflow automation capabilities"""
        print("\n‚öôÔ∏è  Testing Workflow Automation...")

        start_time = time.time()
        details = []
        evidence = {}

        try:
            # Test workflow creation
            workflow_def = {
                'name': 'E2E Test Workflow',
                'description': 'Automated test workflow for 98% validation',
                'trigger_type': 'manual',
                'steps': [
                    {
                        'name': 'test_step',
                        'action_type': 'send_notification',
                        'config': {'message': 'E2E testing workflow step'}
                    }
                ]
            }

            response = requests.post(
                f"{self.backend_url}/api/v1/workflows",
                json=workflow_def,
                timeout=10
            )

            if response.status_code in [200, 201]:
                workflow_data = response.json()
                workflow_id = workflow_data.get('id') or workflow_data.get('workflow_id')
                details.append("‚úÖ Workflow creation successful")
                evidence['workflow_creation'] = workflow_data

                # Test workflow execution
                if workflow_id:
                    exec_response = requests.post(
                        f"{self.backend_url}/api/v1/workflows/{workflow_id}/execute",
                        json={'context': {'test_mode': True}},
                        timeout=15
                    )

                    if exec_response.status_code in [200, 201]:
                        exec_data = exec_response.json()
                        details.append("‚úÖ Workflow execution successful")
                        evidence['workflow_execution'] = exec_data
                    else:
                        details.append(f"‚ö†Ô∏è  Workflow execution error: {exec_response.status_code}")
                else:
                    details.append("‚ö†Ô∏è  No workflow ID returned")

            else:
                details.append(f"‚ùå Workflow creation failed: {response.status_code}")

        except Exception as e:
            details.append(f"‚ùå Workflow automation test failed: {str(e)}")

        execution_time = time.time() - start_time

        return TestResult(
            category="Workflow Automation",
            test_name="End-to-End Workflow Testing",
            success=len([d for d in details if d.startswith("‚úÖ")]) >= len(details) / 2,
            details=details,
            evidence=evidence,
            execution_time=execution_time
        )

    async def test_service_integrations(self) -> TestResult:
        """Test third-party service integrations"""
        print("\nüîó Testing Service Integrations...")

        start_time = time.time()
        details = []
        evidence = {}
        success_count = 0
        total_tests = 0

        # Test Slack integration
        if 'slack_bot_token' in self.credentials:
            total_tests += 1
            try:
                response = requests.get(
                    "https://slack.com/api/auth.test",
                    headers={'Authorization': f'Bearer {self.credentials["slack_bot_token"]}'},
                    timeout=10
                )

                if response.status_code == 200 and response.json().get('ok'):
                    details.append("‚úÖ Slack API: Authentication successful")
                    evidence['slack_auth'] = response.json()
                    success_count += 1
                else:
                    details.append("‚ùå Slack API: Authentication failed")
            except Exception as e:
                details.append(f"‚ùå Slack API: Connection failed - {str(e)}")
        else:
            details.append("‚è≠Ô∏è  Slack: Skipped (no token)")

        # Test GitHub integration
        if 'github_token' in self.credentials:
            total_tests += 1
            try:
                response = requests.get(
                    "https://api.github.com/user",
                    headers={'Authorization': f'token {self.credentials["github_token"]}'},
                    timeout=10
                )

                if response.status_code == 200:
                    details.append("‚úÖ GitHub API: Authentication successful")
                    evidence['github_auth'] = response.json()
                    success_count += 1
                else:
                    details.append(f"‚ùå GitHub API: Authentication failed ({response.status_code})")
            except Exception as e:
                details.append(f"‚ùå GitHub API: Connection failed - {str(e)}")
        else:
            details.append("‚è≠Ô∏è  GitHub: Skipped (no token)")

        # Test internal service integrations
        internal_services = [
            ('NLP Service', '/api/v1/nlp/health'),
            ('Workflow Engine', '/api/v1/workflows/health'),
            ('Database', '/api/v1/health'),
            ('BYOK System', '/api/v1/byok/health')
        ]

        for service_name, endpoint in internal_services:
            total_tests += 1
            try:
                response = requests.get(f"{self.backend_url}{endpoint}", timeout=5)
                if response.status_code == 200:
                    details.append(f"‚úÖ {service_name}: Healthy")
                    evidence[f"{service_name.lower().replace(' ', '_')}_health"] = response.json()
                    success_count += 1
                else:
                    details.append(f"‚ö†Ô∏è  {service_name}: Status {response.status_code}")
            except:
                details.append(f"‚ö†Ô∏è  {service_name}: Not available")

        if total_tests == 0:
            success_rate = 0.0
        else:
            success_rate = success_count / total_tests

        execution_time = time.time() - start_time
        details.append(f"üìä Service Integration Success Rate: {success_rate:.1%} ({success_count}/{total_tests})")

        return TestResult(
            category="Service Integration",
            test_name="Multi-Service Integration Testing",
            success=success_rate >= 0.6,
            details=details,
            evidence=evidence,
            execution_time=execution_time
        )

    async def test_data_analysis_capabilities(self) -> TestResult:
        """Test data analysis and business intelligence"""
        print("\nüìä Testing Data Analysis Capabilities...")

        start_time = time.time()
        details = []
        evidence = {}

        # Test data analysis workflows
        test_scenarios = [
            {
                'name': 'Sales Data Analysis',
                'data': {'sales': [100, 150, 200, 175, 300], 'period': 'Q4 2024'},
                'analysis_type': 'trend'
            },
            {
                'name': 'Customer Sentiment Analysis',
                'data': {'feedback': ['Great product', 'Needs improvement', 'Excellent service']},
                'analysis_type': 'sentiment'
            },
            {
                'name': 'Performance Metrics',
                'data': {'metrics': {'response_time': 120, 'throughput': 1000, 'error_rate': 0.01}},
                'analysis_type': 'performance'
            }
        ]

        success_count = 0

        for scenario in test_scenarios:
            try:
                response = requests.post(
                    f"{self.backend_url}/api/v1/nlp/analyze",
                    json={
                        'text': json.dumps(scenario['data']),
                        'analysis_type': scenario['analysis_type']
                    },
                    timeout=10
                )

                if response.status_code == 200:
                    result = response.json()
                    details.append(f"‚úÖ {scenario['name']}: Analysis successful")
                    evidence[f"{scenario['name'].lower().replace(' ', '_')}_analysis"] = result
                    success_count += 1
                else:
                    details.append(f"‚ö†Ô∏è  {scenario['name']}: Analysis error ({response.status_code})")

            except Exception as e:
                details.append(f"‚ùå {scenario['name']}: Analysis failed - {str(e)}")

        # Test dashboard data
        try:
            dashboard_response = requests.get(
                f"{self.backend_url}/api/v1/analytics/dashboard",
                timeout=10
            )
            if dashboard_response.status_code == 200:
                details.append("‚úÖ Analytics Dashboard: Data accessible")
                evidence['dashboard_data'] = dashboard_response.json()
                success_count += 1
            else:
                details.append("‚ö†Ô∏è  Analytics Dashboard: Not available")
        except:
            details.append("‚ö†Ô∏è  Analytics Dashboard: Connection failed")

        execution_time = time.time() - start_time
        success_rate = success_count / len(test_scenarios)
        details.append(f"üìä Data Analysis Success Rate: {success_rate:.1%} ({success_count}/{len(test_scenarios)})")

        return TestResult(
            category="Data Analysis",
            test_name="Business Intelligence Testing",
            success=success_rate >= 0.7,
            details=details,
            evidence=evidence,
            execution_time=execution_time
        )

    async def calculate_truth_score(self) -> Dict[str, Any]:
        """Calculate overall truth validation score"""
        print("\nüéØ Calculating 98% Truth Validation Score...")

        if not self.test_results:
            return {'overall_score': 0.0, 'validation_level': 'NO_DATA'}

        # Weight categories by importance
        category_weights = {
            'AI Integration': 0.30,      # 30% - Core AI capabilities
            'Workflow Automation': 0.25,  # 25% - Core functionality
            'Service Integration': 0.25,  # 25% - Real-world integration
            'Data Analysis': 0.20        # 20% - Business value
        }

        weighted_scores = []
        category_scores = {}

        for result in self.test_results:
            category = result.category
            if category not in category_weights:
                continue

            # Calculate category score based on success and detail quality
            success_ratio = len([d for d in result.details if d.startswith("‚úÖ")]) / len(result.details)
            category_score = 1.0 if result.success else success_ratio

            weighted_score = category_score * category_weights[category]
            weighted_scores.append(weighted_score)
            category_scores[category] = {
                'score': category_score,
                'weight': category_weights[category],
                'weighted_score': weighted_score,
                'tests_passed': len([d for d in result.details if d.startswith("‚úÖ")]),
                'total_tests': len(result.details)
            }

        overall_score = sum(weighted_scores)

        # Determine validation level
        if overall_score >= 0.98:
            validation_level = "EXCEPTIONAL (98%+ Achieved)"
            status = "üèÜ EXCEPTIONAL SUCCESS"
        elif overall_score >= 0.95:
            validation_level = "EXCELLENT (95%+ Achieved)"
            status = "üéâ EXCELLENT SUCCESS"
        elif overall_score >= 0.90:
            validation_level = "VERY GOOD (90%+ Achieved)"
            status = "‚úÖ VERY GOOD"
        elif overall_score >= 0.80:
            validation_level = "GOOD (80%+ Achieved)"
            status = "‚úÖ GOOD"
        elif overall_score >= 0.70:
            validation_level = "ACCEPTABLE (70%+ Achieved)"
            status = "‚ö†Ô∏è  ACCEPTABLE"
        else:
            validation_level = "NEEDS IMPROVEMENT (<70%)"
            status = "‚ùå NEEDS IMPROVEMENT"

        return {
            'overall_score': overall_score,
            'validation_level': validation_level,
            'status': status,
            'category_breakdown': category_scores,
            'target_achieved': overall_score >= 0.98
        }

    async def generate_comprehensive_report(self, truth_score: Dict[str, Any]) -> Dict[str, Any]:
        """Generate comprehensive validation report"""

        total_execution_time = time.time() - self.start_time

        report = {
            'test_metadata': {
                'timestamp': datetime.now().isoformat(),
                'total_execution_time': total_execution_time,
                'target_truth_score': 0.98,
                'actual_truth_score': truth_score['overall_score'],
                'target_achieved': truth_score['target_achieved'],
                'credentials_tested': len(self.credentials),
                'validation_level': truth_score['validation_level']
            },
            'test_results_summary': {
                'total_categories_tested': len(self.test_results),
                'successful_categories': len([r for r in self.test_results if r.success]),
                'total_tests_run': sum(len(r.details) for r in self.test_results),
                'total_successful_tests': sum(len([d for d in r.details if d.startswith("‚úÖ")]) for r in self.test_results)
            },
            'category_results': {},
            'evidence_summary': {},
            'marketing_claims_validation': {},
            'recommendations': []
        }

        # Process category results
        for result in self.test_results:
            report['category_results'][result.category] = {
                'success': result.success,
                'execution_time': result.execution_time,
                'tests_passed': len([d for d in result.details if d.startswith("‚úÖ")]),
                'total_tests': len(result.details),
                'details': result.details,
                'evidence_collected': len(result.evidence) > 0
            }

        # Evidence summary
        all_evidence = {}
        for result in self.test_results:
            all_evidence.update(result.evidence)

        report['evidence_summary'] = {
            'total_evidence_items': len(all_evidence),
            'evidence_types': list(set(k.split('_')[0] for k in all_evidence.keys())),
            'has_real_api_evidence': len(self.credentials) > 0
        }

        # Marketing claims validation
        marketing_claims = {
            'AI-Powered Workflow Automation': truth_score['category_breakdown'].get('AI Integration', {}).get('weighted_score', 0) + truth_score['category_breakdown'].get('Workflow Automation', {}).get('weighted_score', 0),
            'Multi-Provider Integration': truth_score['category_breakdown'].get('Service Integration', {}).get('weighted_score', 0),
            'Real-Time Analytics': truth_score['category_breakdown'].get('Data Analysis', {}).get('weighted_score', 0),
            'Enterprise-Grade Reliability': truth_score['overall_score']  # Overall score represents reliability
        }

        report['marketing_claims_validation'] = {
            claim: {
                'validation_score': min(score * 2, 1.0),  # Scale to 0-1 range
                'validated': score >= 0.4
            }
            for claim, score in marketing_claims.items()
        }

        # Recommendations
        if truth_score['overall_score'] >= 0.98:
            report['recommendations'] = [
                "üèÜ EXCEPTIONAL: Ready for premium marketing with 98%+ validation",
                "üìà Leverage test results for high-confidence marketing claims",
                "üéØ Emphasize real API integration capabilities in marketing"
            ]
        elif truth_score['overall_score'] >= 0.90:
            report['recommendations'] = [
                "‚úÖ EXCELLENT: Strong foundation for marketing claims",
                "üîß Address minor issues to reach 98% validation target",
                "üìä Focus on improving underperforming categories"
            ]
        else:
            report['recommendations'] = [
                "‚ö†Ô∏è  Review and improve core functionality before marketing launch",
                "üîç Focus on fixing failed tests in critical categories",
                "üß™ Consider additional testing with more credentials"
            ]

        return report

    async def run_comprehensive_tests(self) -> Dict[str, Any]:
        """Run comprehensive E2E integration tests"""
        print("\nüöÄ Starting ATOM E2E Integration Testing for 98% Truth Validation")
        print("="*80)

        try:
            # Step 1: Collect credentials
            if not await self.collect_credentials_interactive():
                return {'error': 'Credential collection failed'}

            # Step 2: Start backend server
            await self.start_backend_server()

            # Step 3: Run comprehensive test suites
            print("\nüß™ Running Comprehensive Test Suites...")

            test_suites = [
                self.test_ai_integrations(),
                self.test_workflow_automation(),
                self.test_service_integrations(),
                self.test_data_analysis_capabilities()
            ]

            # Execute all test suites
            for test_suite in test_suites:
                result = await test_suite
                self.test_results.append(result)

                # Print immediate results
                print(f"\nüìã {result.category} Results:")
                for detail in result.details:
                    print(f"   {detail}")
                print(f"   ‚è±Ô∏è  Execution Time: {result.execution_time:.2f}s")

            # Step 4: Calculate truth score
            truth_score = await self.calculate_truth_score()

            # Step 5: Generate comprehensive report
            report = await self.generate_comprehensive_report(truth_score)

            # Print final results
            print("\n" + "="*80)
            print("üèÅ FINAL E2E INTEGRATION TEST RESULTS")
            print("="*80)

            print(f"\n{truth_score['status']} - {truth_score['validation_level']}")
            print(f"üéØ Overall Truth Score: {truth_score['overall_score']:.1%}")
            print(f"üéØ Target (98%): {'‚úÖ ACHIEVED' if truth_score['target_achieved'] else '‚ùå NOT ACHIEVED'}")

            print(f"\nüìä Category Breakdown:")
            for category, data in truth_score['category_breakdown'].items():
                status = "‚úÖ" if data['score'] >= 0.8 else "‚ö†Ô∏è" if data['score'] >= 0.6 else "‚ùå"
                print(f"   {status} {category}: {data['score']:.1%} ({data['tests_passed']}/{data['total_tests']} tests)")

            print(f"\nüìã Overall Test Summary:")
            summary = report['test_results_summary']
            print(f"   üìä Categories Tested: {summary['total_categories_tested']}")
            print(f"   ‚úÖ Successful Categories: {summary['successful_categories']}")
            print(f"   üß™ Total Tests Run: {summary['total_tests_run']}")
            print(f"   ‚úÖ Successful Tests: {summary['total_successful_tests']}")
            print(f"   üìà Overall Success Rate: {summary['total_successful_tests']/summary['total_tests_run']:.1%}")

            print(f"\nüí° Recommendations:")
            for rec in report['recommendations']:
                print(f"   {rec}")

            return {
                'truth_score': truth_score,
                'report': report,
                'test_results': self.test_results,
                'evidence': self.evidence_collected
            }

        except KeyboardInterrupt:
            print("\n\n‚ö†Ô∏è  Testing interrupted by user")
            return {'error': 'Testing interrupted'}
        except Exception as e:
            print(f"\n‚ùå Testing failed with error: {str(e)}")
            logger.exception("Testing failed")
            return {'error': str(e)}
        finally:
            # Cleanup backend process
            if self.backend_process:
                try:
                    self.backend_process.terminate()
                    self.backend_process.wait(timeout=5)
                except:
                    pass

            # Clear credentials from memory
            self.credentials.clear()
            print("\nüßπ Cleanup completed - credentials cleared from memory")

async def main():
    """Main execution function"""
    tester = E2EIntegrationTester()

    try:
        results = await tester.run_comprehensive_tests()

        # Save results to file
        if 'error' not in results:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            report_file = f"E2E_INTEGRATION_VALIDATION_REPORT_{timestamp}.json"

            with open(report_file, 'w') as f:
                json.dump(results, f, indent=2, default=str)

            print(f"\nüíæ Detailed report saved to: {report_file}")

            # Also save a human-readable version
            readable_file = f"E2E_INTEGRATION_VALIDATION_REPORT_{timestamp}.md"

            with open(readable_file, 'w') as f:
                f.write(f"# ATOM E2E Integration Validation Report\n\n")
                f.write(f"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"**Target:** 98% Truth Validation\n")
                f.write(f"**Achieved:** {results['truth_score']['overall_score']:.1%}\n")
                f.write(f"**Status:** {results['truth_score']['validation_level']}\n\n")

                f.write("## Executive Summary\n\n")
                f.write(f"{results['truth_score']['status']} - The ATOM platform achieved **{results['truth_score']['overall_score']:.1%}** truth validation score. ")
                f.write(f"This {'exceeds' if results['truth_score']['target_achieved'] else 'falls short of'} the 98% target.\n\n")

                f.write("## Category Results\n\n")
                for category, data in results['truth_score']['category_breakdown'].items():
                    f.write(f"- **{category}:** {data['score']:.1%} ({data['tests_passed']}/{data['total_tests']} tests)\n")

                f.write("\n## Evidence Collected\n\n")
                f.write(f"- Total Evidence Items: {results['report']['evidence_summary']['total_evidence_items']}\n")
                f.write(f"- Real API Testing: {'Yes' if results['report']['evidence_summary']['has_real_api_evidence'] else 'No'}\n")

                f.write("\n## Recommendations\n\n")
                for rec in results['report']['recommendations']:
                    f.write(f"- {rec}\n")

            print(f"üìÑ Human-readable report saved to: {readable_file}")

        return results

    except Exception as e:
        print(f"‚ùå Fatal error: {str(e)}")
        logger.exception("Fatal error in main")
        return None

if __name__ == "__main__":
    asyncio.run(main())