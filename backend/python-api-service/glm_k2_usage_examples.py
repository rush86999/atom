"""
GLM-4.6 and Kimi K2 Usage Examples

This file demonstrates how to use the newly added GLM-4.6 and Kimi K2 providers
in the ATOM BYOK system.
"""

from glm_46_handler_real import GLM46ServiceReal, get_glm_46_service_real
from kimi_k2_handler_real import KimiK2ServiceReal, get_kimi_k2_service_real
from user_api_key_service import get_user_api_key_service


def example_glm_46_usage():
    """Example usage of GLM-4.6 service"""
    print("ğŸš€ GLM-4.6 (Zhipu AI) Usage Example")
    print("=" * 50)
    
    # Get service instance
    glm_service = get_glm_46_service_real()
    
    # Test connection (if API key is configured)
    print("1. Testing GLM-4.6 connection...")
    test_result = glm_service.test_connection()
    if test_result.get("success"):
        print(f"âœ… Connection successful: {test_result.get('message')}")
    else:
        print(f"âŒ Connection failed: {test_result.get('message')}")
        return
    
    # Example chat completion
    print("\n2. Chat completion example...")
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ã€‚"},
        {"role": "user", "content": "è¯·ç”¨ä¸­æ–‡ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}
    ]
    
    chat_result = glm_service.chat_completion(
        messages=messages,
        model="glm-4.6",
        max_tokens=150,
        temperature=0.7
    )
    
    if chat_result.get("success"):
        print(f"âœ… Chat response: {chat_result.get('content')}")
        print(f"ğŸ“Š Usage: {chat_result.get('usage')}")
        print(f"ğŸ“ Model: {chat_result.get('model')}")
    else:
        print(f"âŒ Chat failed: {chat_result.get('error')}")
    
    # Example embedding
    print("\n3. Embedding example...")
    texts = ["ä½ å¥½ä¸–ç•Œ", "Hello world", "äººå·¥æ™ºèƒ½"]
    
    embed_result = glm_service.embedding(
        texts=texts,
        model="embedding-2"
    )
    
    if embed_result.get("success"):
        embeddings = embed_result.get("embeddings", [])
        print(f"âœ… Generated {len(embeddings)} embeddings")
        if embeddings:
            print(f"ğŸ“ First embedding dimension: {len(embeddings[0].get('embedding', []))}")
        print(f"ğŸ“Š Usage: {embed_result.get('usage')}")
    else:
        print(f"âŒ Embedding failed: {embed_result.get('error')}")
    
    # Get model information
    print("\n4. Model information...")
    model_info = glm_service.get_model_info("glm-4.6")
    if model_info.get("success"):
        info = model_info.get("model_info", {})
        print(f"âœ… Model: {info.get('name')}")
        print(f"ğŸ“ Description: {info.get('description')}")
        print(f"ğŸ“ Context Length: {info.get('context_length')}")
        print(f"ğŸ’° Input Cost: ${info.get('input_cost', 0)}/1K tokens")
        print(f"ğŸ’° Output Cost: ${info.get('output_cost', 0)}/1K tokens")
        print(f"ğŸ¯ Capabilities: {', '.join(info.get('capabilities', []))}")


def example_kimi_k2_usage():
    """Example usage of Kimi K2 service"""
    print("\n\nğŸš€ Kimi K2 (Moonshot AI) Usage Example")
    print("=" * 50)
    
    # Get service instance
    kimi_service = get_kimi_k2_service_real()
    
    # Test connection (if API key is configured)
    print("1. Testing Kimi K2 connection...")
    test_result = kimi_service.test_connection()
    if test_result.get("success"):
        print(f"âœ… Connection successful: {test_result.get('message')}")
    else:
        print(f"âŒ Connection failed: {test_result.get('message')}")
        return
    
    # Example chat completion
    print("\n2. Standard chat completion example...")
    messages = [
        {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ çš„èƒ½åŠ›ã€‚"}
    ]
    
    chat_result = kimi_service.chat_completion(
        messages=messages,
        model="moonshot-v1-8k",
        max_tokens=150,
        temperature=0.7
    )
    
    if chat_result.get("success"):
        print(f"âœ… Chat response: {chat_result.get('content')}")
        print(f"ğŸ“Š Usage: {chat_result.get('usage')}")
        print(f"ğŸ“ Model: {chat_result.get('model')}")
    else:
        print(f"âŒ Chat failed: {chat_result.get('error')}")
    
    # Example long context chat
    print("\n3. Long context chat example...")
    long_messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“é—¨å¤„ç†é•¿æ–‡æ¡£çš„AIåŠ©æ‰‹ã€‚"},
        {"role": "user", "content": "æˆ‘æœ‰ä¸€ä¸ªå¾ˆé•¿çš„æ–‡æ¡£éœ€è¦åˆ†æ..."}
    ]
    
    long_context_result = kimi_service.long_context_chat(
        messages=long_messages,
        model="moonshot-v1-128k",
        max_tokens=200,
        temperature=0.3
    )
    
    if long_context_result.get("success"):
        print(f"âœ… Long context response: {long_context_result.get('content')[:100]}...")
        print(f"ğŸ“Š Usage: {long_context_result.get('usage')}")
        print(f"ğŸ“ Model: {long_context_result.get('model')}")
    else:
        print(f"âŒ Long context chat failed: {long_context_result.get('error')}")
    
    # Example document analysis
    print("\n4. Document analysis example...")
    document_text = """
    äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼ŒAIï¼‰æ˜¯ä¸€é—¨æ–°çš„æŠ€æœ¯ç§‘å­¦ã€‚
    å®ƒæ˜¯ç ”ç©¶ã€å¼€å‘ç”¨äºæ¨¡æ‹Ÿã€å»¶ä¼¸å’Œæ‰©å±•äººçš„æ™ºèƒ½çš„ç†è®ºã€æ–¹æ³•ã€æŠ€æœ¯åŠåº”ç”¨ç³»ç»Ÿçš„æŠ€æœ¯ç§‘å­¦ã€‚
    è¯¥é¢†åŸŸçš„ç ”ç©¶åŒ…æ‹¬æœºå™¨äººã€è¯­è¨€è¯†åˆ«ã€å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œä¸“å®¶ç³»ç»Ÿç­‰ã€‚
    """
    
    doc_analysis_result = kimi_service.analyze_document(
        document_text=document_text,
        question="è¯·åˆ†æè¿™æ®µå…³äºäººå·¥æ™ºèƒ½çš„æ–‡æœ¬ï¼Œæå–å…³é”®ä¿¡æ¯ã€‚",
        model="moonshot-v1-128k"
    )
    
    if doc_analysis_result.get("success"):
        print(f"âœ… Document analysis: {doc_analysis_result.get('content')[:200]}...")
        print(f"ğŸ“ Document length: {doc_analysis_result.get('document_length')} characters")
        print(f"ğŸ“Š Usage: {doc_analysis_result.get('usage')}")
    else:
        print(f"âŒ Document analysis failed: {doc_analysis_result.get('error')}")
    
    # Example reasoning chat
    print("\n5. Complex reasoning example...")
    reasoning_result = kimi_service.reasoning_chat(
        problem="å¦‚æœä¸€ä¸ªæˆ¿é—´é‡Œæœ‰3åªçŒ«ï¼Œæ¯åªçŒ«æŠ“äº†2åªè€é¼ ï¼Œè¯·é—®æ€»å…±æŠ“äº†å¤šå°‘åªè€é¼ ï¼Ÿè¯·è¯¦ç»†è§£é‡Šä½ çš„æ¨ç†è¿‡ç¨‹ã€‚",
        context="è¿™æ˜¯ä¸€ä¸ªç®€å•çš„æ•°å­¦é€»è¾‘é—®é¢˜ã€‚",
        model="moonshot-v1-32k"
    )
    
    if reasoning_result.get("success"):
        print(f"âœ… Reasoning response: {reasoning_result.get('content')}")
        print(f"ğŸ§  Type: {reasoning_result.get('reasoning_type')}")
    else:
        print(f"âŒ Reasoning failed: {reasoning_result.get('error')}")
    
    # Get model information
    print("\n6. Model information...")
    models_to_check = ["moonshot-v1-8k", "moonshot-v1-32k", "moonshot-v1-128k"]
    for model in models_to_check:
        model_info = kimi_service.get_model_info(model)
        if model_info.get("success"):
            info = model_info.get("model_info", {})
            print(f"âœ… {info.get('name')}:")
            print(f"   ğŸ“ Context: {info.get('context_length')} tokens")
            print(f"   ğŸ’° Cost: ${info.get('input_cost', 0)}/1K in, ${info.get('output_cost', 0)}/1K out")
            print(f"   ğŸ¯ {', '.join(info.get('capabilities', []))}")


def example_byok_integration():
    """Example usage with BYOK system"""
    print("\n\nğŸ” BYOK System Integration Example")
    print("=" * 50)
    
    # Get user API key service
    user_api_service = get_user_api_key_service()
    
    # Example user ID
    test_user_id = "example_user_123"
    
    # Show available providers
    print("1. Available providers in BYOK system:")
    from user_api_key_routes import AVAILABLE_AI_PROVIDERS
    
    for provider_key, provider_config in AVAILABLE_AI_PROVIDERS.items():
        print(f"   ğŸ“‹ {provider_config['name']}")
        print(f"      ğŸ¯ {', '.join(provider_config['capabilities'])}")
        print(f"      ğŸ’° Cost: {provider_config.get('cost_savings', 'Varies')}")
    
    # Example: Test configured keys (if any)
    print(f"\n2. Testing configured services for user {test_user_id}:")
    configured_services = user_api_service.list_user_services(test_user_id)
    
    if configured_services:
        for service in configured_services:
            test_result = user_api_service.test_api_key(test_user_id, service)
            print(f"   ğŸ”‘ {service}: {'âœ… Working' if test_result.get('success') else 'âŒ Failed'}")
    else:
        print("   ğŸ“ No services configured yet")
    
    print("\n3. To configure new providers:")
    print("   ğŸŒ Web: Navigate to /settings and click 'AI Providers' tab")
    print("   ğŸ–¥ï¸  Desktop: Open Settings > AI Provider Settings")
    print("   ğŸ”‘ Add your GLM-4.6 or Kimi K2 API keys")
    print("   âœ… Test connection to verify they work")


if __name__ == "__main__":
    print("ğŸ¯ GLM-4.6 and Kimi K2 Usage Examples")
    print("=" * 60)
    
    # Run examples
    try:
        example_glm_46_usage()
        example_kimi_k2_usage()
        example_byok_integration()
        
        print("\n\nğŸ‰ Examples completed successfully!")
        print("\nğŸ“– For more information:")
        print("   ğŸ“„ Check GLM_K2_INTEGRATION_SUMMARY.md")
        print("   ğŸ“– Read BYOK_USER_GUIDE.md")
        print("   ğŸ”§ See handler source code for advanced usage")
        
    except Exception as e:
        print(f"\nâŒ Error running examples: {e}")
        print("\nğŸ’¡ Note: These examples require API keys to be configured")
        print("   Set GLM_4_6_API_KEY and KIMI_K2_API_KEY environment variables")
        print("   Or configure them through the BYOK system settings")