[pytest]
# Pytest Configuration for Atom Testing Framework
# Supports property-based, fuzzy, mutation, and chaos testing

# Test Discovery
pythonpath = .
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*

# Markers
markers =
    # Test Type Markers
    unit: Unit tests (fast, isolated)
    integration: Integration tests (slower, requires dependencies)
    property: Property-based tests using Hypothesis
    invariant: Invariant tests (critical system invariants)
    contract: Interface contract tests
    fast: Fast tests (<0.1s)
    slow: Slow tests (> 1 second)
    fuzzy: Fuzzy tests using Atheris or python-fuzz
    mutation: Mutation testing validation
    chaos: Chaos engineering tests

    # Domain Markers
    financial: Financial operations tests
    security: Security validation tests
    api: API contract tests
    database: Database model tests
    workflow: Workflow execution tests
    episode: Episode management tests
    agent: Agent coordination tests
    governance: Agent governance tests

    # Priority Markers
    P0: Critical priority (security, financial)
    P1: High priority (core business logic)
    P2: Medium priority (API, tools)
    P3: Low priority (nice-to-have)

    # Governance Markers
    student: Tests for STUDENT maturity agents
    intern: Tests for INTERN maturity agents
    supervised: Tests for SUPERVISED maturity agents
    autonomous: Tests for AUTONOMOUS maturity agents

    # Quality Markers
    flaky: Tests that may be flaky and need retry (temporary workaround only)

    # Scenario Markers
    scenario: Scenario-based end-to-end tests

# Reporting
addopts = -v --strict-markers --tb=short --reruns 3 --reruns-delay 1 --ignore=tests/integration/episodes/test_lancedb_integration.py --ignore=tests/integration/episodes/test_graduation_validation.py --ignore=tests/integration/episodes/test_episode_lifecycle_lancedb.py --ignore=tests/integration/governance/test_graduation_exams.py --ignore=tests/unit/test_agent_integration_gateway.py --cov=core --cov=api --cov=tools --cov-report=html:tests/coverage_reports/html --cov-report=term-missing:skip-covered --cov-report=json:tests/coverage_reports/metrics/coverage.json --showlocals

# Async support
asyncio_mode = auto

# Hypothesis Settings
# Note: hypothesis_strategy, hypothesis_max_examples, and hypothesis_derandomize
# are deprecated. Hypothesis now uses settings.profile instead.
# See: https://hypothesis.readthedocs.io/en/latest/settings.html

# Ignore Patterns
# Note: 'ignore' is deprecated in pytest 7.4+. Use --ignore command-line option
# or configure in pyproject.toml if needed.

# Logging
log_cli = true
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S


# ============================================================================
# COVERAGE CONFIGURATION
# ============================================================================
#
# Coverage settings for measuring test coverage across core, api, and tools modules.
# Configuration follows pytest-cov and coverage.py standards.
#
[coverage:run]
source = backend
omit =
    */tests/*
    */test_*.py
    */__pycache__/*
    */migrations/*
    */venv/*
    */virtualenv/*
    .venv/*
    env/*
branch = true

[coverage:report]
precision = 2
show_missing = True
skip_covered = false
exclude_lines =
    pragma: no cover
    def __repr__
    raise AssertionError
    raise NotImplementedError
    if __name__ == .__main__.:
    if TYPE_CHECKING:
    class .*\\bProtocol\\):
    @(abc\\.)?abstractmethod

[coverage:html]
directory = tests/coverage_reports/html

[coverage:xml]
output = tests/coverage_reports/metrics/coverage.xml
# ============================================================================
# FLAKY TEST DETECTION CONFIGURATION
# ============================================================================
#
# Pytest-rerunfailures is configured to automatically retry failed tests up to
# 3 times with a 1-second delay between retries. This helps detect flaky tests
# that fail intermittently due to timing issues, race conditions, or external
# dependencies.
#
# Configuration:
#   --reruns 3           : Retry failed tests up to 3 times before reporting failure
#   --reruns-delay 1     : Wait 1 second between retry attempts
#   --rerun-exclude      : Exclude certain tests from retry (e.g., expected failures)
#
# Usage:
#   1. If a test fails intermittently, investigate the root cause first
#   2. As a temporary workaround ONLY, mark with @pytest.mark.flaky
#   3. Fix the underlying issue (proper async coordination, mocks vs real services)
#   4. Remove the @pytest.mark.flaky marker once the test is stable
#
# Common causes of flaky tests:
#   - Race conditions in parallel execution
#   - Improper async/await handling
#   - External service dependencies (network, databases)
#   - Time-based assertions without proper mocking
#   - Shared state between tests
#   - Non-deterministic test data (random, timestamps)
#
# Investigating flaky test failures:
#   1. Run the test in isolation: pytest tests/test_module.py::test_function -v
#   2. Run with verbose output to see retry attempts: pytest -v --reruns 3
#   3. Check for shared state in fixtures and test data
#   4. Add proper mocks for external dependencies
#   5. Use unique_resource_name fixture for parallel test isolation
#
# NOTE: @pytest.mark.flaky is a TEMPORARY workaround. The goal is to fix
#       flaky tests, not mask them with automatic retries.
# ============================================================================
