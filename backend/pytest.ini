[pytest]
# Pytest Configuration for Atom Testing Framework
# Supports property-based, fuzzy, mutation, and chaos testing

# Test Discovery
pythonpath = .
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*

# Markers
markers =
    # Test Type Markers
    unit: Unit tests (fast, isolated)
    integration: Integration tests (slower, requires dependencies)
    property: Property-based tests using Hypothesis
    invariant: Invariant tests (critical system invariants)
    contract: Interface contract tests
    slow: Slow tests (> 1 second)
    fuzzy: Fuzzy tests using Atheris or python-fuzz
    mutation: Mutation testing validation
    chaos: Chaos engineering tests

    # Domain Markers
    financial: Financial operations tests
    security: Security validation tests
    api: API contract tests
    database: Database model tests
    workflow: Workflow execution tests
    episode: Episode management tests
    agent: Agent coordination tests
    governance: Agent governance tests

    # Priority Markers
    P0: Critical priority (security, financial)
    P1: High priority (core business logic)
    P2: Medium priority (API, tools)
    P3: Low priority (nice-to-have)

    # Governance Markers
    student: Tests for STUDENT maturity agents
    intern: Tests for INTERN maturity agents
    supervised: Tests for SUPERVISED maturity agents
    autonomous: Tests for AUTONOMOUS maturity agents

    # Quality Markers
    flaky: Tests that may be flaky and need retry (temporary workaround only)

# Reporting
addopts = -v --strict-markers --tb=short --reruns 3 --reruns-delay 1 --cov=core --cov=api --cov=tools --cov-report=html:tests/coverage_reports/html --cov-report=term-missing:skip-covered --cov-report=json:tests/coverage_reports/metrics/coverage.json --cov-fail-under=80 --cov-branch --showlocals

# Async support
asyncio_mode = auto

# Hypothesis Settings
hypothesis_strategy = conservative
hypothesis_max_examples = 200
hypothesis_derandomize = false

# Ignore Patterns
ignore =
    # Ignore development/debug scripts
    tests/investigate_*.py
    tests/verify_*.py
    tests/quick_*.py
    tests/final_*.py
    tests/simple_*.py
    # Ignore mock test files
    tests/mock_*.py

# Logging
log_cli = true
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S


# ============================================================================
# FLAKY TEST DETECTION CONFIGURATION
# ============================================================================
#
# Pytest-rerunfailures is configured to automatically retry failed tests up to
# 3 times with a 1-second delay between retries. This helps detect flaky tests
# that fail intermittently due to timing issues, race conditions, or external
# dependencies.
#
# Configuration:
#   --reruns 3           : Retry failed tests up to 3 times before reporting failure
#   --reruns-delay 1     : Wait 1 second between retry attempts
#   --rerun-exclude      : Exclude certain tests from retry (e.g., expected failures)
#
# Usage:
#   1. If a test fails intermittently, investigate the root cause first
#   2. As a temporary workaround ONLY, mark with @pytest.mark.flaky
#   3. Fix the underlying issue (proper async coordination, mocks vs real services)
#   4. Remove the @pytest.mark.flaky marker once the test is stable
#
# Common causes of flaky tests:
#   - Race conditions in parallel execution
#   - Improper async/await handling
#   - External service dependencies (network, databases)
#   - Time-based assertions without proper mocking
#   - Shared state between tests
#   - Non-deterministic test data (random, timestamps)
#
# Investigating flaky test failures:
#   1. Run the test in isolation: pytest tests/test_module.py::test_function -v
#   2. Run with verbose output to see retry attempts: pytest -v --reruns 3
#   3. Check for shared state in fixtures and test data
#   4. Add proper mocks for external dependencies
#   5. Use unique_resource_name fixture for parallel test isolation
#
# NOTE: @pytest.mark.flaky is a TEMPORARY workaround. The goal is to fix
#       flaky tests, not mask them with automatic retries.
# ============================================================================
